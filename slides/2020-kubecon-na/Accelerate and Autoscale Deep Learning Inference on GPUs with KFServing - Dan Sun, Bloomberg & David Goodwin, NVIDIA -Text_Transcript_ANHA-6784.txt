Accelerate and Autoscale Deep Learning Inference on GPUs with KFServing: ANHA-6784 - events@cncf.io - Thursday, November 19, 2020 3:47 PM - 37 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone. Welcome to The Talk today for accelerate and a standardized deep learning friends with gift wrapping.
00:00:27 [W] Hello, everyone. Welcome to The Talk today for accelerator and standardized deep learning friends West Coast laughing.
00:00:34 [W] The presenter today is the endzone from Berber from Bloomberg and the deputy grooving from Nvidia.
00:00:39 [W] We have to let you know today the first part we will talk about accelerated learning friends to escape serving the second part. We will talk about caves R&B to inference protocol.
00:00:51 [W] Deploying and models at secure is one of the most challenges for companies were learning to create value through AI as models becomes more complex and deep and more deep distrust becomes even harder.
00:01:05 [W] As a data scientist or emerging year.
00:01:07 [W] I want to serve standardized deep learning models like tensorflow of python church with minimal efforts and address care in a unified way.
00:01:17 [W] I may also want to bring in custom pre-and post processing before and after the inference.
00:01:23 [W] Running inference on deployment models can be slower on CPU.
00:01:27 [W] I also want to accelerate the inference by deploying models on two gpus to here's a powerful computer source, but deploying a single model per GPU can underutilize gpus.
00:01:38 [W] gpus. I want an easy way to serve multiple models behind a single unified and point which can scale to hundreds to thousands of waters.
00:01:47 [W] You don't have to save to computer resource.
00:01:49 [W] I will also run to Auto Scale based on workloads and allow me to scale down to zero when there's no traffic central series.
00:01:58 [W] in order to ensure safe production rods
00:02:02 [W] I I want to deploy models with zero downtime and can use multiple deployment strategies like Shadow Canary and the blue-green robots.
00:02:11 [W] And it can use multiple deployment strategies like Shadow Canary and the blue-green robots.
00:02:21 [W] In order to solve all these problems kept serving is open source project funded by Google seldom, Bloomberg and media Microsoft and IBM under kubeflow kubeflow was the perfect meeting ground for these companies.
00:02:34 [W] Keep serving is following all the open-source principles Builder to build open and crawl native serving Solutions.
00:02:43 [W] Give something provides Kuma knative custom resource for serving a male models across deep learning Frameworks with a simple intuitive and consistent experience. It encapsulates a complex complexity of auto-scaling networking has checking
00:02:58 [W] Figuration to bring cutting-edge serving features like GPU Auto scaling and a canary Rose to your MLG promise.
00:03:08 [W] It enables a simple pluggable and the complete story for production and Mercer V including pre-processing prediction and explanation.
00:03:20 [W] Care serving codifies the back best practice of Kuma latest in appearance with connective give certain he's able to enable serverless inference and automatic this Gear Up and Down according to the increased workload cave serving extracts a common model serving features like
00:03:35 [W] Multimodal holding patching into a cycle agent so that or integrated model server can benefit and all these features.
00:03:31 [W] Kev Summit Apartments are immutable every new to prom and the results in your new version and the traffic is macro it over to new regime only after the new parts are ready and passing the Readiness check to ensure safe production rods.
00:03:43 [W] It also employs various other Rod strategies such as Canary and the progressive rods.
00:03:52 [W] The main calf strain components.
00:03:54 [W] Are you still in Grass Gateway connective autoscaler and infra service is tearing grass Gateway rods the external requests to the internet service between the service part contains two or three containers depending on the features thing about an evening. Service llamo.
00:04:09 [W] The main containers in up in the key infrastructure as part of our queue proxy kept serving agent and model server.
00:04:17 [W] After the Ingress the request press hit the cue proxy which improves the concurrency limit and timeout as n grows Rule and optional sychar agent if requested Branch logging and batching is able to the requester finally hits the model
00:04:32 [W] It's a model server for each frame Rook implements rest or grpc handlers and it can load multiple models.
00:04:33 [W] So inference Service Parts can be autoscale based on CPU or inference workloads.
00:04:44 [W] Kev serving 0.5 release promotes the infrastructure is API version from V1 V2 to V1 beta-1a suppose the standard in principle tensorflow by Koch is Killin extra boost.
00:04:56 [W] Mmm Frameworks.
00:04:57 [W] It also provides SDK for user to plugin custom component while still benefit or the common serving teachers from cave serving.
00:05:06 [W] If you want me to run API further simplifies and enables a data science deaf friendly interface and also maintains the flexibility kubernative spot templates back provides.
00:05:17 [W] In about a few few llamo lines, you could have described all the infrastructure. You need to get up your your models up and running on the other hand user can still specify Advance the fields we need to
00:05:32 [W] Have serving not only provides a unified interface for control plan. It also tries to standardize the data plane protocol Across America Frameworks, which we will cover later.
00:05:44 [W] 60.5 we also added a Marty model serving capability to improve the resource utilization.
00:05:50 [W] from kept serving
00:05:39 [W] the b 1 beta 1 API further simplifies and enables a data science the friendly interface and also maintains the flexibility kubernative spot template provides.
00:05:50 [W] In about a few few llamo lines, you could have described all the infrastructure. You need to get up your your models up and running on the other hand user can still specify Advance the fields we need to
00:06:05 [W] If serving not only provides a unified interface for control plan, it also tries to standardize the data plane protocol Across America Frameworks, which we will cover later.
00:06:17 [W] Since 0.5.
00:06:19 [W] also added a Marty model serving capability to improve the resource utilization.
00:06:25 [W] For tens of Role Models cave serving uses the TF serving as the underlying model server, which is a flexible high-performance serving solution, which is positive role model for Matt and graft it.
00:06:35 [W] Tear serving uses the tensorflow rest and grpc protocol protocol, which is similar to keep serving V1 protocol for pythons models give serving integrate starch syrup which provides an easy way to serve both eager model and touch scraping model which
00:07:36 [W] Earth which is a flexible high-performance serving solution, which is positive the model format and graft it.
00:07:42 [W] Tear serving uses the tensorflow rest when grpc protocol protocol which is similar to keep serving V1 protocol for pythons models give serving integral parts of which provides an easy way to serve both eager model and touch scraper model, which
00:07:57 [W] Keep serving is what country working with tar serve to confirm to the cave serving be to Protection Program.
00:08:05 [W] In d 1 beta 1 API, we mainly supports three components predictor Transformer and explainer predictor is a required component and the Fearsome Predator is mapped to kubernative departments or template Fields such as replica service account.
00:08:19 [W] No Divinity on a predictor user can specify the model framework which naturally maps to continue feels like command arguments environment variables.
00:08:29 [W] Same applies to the Transformer an explainer component.
00:08:34 [W] Immediate Triton even server provides are clouds in resolution on my son Nvidia gpus the server supports multiple deployment Frameworks such as tensorflow.
00:08:46 [W] I coach Onyx with both dressed and grpc importance is support model repo which versioning and allow multiple models to run simultaneously on the same GPU.
00:08:55 [W] with passion support.
00:08:59 [W] Only free service pack the storageos II is pointed to the models to model repo which can contain multiple models as you can see from spok.
00:09:09 [W] We also set the only pronounce words in one variable to improve the inference can performance.
00:09:16 [W] The default steep you limit on for inference service part is one.
00:09:20 [W] You can also choose to schedule the tracking of a spot on gpus by specify it on Spire GPU under the resource section.
00:09:30 [W] We can also choose to add node Affinity or tolerance to schedule to a particular node such as Tipo GPU.
00:09:39 [W] As you may know Bloomberg's customer service is an essential part of Bloomberg. Terminal.
00:09:45 [W] Our is represented as we work very hard to provide the best customer service to our clients by answering questions with high quality and the speed the wraps are push the content of to help answer questions in a smart resource window this use case as powered by our fine
00:10:00 [W] In deployed onto kept serving on production both fine tune the stage the data we are using are categorized and annotate bof accuse which gives us 1/2 million questions. Here's the problem is formulated
00:10:10 [W] It's up to questions and output as a similarity score. The better model is saved using export save the model A p.m.
00:10:16 [W] Which contains complete tensorflow program including where weights and a computation. It does not require the original model building codes to run which makes it useful for sharing and deploying.
00:10:28 [W] As you know Bert model at interest time requires significant computation time and it doing this on CPU can be slow which can take a seconds open the time can take seconds this house done challenge to me to the latency requirements for this versatile for this
00:10:43 [W] The point about model to meet all the production criminal is challenging task in to take all latencies report scaling house track safe throughout monitoring or into consideration.
00:10:52 [W] And and also how can you scared to serve hundreds of about modules with your limited risk DP resources?
00:11:03 [W] First to accelerate the a parting Prince we deployed 0.2 the model on Cave serving with tensorflow serving component which expect the tensor inputs.
00:11:12 [W] Since the user input here is the question here in text.
00:11:16 [W] We also deployed the Cavs learning transformer for the sentence documentation and tokenization Cave serving then automatically words of the court to TF serving in this way.
00:11:26 [W] You can scale Transformer and a particular differently why deployed as a while so you can deploy it as a single unit.
00:11:35 [W] By deploying the model on gpus WE achieve 20 x speed up.
00:11:40 [W] We also experiment the point about model on Nvidia Triton server, which achieves a similar performance gains.
00:11:53 [W] We have tested the performance on a meteor v-102 gpus with 24 layer of p16 virt squad model by deploying but model on GPU we can get to 50 millisecond which usually takes to two second three second running prints on CPU
00:12:09 [W] Sarah pod loaded whispered model the latest increase linearly with the concurrency without patching the request.
00:12:10 [W] We can scare up the Triton into the server instance automatically with kept serving as you can see from the previous table. You get the best of reference we can continue concurrence is low.
00:12:20 [W] So here we try to set the container concurrency to one and that Kennedy of autoscaler scalpings impress service part automatically based on infrared in Friedrich concurrent requests within a Time window.
00:12:36 [W] By setting concurrency to one simple service products auto scan automatically scales up when continue reaches to the max concurrency in this experiment the perfect crunch generates the infrastructures request to your model and measures the support and latency over time window.
00:12:53 [W] And repeats the measurement until it gets stable value for example wind above the ripped in proquest 5 is Skip to five parts to handle the concurrency from the result table.
00:13:05 [W] You can see that the average latency for higher concurrency can maintain as low as when concurrence is low.
00:13:12 [W] You're awesome latest Spike, which are caused by the Pod code startup time which includes model downloading and parts founding time.
00:13:19 [W] concurrency limit
00:13:06 [W] As you can see this report does not scale linearly perfectly because of the inference service code a start-up time one way to alleviate. The issue is to catch the model on PVC after downloading from the remote storage so that the models can be directly
00:13:22 [W] Is to catch the model on PVC after downloading from the remote storage so that the models can be directly among two on mounted on two parts.
00:13:28 [W] Chief you auto-scaling based on GPU metrics can be hard can edit the implements our request volume-based autoscaler, which allows you to scare to n from 0 both on CPU and a GPU.
00:13:40 [W] number of paths needs to scale up. The load is calculated by the in Fright requests and concurrency Target. If the container concurrency can handle five requests concurrently and your infrared requests are 50 then Canadian automatically scale to 10 parts.
00:13:59 [W] Batting for instance is a process of aggregating inference request and standing in sending this aggregated requests through the entire program for inference or at once kept something was done to natively support version of incoming inference requests.
00:14:12 [W] the functionality enables you to use your computer resource optimally because most dire of Frameworks are optimized for batch requests.
00:14:20 [W] You can configure Max batch size and maxillae 10 see on internet service based action the character psychology into their nose the maximal batch size that the more
00:14:28 [W] Can handle and is a maximal times the agent to the way to for fear, each batch request on the inter-service back user can also enable scale down 2-0 to save computer source up the batteries down.
00:14:43 [W] Anything about you to use your computer resource optimally because most dear Frameworks are optimized for batch requests.
00:14:50 [W] You can configure Max batch size and maxillae, Tennessee on inference servicemeshcon the character psychology into their nose the maximal batch size that the model can handle and the maximal times the agent to the way to for fear each batch
00:15:43 [W] Transformer which can communicate to models over with a standardized data plan motoko which expect answering an attacker out Transformer and predicted can also be deployed and rolled out as a single unit in future kept serving also plans to support
00:15:58 [W] box Transformer like text Coconut vision and image transformation
00:16:03 [W] Here's Irving allows changing transform always any in print server here.
00:16:07 [W] We can easily swap to use different one or server. As long as they speak the same data plane protocol protocol.
00:16:12 [W] So pre-process Handler transform the raw data into the tensor according to the V2 data plane protocol and you can see the post post process Handler transform the raw prediction into a response for Downstream consumption.
00:16:29 [W] The Transformer does not enforce a particular data schema. It could be a list of text or images.
00:16:37 [W] The prodyna single modern GPU can only utilize the pressures GPU resources Tiff serving touch serve Triton or allow co-locating multiple models onto the same GPU in the container the to request arrives simultaneously one for each model.
00:16:52 [W] To a Sims GPU and work on both inference computation parlor keep serving as a training model customer source to enable scheduling models onto the inter-service as scare, which obstruct the way the model placements logic way from the user.
00:17:04 [W] The models that are placed on the same inference service customer service can be accessed from the same service URL.
00:17:12 [W] With the capsule in multimodal circuit design. We start a Tutti Capo in free service and model artifacts training models can be placed onto an infra service before it reaches the configured memory limit here serving train a model scheduler can spawn new shots
00:17:27 [W] Cause the models at scale it also stand cats track proposed to each model and point to reflect the Model T promise status on training models customer source.
00:17:30 [W] I'm training model Cancer Resource user assigned the model to a given Energy Service user also specify the model story. You are and a model framework.
00:17:38 [W] So memories resource estimation is used for scheduling models onto the internet service based on the capacity left arm given even service shot and the schedule automatically spok new shots to host the new models if all current shots are at capacity.
00:17:54 [W] The train a model scheduler decides which service shouted to place the model and writes the model configuration into the shark componentconfig map. The caves are inside caging then reconcile to configure map to pour new models and remove the little models in the model repo that
00:18:09 [W] The train a model scheduler decides which service shouted to place the model and writes the model configuration into the shark componentconfig map the cave serving sidecar aging then reconcile to configure map to pour new models and remove the little models in the model repo. That
00:18:28 [W] part of
00:18:30 [W] so 18 is then stands a signal to the model server to Low download the model after model is successfully loaded.
00:18:37 [W] On model server it can then be assessed of run a Unified Service endpoint with the model name specify on the UI o pass.
00:18:47 [W] Why we developed a multi-modal serving we could fuel a scalability issues as well for a single is during rescued way. We handle limits about rounding Tucson and services, but we want to actually Scout a hundred Decay models.
00:19:00 [W] Can we support our country a hundred k3s model customer sauce.
00:19:06 [W] Source. We actually bounded by the some resource limit scarcity, even we can simply deploy a hundred K models on 2002 service. We missed your head to the limit of the number of
00:19:17 [W] Which service we can create on the great way for locking the models.
00:19:21 [W] So the phase two Mighty model surveys to find out assessments.
00:19:28 [W] We are excited that kept serving 0.5 is released.
00:19:33 [W] Where's the V 1 Theta 1 API and try to waste fee to protocol and here you can check the view on beta 1 I've see I've see doc multi-model serving is in our first steps and the next step is to make to
00:19:49 [W] Give some instances open community.
00:19:51 [W] So we love you can be contributions and you're welcome to join our back bi-weekly working group meeting and here is the GitHub and example links.
00:20:05 [W] Thank you. I will hand over to degrading for the second part.
00:20:20 [W] Hi, my name is David Goodwin.
00:20:22 [W] I'm going to be talking about the KF serving inference protocol version 2 you might have also heard this called the the two data plane protocol.
00:20:32 [W] So before we talk about the specifics of the protocol less look at kind of the domain where this protocol is important and is meant to be used. So the diagram here shows kind of a representative way.
00:20:47 [W] Wang can say a containerized inference server. So in the grey box on the right, you see there's this inference server that is capable of Performing deep learning a machine-learning inferences on behalf of whatever
00:20:49 [W] Performing deep learning a machine-learning inferences on behalf of whatever clients there are and it is is inside of a kind of deployment environment.
00:20:50 [W] It has access to the model repository which is holding all the different models that it might need to serve and there also may be some load balancers and some autoscaler zettaset ra those those aren't required.
00:21:03 [W] This is just showing kind of a representative example and then on the left you see there's some clients so these are clients.
00:21:09 [W] I want to directly or indirectly take advantage of some deep learning or some machine learning models. And so they can communicate for example, maybe there's an ASR an automatic speech recognition kind of service that they want to use which will in turn need
00:21:24 [W] Inferencing and there could be multiple of these workloads. And again, there can be load balancers and in the deployment the protocol is concerned about how the clients communicate with the workloads and also have the workloads then communicate with the
00:21:39 [W] Ours is themselves and the protocol is meant to be a single protocol that can be used in any of these areas.
00:21:41 [W] So given that background for the KF serving inference protocol.
00:21:51 [W] Why do we need the standard?
00:21:54 [W] Well, like most reasons why you need a standard you want the say the inference clients which in the previous diagram where the things on the left it needed to use some inferencing.
00:22:04 [W] We want them to be able to talk to multiple different servers or services to increase their portability to make them more interchangeable. And on the server side you want to allow as many different types of clients as possible maybe ones that weren't originally meant written even to use
00:22:19 [W] To use your server or service and that increases the utility of your service.
00:22:19 [W] And of course you want them to operate seamlessly and all sorts of platforms like KF servings, which is standardized around this protocol.
00:22:26 [W] So some of the high level of requirements are support support both ease of use and high performance, which we'll talk about some they need to be extensible.
00:22:37 [W] So the KFC serving protocol talk about is has a core protocol which is kind of the required part, but there's a extension mechanism in it.
00:22:45 [W] It allows either for in the future there to be standard extensions added to it.
00:22:51 [W] So optional parts of the of the core or you could have a server will see that server specific ones, which are individuals inference servers can decide to implement their own extensions and there needs to be both a grpc and an
00:23:06 [W] And a Json based implementation for this which will look at also in a minute.
00:23:07 [W] So I mentioned there's the core protocol and extension the core protocol which we'll go through in some detail is required for all conforming servers. So if you want to if you want to write an inference server and you want to say that your inference server supports the KF serving
00:23:22 [W] need to implement all these and they include in points to understand if the server is live and ready to get kind of metadata about the server and then for all the models that are available on the server all the Deep learning machine learning models that are available
00:23:28 [W] Model is ready and metadata about the model. And of course the primary reason for doing all this there needs to be an inferencing in point to allow you to actually perform inferencing and then again extensions are
00:23:40 [W] Currently the standard doesn't have any there's no standard extensions.
00:23:39 [W] So there's no optional parts of the core, but we will see there are some extensions have been implemented by specific inference servers and in particular we're going to look at some the Triton in front server has implemented some extensions or
00:23:54 [W] okay, so the liveness in the Readiness in point some together, you can think of these as the health and points and on the server side, there's two different endpoints that you can see whether the server is live and all ready to receive requests
00:24:09 [W] And something like kubernative you can directly use these for liveness probe and Readiness probe and on the model side.
00:24:15 [W] There's just a ready and that can be used to indicate if the model is ready to receive requests.
00:24:19 [W] And so this is some example here again. There's an HTTP rest Style version of this protocol and it just returns for these returns that uses the HTTP status code to indicate whether something is live.
00:24:32 [W] We're ready and here we're asking if the serverless
00:24:36 [W] over is live and you get an OK back. So that means the server is live grpc side has you know, it's very similar.
00:24:43 [W] It's course. It's using the RPC style.
00:24:46 [W] But there's a serverless I've and it would turn a bowl of to frost response that would indicate whether the circle is alive.
00:24:53 [W] All right.
00:24:54 [W] So let's move on metadata is another core part of the protocol and the server metadata is things like name versions what extensions are supported. The model metadata is more interesting you can kind of asked so what you know for a given model that's on the server
00:25:09 [W] You can find out what versions of the model are available.
00:25:09 [W] The protocol allows the server to have multiple versions of the same model and then you know, most importantly maybe what input tensors are you required to send into the model and what outputs can you get back and for the inputs you can see
00:25:24 [W] Quick tensors.
00:25:16 [W] Are you required to send into the model? And what outputs can you get back?
00:25:20 [W] And for the inputs you can see here. I've showing for this example and in going forward here.
00:25:24 [W] I'm just going to kind of show the kind of the Json HTTP examples the grpc Prototype based grpc examples are very similar.
00:25:34 [W] It's just kind of different syntax. So I'm not going to bother to show both. So here you can see from the URL. We're using the V /v to is / Marzi.
00:25:44 [W] Models / resnet gives us the metadata for the resident 50 model.
00:25:50 [W] We see that there's just one version version 1 is available.
00:25:52 [W] It's a tensor flow graph deaf model, which is just the type. So the platform is a the type of the machine learning or deep learning framework or representation for this model.
00:26:06 [W] That's kind of informative. One of the advantages of the of the protocol. Of course is that it's the same protocol.
00:26:14 [W] You you send you use the inputs and output tensors and to communicate with the server's the same way. No matter what the actual underlying machine learning or deep learning model is but for informational purposes, it's their inputs.
00:26:29 [W] this case there's only one input and one output on this model and you get the name of the input the shape in the data type, which is very important because this allows you then as a client you can then if you want to discover like well what's you know, what kind of data shape of the data is expected
00:26:44 [W] If you can get that from the metadata and similarly for the outputs, it will tell you what type of output tensors are going to be returned to you and their their shape and their data type.
00:26:48 [W] So now for the again the primary in point our API that's in the protocol of course is for the inferencing.
00:27:00 [W] And again, this is just showing the HTTP the rest style but here's the end point again. We saw before V 2 models resnet 50. So for the resident 50 model we want to do an inference. Does that infer their and in this case?
00:27:14 [W] this is the resonant 50-mile we're going to send in this is just a picture. I
00:27:20 [W] And so resonant 50 is if you're not familiar as an image classification is a very famous image classification deep learning model and it takes in a an image in this case the Intel justify the shape.
00:27:35 [W] Is to 24 by 2 24 so over a very small image with three channels and and it'll do a classification resident 50 will do a classification in and out of in and tell you what that image is. And in this case resident 50, it's been
00:27:48 [W] That data set which there's like a thousand different objects that recognizes and one of them you can see from here. So we on the left here. You see the post post this we send in the input and
00:27:54 [W] Tell the shape that we're sending in and the data here.
00:27:53 [W] I didn't list all the data because we're going to talk about that.
00:27:55 [W] There's a lot of data so just but there would be it's an array of the Json array of the data and they'll be you can tell from the shape. They'll be 224 times 224 times three elements in the state array, which is a lot
00:28:10 [W] This is what the protocol requires the format that's required by the protocol that server needs to respond with no repeating back the model name and version and then giving you the outputs back and the outputs in this case.
00:28:20 [W] You can see the name of it there and the shape.
00:28:23 [W] It's just a single element.
00:28:24 [W] And in this case, it's a single element that tells you the classification of this item and it's a coffee mug so did quite well actually on that on that classification and so here by looking at this example we can see
00:28:36 [W] See, you know, the inference protocol like the other prodyna other ones.
00:28:40 [W] I just talked about earlier.
00:28:40 [W] It's very simple very simple. It's just the necessary kind of basic information that needs to be sent to the inferencing server and back and all based around having input tensors and output tensors
00:28:56 [W] As talked about earlier.
00:28:55 [W] It's very simple.
00:28:57 [W] Very simple.
00:28:57 [W] It's just the necessary kind of basic information that needs to be sent to the inferencing server and back and all based around having input tensors and output tensors with their data types
00:29:21 [W] types and shapes
00:29:23 [W] so let's take a so that's the core protocol.
00:29:28 [W] We just talked about again. The required core parts of the protocol that every conforming server must Implement and in doing that and implementing that the you'll have a server that can talk to in any client that also includes protocol.
00:29:44 [W] That and implementing that the you'll have a server that can talk to in any client that also moments protocol for any type of deep learning and machine learning model that that supports and send in the tensors
00:30:00 [W] Or any type of deep learning and machine learning model that that supports and send in the tensors get back the results and kind of a standard seamless way which satisfies the primary requirements of the protocol or
00:30:15 [W] A call for that matter. So trying inference survey here trying in front server is an open-source inference server. I have a reference link later on you're interested in taking a look at it, but it's it implements the both the HTTP rest
00:30:29 [W] BC KF serving in Firenze protocols
00:30:31 [W] It's a multi framework multi-model.
00:30:33 [W] It's puts gpus and CPUs. So multi framework. I mean if you have a tensor flow or a pie torch or an onyx model again that all those types of Frameworks are supported for inside the inference server.
00:30:48 [W] And once again the core, so it's a conforming server.
00:30:49 [W] It also has some extensions in those so briefly want to talk about extensions here.
00:30:53 [W] So the core is completely sufficient that we just talked about is completely sufficient for making an inference server. However, especially in the area of performance, it can be lacking especially for the HTTP protocol or primary for the HTTP protocol.
00:31:08 [W] Is that train implements?
00:31:07 [W] There's some per model statistics.
00:31:09 [W] So you'll notice in the core protocol.
00:31:13 [W] There was no way say to find out from the server how many inferences possibly had been performed for particular model or you know, how long on average were those taking that, you know kind of Statistics like that Triton adds an extension that provides that information
00:31:28 [W] Incheon for model repository management, so not only can you query Which models are available on the server, but you can load it unload those models or load new models in a mode models also in as an extension and trying their support for a stateful inferencing.
00:31:40 [W] also in as an extension in trying their support for a stateful inferencing which is a lot of language models and other models that are implemented say within our and n-type implementation where their state in the model and
00:31:54 [W] Type implementation where their state in the model and so you have to order of the inferences received is very important.
00:32:00 [W] There's support for that for performance.
00:32:04 [W] Now, the last two items are primary performance bullets cancers, as we saw in the example above you actually send the tensor data is part of the Json message that can be very expensive sending that over the network and so you
00:32:19 [W] If you're communicating to the Triton in front server on the same system from another process on the same system.
00:32:25 [W] you can instead communicate by System shared memory or by GP shared memory and that then you don't have to send the raw data over the over actually over the network even local network in or in
00:32:40 [W] Solder into a grpc protobuf you don't have to do that encoding decoding you can just access it directly in memory.
00:32:40 [W] And similarly the last one which is just an extension that only applies to the HTTP rest part of the protocol is a way to communicate tensors using binary data still going over the network Now using shared memory, but a much more efficient way and talk about that in the next slide.
00:32:55 [W] Um, so and I call this one out. So for all the extensions, I just showed here again.
00:32:55 [W] These are extensions.
00:32:56 [W] They are optional and so the service don't have to implement these. In fact, they're not part of the standard.
00:33:04 [W] Anyway, they're just Triton specific service could Implement these their trains open source.
00:33:09 [W] These are the the specs for all these is published and I you know, I have a link later. But again, they're optional parts.
00:33:18 [W] However, the in particular that I want to call it the HTTP Json the HT rest the Json implementation for high performance.
00:33:26 [W] If we look at we go back on the on the we have this binary data extension and Triton which resolves a very important problem.
00:33:33 [W] So if you look at the code on the left or the output on the left where we post this is the same what we did before where we posted.
00:33:41 [W] We sent in a single small image at 224 by 224 with three Channel image is quite small.
00:33:47 [W] There's still a hundred fifty thousand FP numbers in this data array which course I didn't list them all and so that's a lot not just for a single small image and the problem is that this is very readable right is very usable.
00:34:01 [W] I mean lot it's easy to generate this Json in a lot of different clients a lot of different languages. A lot of different toolkits can already do all this stuff however to do that you first have to so if
00:34:11 [W] As floating Point numbers you first have to basically encode them into Strings.
00:34:13 [W] so print them basically, so that's that's time consuming and then you send them over the wire.
00:34:19 [W] Do the server which is on you know on the other end and then it has to read this Json.
00:34:25 [W] It's basically just a string right so it has to actually parse these strings into floating Point numbers a hundred and fifty thousand of them just to do your inferencing and so that becomes a huge bottleneck basically makes for any any unless your
00:34:40 [W] All input and output tensors or if you just don't really care.
00:34:36 [W] This is a research model or something on the side.
00:34:38 [W] You don't you know form doesn't matter to you. Then this is not really acceptable.
00:34:44 [W] It's not really useful for a production kind of deployment. But still the advantages again going to the ease of use, you know, this. Json is very easy to generate and there's a lot of advantages to doing it this way.
00:34:55 [W] But so let's can we combine those and so this binary data extension does just that so on the right?
00:35:00 [W] We can see using the binary day extension. Basically the header part. The Json part is more or less the same except you have this extra parameter in here what you basically say that hey, I'm not giving you the data here in Json.
00:35:13 [W] It's just but I'm just telling you how big it's going to be which is a little bit redundant but we that's done intentionally just to be redundant and then you just after the Json message is just the raw binary data and there is an extra
00:35:28 [W] In your post just to the server can figure out where the Json metadata header is separating it from the actual raw data, but in doing this now, there's no more. You're still sending, you know 600.
00:35:38 [W] In two thousand bytes of data, you can't that's that's kind of unavoidable.
00:35:41 [W] That's actually your image.
00:35:42 [W] You have to send it over but there's no encoding or decoding overheads.
00:35:49 [W] The data can just be pulled right off the wire and kind of sent right into the into the model so and in so for instance, this is just one data point running on a running on a local network. So actually it's minimizing the
00:36:04 [W] It's kind of removing the network part of this if we send a hundred twenty eight of these requests and time that just to kind of even it out and make sure there's no blips, you know doing the binary data really has like a 17 x speed up a huge amount.
00:36:17 [W] So you kind of remove that bottleneck from from the problem.
00:36:22 [W] So that again that's important example to show how the protocol through its extensions in this case, but in general the protocol does allow the usability and because the flexibility of the protocol you can add have extensions like this that also allow you along with say the shared memory to get great
00:36:38 [W] Performance as well.
00:36:39 [W] And so as I mentioned there's a couple references here in Closing one is to the protocol itself.
00:36:44 [W] You can find it in the KF serving GitHub and comments are welcome on that take a look at it. And then for triton, this is a link you can see from this GitHub. You can find actually the full Triton.
00:36:59 [W] Repo but this link directly takes you to the extensions in the inferencing protocols that Triton implements.
00:37:08 [W] Thank you.
