Rook: Intro and Ceph Deep Dive: ZEUJ-5603 - events@cncf.io - Friday, November 20, 2020 3:11 PM - 33 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone.
00:00:01 [W] Welcome to The Rook intro and Seth deep. Dive talk for coupon North America here. We are virtual again.
00:00:09 [W] happy to be with you.
00:00:10 [W] I am Travis Nielsen one of the maintainers on The Rook project since its beginning.
00:00:16 [W] Happy to be here with Sebastian.
00:00:17 [W] Hello virtual coupon.
00:00:20 [W] I'm Sebastian. I work with Travis as one of the rook maintainers and also work for Red Hat.
00:00:27 [W] Like then also to answer questions will have Blaine Gardener and Alexander trust other Rook maintainers.
00:00:33 [W] All right, first big announcement Rook is graduated.
00:00:36 [W] We are very excited about this.
00:00:38 [W] We've been working hard on this for the last few years just a little history here.
00:00:44 [W] So Rook did get initially inducted as a Sandbox project back almost three years ago in January 2018 for guests to incubation and September of the same year and then
00:00:56 [W] It now in October just just this last month. So thank you a big thank you to The Rook Community cncf Committee rook maintainers and everyone involved.
00:01:08 [W] It has been a wonderful journey and we're grateful your support for your support and look forward to continued growth in the project with that support.
00:01:19 [W] So what does this mean to get to graduated? It really means that we have a lot of large scale production.
00:01:26 [W] Ian deployments out there.
00:01:29 [W] We know people are using it to store their their critical data in production environments.
00:01:35 [W] And this is it's a great evidence of that.
00:01:39 [W] Maybe the third point is that really the Rook is an open project.
00:01:27 [W] It's not only open source, but the governance around the project says that hey this is this is Bender neutral.
00:01:35 [W] have multiple companies contributing expected to contribute so that it stays remains a project that is useful to the whole community and not vendor driven.
00:01:47 [W] So again, thanks for all your support.
00:01:50 [W] We've had a great journey to get here.
00:01:52 [W] All right, let's get started on.
00:01:53 [W] You know, what?
00:01:54 [W] What are we talking about?
00:01:55 [W] What is Rook?
00:01:56 [W] What is it address? What what are issues with storage and communities?
00:02:01 [W] Well, kubernative is a trip traditionally a platform, of course, the manager distributed apps and those apps have been stateless for the most part if you want storage why on deck storage external to
00:02:16 [W] Storage that's not portable. You have to figure out how to deploy it you have maybe you have a Reliance on a cloud provider to give you that managed storage and quickly you get into a vendor lock-in if you are using
00:02:27 [W] If you're managing that storage, okay, you have to build a to operations at once. You set it up. You got an upgraded.
00:02:28 [W] You've got to manage it make sure it stays reliable.
00:02:32 [W] So all of those are challenges that that everyone has with any stifel application and kubernative.
00:02:38 [W] So here's why Rook comes in.
00:02:40 [W] We saw this as a real challenge with Kuma Nettie's and we wanted to address it and kind of solve this problem for the community. So Rook really does have the
00:02:50 [W] All of making storage of available inside your kubernative cluster.
00:02:53 [W] It's not something external.
00:02:55 [W] It's something that's deployed as any other application in your kubenetes cluster.
00:03:02 [W] It's managed with kubenetes operators in C are d s just like any other applications and because it's driven with operators, the deployment of the storage
00:03:17 [W] Needed so deploying it configuring it upgrading it. You you get to decide how it's deployed through the CRTs and other settings, but ultimately Rook manages all the headaches of that day to
00:03:31 [W] As well to get it installed and once you have deployed Rook now, you can consume the storage just like any other kubernative storage with storage class has with persistent volume claims, so I bring the storage platform inside.
00:03:36 [W] Storage platform inside kubernative is the point I Rook is open source with an Apache 2 license.
00:03:41 [W] So it is so what storage are we talking about here? We have several different storage providers in Rook different levels of progression.
00:03:50 [W] So Seth is are stable storage provider.
00:03:55 [W] It's been in since the start of the project since then we've added other storage providers as well to expand the storage.
00:04:03 [W] At form and a fast Cassandra You by DB cockroach DB.
00:04:08 [W] These are still an alpha and we're always looking for Community involvement to help grow those
00:04:14 [W] We do have one source provider that is deprecated.
00:04:18 [W] So edge of has the owners of edge s are working on a replacement for it.
00:04:23 [W] The timeline of that I am not aware of but that's why this deprecated since something new will be coming.
00:04:30 [W] All right, so kind of overall project.
00:04:32 [W] You know, how big is Rook?
00:04:35 [W] Where is it?
00:04:35 [W] So version 1.5 is our latest release at least it will be as of tubecon. I have November 2020 we
00:04:44 [W] while their lease out
00:04:47 [W] as far as how popular project is all the GitHub Stars downloads going on 200 million downloads now for our containers.
00:04:56 [W] And to the community, these are just a few of the stats that show how much it is growing.
00:05:01 [W] Okay. So rook and SEF if we're getting into the ceph portion of this.
00:05:07 [W] What does it mean to bring SEF into kubernative?
00:05:10 [W] That's what Rook will do Rook will bring the ceph storage layer into Kuma teas that make it work.
00:05:16 [W] so what is f if you're not familiar with it Seth is another open source project, but it is a software-defined storage solution that provides
00:05:26 [W] block shared file system and object storage.
00:05:30 [W] That is S3 compliant.
00:05:32 [W] So these are the really the three building blocks of any storage system.
00:05:37 [W] So you'll find out there and Seth has them all in one in one system no need for deploying one solution for block and another one for object or file system Seth provides all of those types of storage together,
00:05:52 [W] Project.
00:05:47 [W] We really saw Seth as being reliable platform.
00:05:50 [W] It's been around for a long time.
00:05:51 [W] We just wanted to bring it to kubernative.
00:05:53 [W] So how did we bring into kubernative think of it as having three different layers in the system where Rook is really the operator in kubernative is that owns the management of Seth so the deployment of It
00:06:08 [W] Observe like Rook manages the configuration of stuff.
00:06:05 [W] If you're familiar with Seth, you know that it is kind of a complicated system to deploy as distributed storage.
00:06:14 [W] It's a rook takes a lot of that complexity out of it and manages it for you second layer.
00:06:20 [W] Now, we've got the ceph CSI driver.
00:06:23 [W] So this CSI driver just like any other storage platform with kubernetes Will provision and then Mount the storage into your
00:06:30 [W] A pod so this is how you consume this the ceph storage. Once you have Rook installed and then finally the third layer is the data layer.
00:06:40 [W] So when you are reading and writing data to the cluster Seth is purely the one that is responding to that that layer and there is no Rook management code in the data path.
00:06:53 [W] It's just Seth at the data layer.
00:06:56 [W] So you really get performance high performance storageos.
00:07:01 [W] Edge at that layer.
00:07:02 [W] All right. So let's look at some pictures of what these layers look like.
00:07:04 [W] So for the first layer with Rook, so this is a picture trying to show that hey we got a single cluster with three nodes.
00:07:12 [W] So each of these black boxes is is a node in your kubenetes cluster. And each of these nodes is running pods the run different types of demons.
00:07:21 [W] So in the center Center node here, we've got the Rook operator poddisruptionbudgets.
00:07:25 [W] Bridge at that layer.
00:07:25 [W] All right. So let's look at some pictures of what these layers look like.
00:07:27 [W] So for the first layer with Rook, so this this is a picture trying to show that we've got a single cluster with three nodes.
00:07:35 [W] So each of these black boxes is is a node in your kubenetes cluster. And each of these nodes is running pods the run different types of demons.
00:07:44 [W] in the center Center node here, we've got the Rook operator poddisruptionbudgets.
00:07:49 [W] And of course that that's the management layer that's going to manage everything else.
00:07:54 [W] So this operator pot is is going to deploy SEF and the CSI driver depending on what settings and how you tell it to be configured.
00:08:06 [W] Okay.
00:08:07 [W] So the these blue pods are really core Rook pods that are providing the management layer the discovery poddisruptionbudgets.
00:08:19 [W] it is available on each node, so you
00:08:23 [W] So Rook knows how to deploy The Zephyr keptn ins. The green pods are CSI drivers.
00:08:29 [W] So on each node, you need a CSI driver that will mount mount the storage and then the red pods are all the ceph demons. So there are quite a number of Seth demons that provide that data layer the
00:08:44 [W] The layer the mons and no STDs are backed by local storage on the Node.
00:08:52 [W] That's very important for step to have that local storage.
00:08:55 [W] Anyway Rook deploys all these pods for you.
00:08:57 [W] And does that management at the kubernative use resource layer creating deployments pods Services?
00:09:06 [W] Okay.
00:09:06 [W] So now the next layer we got CSI provisioning. So this is this is a picture of how your application
00:09:14 [W] action can request storage some the CSI driver.
00:09:19 [W] So if we start on the left here, we've got an application that needs block storage.
00:09:22 [W] Okay, so you create a persistent volume claim usually with read/write Once access.
00:09:31 [W] Okay, so that when you create that claim the the request goes to the storage class, which is which uses F RB D that Rook sets up for you. And then the CSI driver will Mount that that's
00:09:44 [W] Barbecue storage into your application pot. Something very similar happens for shared file system where you've got two applications who need to share the file system.
00:09:54 [W] So they both have their persistent volume claims and the request goes to the storage class.
00:10:00 [W] And then this CSI driver for the shared file system mounts that storage.
00:10:05 [W] Okay. Now the third type of storage we've got was Seth is object with the S3 envelope now this this is different, but we have
00:10:15 [W] It's consistent as it can be with Block in the file storage.
00:10:19 [W] Okay?
00:10:19 [W] So the if you want object storage really what you want is a bucket so you can read my object to the bucket.
00:10:26 [W] And so we create a bucket claim which requests that storage from the storage class crafted for object storage, and we have a bucket provisioner. Then that actually creates a bucket in
00:10:41 [W] Storage and provides that back to the applications.
00:10:29 [W] All right, so that gives us all three types of storage at the provisioning.
00:10:33 [W] Now.
00:10:34 [W] What does it look like at the data path?
00:10:35 [W] You've installed Rook at layer 1 you've got the CSI driver with your storage Mount to the lower layer to now your application just needs to write and read data to the to the cluster.
00:10:47 [W] So again your application.
00:10:48 [W] already got the volume mounted and there is this F5 Ade kernel driver then that will take care of writing to
00:10:56 [W] uh Seth cluster for you, and that driver will it knows how to connect to the difference F demons the mons and OSD use a manager to make that that write that data to the cluster again at the file systems shared file system layer connect to the MDS demon
00:11:11 [W] You're writing to the stuff cluster for you, and that driver will knows how to connect to the difference F demons the moms and I was D's and manager to make that that write that data to the cluster again at the file system shared file system layer connect to the MDS
00:11:33 [W] Just from the locks and the file system semantics and that's three client then connects to the RDW.
00:11:41 [W] And point which then writes the objects into the sept cluster.
00:11:47 [W] Hopefully we're good there after layer 3 the you've set up rook and now you can write to the cluster.
00:11:55 [W] So what does it take to get started?
00:11:56 [W] This is I'm sure it sounds like a complex system but really to get started with Rook we tried to make it as simple as possible and stalling Seth has never been this simple really there are three stackrox.
00:12:10 [W] Ups for for installing Seth three Rook.
00:12:14 [W] Okay. The first thing is while you need to set up authorization or are back in communities.
00:12:20 [W] So you say could cuddle create commandant Li Mo.
00:12:23 [W] Okay.
00:12:23 [W] next you need to run the operator.
00:12:25 [W] And so you create the operator that gets The Rook operator running, but now they're operator.
00:12:30 [W] You have to tell it how to deploy stuff.
00:12:32 [W] now we've got what we call the cluster that cluster cri-o.
00:12:40 [W] How to deploy stuff you tell it well what version of Staff do you want to deploy and you want SEF octopus or do you want which is be 15 or do you want to an earlier version Seth Nautilus, which would be be 14.
00:12:52 [W] Many settings here which is showing a few of them. How many moms how to deploy the storage?
00:12:58 [W] So that is the basic cluster. Once you've created these three these three manifests you'll have a basic set of cluster up and running and Rook as a side note.
00:13:07 [W] There's also a Helm chart we have that will simplify these first two steps and we're working on held charts for our other CRS going forward.
00:13:16 [W] Let's assume Rook is set up now itself.
00:13:19 [W] What does it take to consume that storage?
00:13:22 [W] Just like any other storage platform.
00:13:25 [W] First of all, the admin has to create the storage class after they have increase the storage class. The application requests that storage was with the PVC and then you create your application pot that will Mount that that volume so here on the right.
00:13:40 [W] That shows this the PVC that's going to be mounted into this this demo web server and really this if you've done any storage during a before this should be very familiar.
00:13:53 [W] There's the same pattern that you used to plug in any other storage provider outside of Rook e them.
00:13:59 [W] Okay now, I'll hand off to Sebastian to talk more about key features that everything ends up.
00:14:05 [W] up. Okay. Thanks Travis.
00:14:07 [W] Now let's dive into some of the key features of Rook.
00:14:11 [W] So first environments Rook step is capable of deploying self in Berman environments.
00:14:19 [W] So bring your own artwork if you have your own infrastructure and you want to work on premise then this scenario is for you.
00:14:27 [W] We also support deploying Rook SEF into the cloud with various Cloud providers.
00:14:35 [W] so if you already if your entire existence of structure relies on the cloud and everything is running there already, then you can consume storage through Rook inside inside the cloud environment,
00:14:50 [W] And if he's running there already, then you can consume storage through Rook inside inside the cloud environment.
00:14:53 [W] But first, you might you might ask yourself.
00:14:56 [W] Why would you run seven the cloud? So basically it's all about consistency because kubenetes has been adopted by all the major Cloud providers today, then it is fairly easy to run any Cloud native application as part of
00:15:11 [W] You read any Cloud native application as part of any Cloud providers available out there.
00:15:16 [W] And also there are a bunch of limitations shortcomings coming from the cloud providers.
00:15:22 [W] For instance. The limitation of PV is you can attach to a given node.
00:15:27 [W] Let's assume that we are in a cloud environment then obviously we have appetizers and those hypervisors typically have a limitation in the number of disks that can be attached to a given virtual machine and
00:15:41 [W] And typically that limit is around 30, which means that if you're looking at providing Dynamic provisioning to application, then you might be limited by the number of VMS times the number of peavey's you can attach to
00:15:56 [W] If you do run rook in the cloud though Rook for be running inside virtual machines and because we will be attaching devices.
00:16:05 [W] Then we use our own technology to provide that image provisioning. Then we can scale up to thousands and thousands of of peavey's instead of that 30 limitation by virtual machine and not only
00:16:20 [W] That limitation but also because we are aggregating storage then we're providing much much better performance overall.
00:16:21 [W] If you were to use a single disc, which is typically attached to the storage class, which essentially represents the flavor of storage then that flavor would have IOS as well as throughput limitations,
00:16:37 [W] A given application you will be limited to what that flavor can can provide where if you use self because as I said again, we are aggregating many peavey's then we're providing much much better performance out of the box.
00:16:47 [W] We have configurable cluster topologies Seth is really amazing as being topology of work Seth knows and because it's being told to wear it is running in
00:17:02 [W] Of topology. It is running on Rook can support that.
00:17:05 [W] So it really allows you to deploy a cluster and Define your what Your topology is.
00:17:11 [W] So if you run self Rook stuff inside the data center and then you have multiple rooms multiple racks and multiple nodes, then we can easily build such topology by signing labeled two nodes and all of that will be reflected
00:17:26 [W] Assigning labeled two notes and all of that will be reflected as part of set by doing. So you're increasing data availability and resiliency because you're spreading it across different filler domains. And again those domains can be
00:17:38 [W] Nodes racks rooms in even across zones between different data centers if you want to stretch your Custer CFCs, I drivers so from the very beginning Rook.
00:17:49 [W] One CSI was released in the spectrums release.
00:17:53 [W] We started to collaborate stiff CSI team to integrate as as much as possible.
00:17:57 [W] So just like Travis mentioned we support a large variety of dynamic provisioning modes all wowx and rocks and this for both block and fat system interfaces.
00:18:10 [W] Rook the latest Breeze of Rook 150 ships with that is version of the CSC Celsius I driver which now supports volume expansion and has better features support snapshot in clones.
00:18:25 [W] We still do support the flags driver, but it has really limited support and we really encourage everybody to switch to using the step. CSI spec upgrading is a donated typically upgrades
00:18:41 [W] Some kind of a tedious process and it is really making administrators nervous, especially when it comes to storage you always be wondering. Okay, if something goes wrong what I'd be losing data first this
00:18:56 [W] First this cannot really happen with serve and also one of the good things about surface from the very beginning. It has proven to be super robust that performing upgrades and it's one of the few storage solution not just a software most
00:19:02 [W] software most of the time that can get in a rolling fashion with no downtime and yeah while providing a service so step is already really good at it and with Rook, we really took
00:19:17 [W] Next we really took that further to the next level by aggregating collecting all your personal knowledge that is needed to do upgrades and we embedded all of that logic into Rook.
00:19:30 [W] logic into Rook. So to lift Rook it is really easy as just changing the image in the spec of the deployment file of the operator to a date to step cluster, then it's as easy as changing the self-image version into the cluster cri-o.
00:19:47 [W] Dalal the details when it comes to the upgrade internally going nuts by nose and making sure they're all recovering and being upgraded in back feeling properly and then go to the other one.
00:20:00 [W] So I've grades Made Easy basically extraterrestrial connection. So this is a really popular scenario and we have introduced it in the 1.3 rating cycle indeed.
00:20:13 [W] The use case is quite simple because not everything is about Greenfield environments.
00:20:18 [W] You already have close to out there up and running which are may be Standalone close to those providing object storage, or you may have set close to as providing.
00:20:29 [W] Block for let's say openstack. For example, you're ready to run applications on kubenetes, but not not really ready to move all of that storage because it is difficult and it's challenging to do
00:20:44 [W] So you want to keep your cluster which was deployed by whatever tool can be in be sensible can be Rook as well. If you want to print the goal is to from your kubernative cluster.
00:20:57 [W] You deploy Rukh and then with a few details you connect to the X naught plus turn and then once the connection is established, we're really into this consumer producer relationship were rookies at this point only consuming the external storage
00:21:12 [W] 2 DX Northwestern and then once the connection is established.
00:21:08 [W] We're really into this consumer producer relationship were rookies at this point only consuming the external storage getting all the connection details and asking them to CSI so that we can provide assistance towards to contain it. But that is the only thing we do we don't get into
00:21:23 [W] Content to see a side so that we can provide persistent storage to contain it. But that is the only thing we do we don't get into the business of managing nodes and things like this when we are in external mode. So we just consume the storage that is available
00:21:39 [W] Object that could provisioning so just like time it's mentioned earlier when we were showing the topology of the storage of phases are supported by Rusev.
00:21:48 [W] We mentioned the object picker claim. And again the this is quite simple as a user. The only thing I care about is because I'm developing an S3 compliant application.
00:22:00 [W] I just want to have a bucket with a few policies on that on that bucket and I want to be able to consume that bucket storing data and retrieving data and that's all I care about.
00:22:11 [W] So in a similar fashion of claiming for block or file then I want to clean for a bucket and forth.
00:22:18 [W] We have embedded for quite a while now the LIE bracket provisioner.
00:22:25 [W] But because we really wanted to have it in the more knative / knative kubenetes way.
00:22:33 [W] There was an effort Upstream to implement a cozy interface which is similar to CSI, but for container object.
00:22:43 [W] And it is really the CSI equivalent of object.
00:22:48 [W] So the captain will be today's announcement proposal was merged Upstream.
00:22:52 [W] And now we will be able to vendors will be able to integrate their object solution through the knative kubernative interface, which is really great.
00:23:02 [W] We have a bunch of features and it's probably one of the most feature-rich release we have ever delivered.
00:23:12 [W] So first is encryption with canvas support during the one full cycle.
00:23:18 [W] We introduced encryption for OSD Zone PVCs, and we're storing encryption Keys into kubernative secrets.
00:23:25 [W] So we all know that kubernative Secrets or not. So secret and secure cause they're just merely a
00:23:31 [W] they 64 has value of your value to move that to the next level.
00:23:36 [W] We have introduced support for can s in the first one. We have introduced it hashicorp volt which is really popular. And now we not only deploy encrypted always this but we store those
00:23:51 [W] He's inside world. And what we as a Key Management Service will manage on the keys for us and it is really secure fashion in a really secure environment all the connections to vote or TLS encrypted.
00:24:00 [W] Today, we only support the token-based authentication in the future. We are planning on supporting the kubenetes knative authentication and obviously much more KMS as well mirroring of block data
00:24:15 [W] Is really interesting because we have been having support for bootstrapping RBD mirror demons for wild but we never got into the business of configuring varying between two sites automatically and now it's possible
00:24:27 [W] five
00:24:25 [W] just to get a step back a little bit SEF by Design is strongly consistent.
00:24:32 [W] meaning that whatever climbed rights data, then it has to wait for all the replicas to be written so that the ride ends acknowledged.
00:24:44 [W] And because to that it makes difficult to stretch step between regions for example because of the latencies and that won't be really practical.
00:24:55 [W] So as as a solution for that we The Rook team has built Our Redeemer the RBD mirror demon so that we can replicate block devices as synchronously between clusters.
00:25:10 [W] Now you can have two communities environments and connect each other's and all the block devices.
00:25:20 [W] So the PVCs will be replicated.
00:25:22 [W] Obviously this works along with with steps TSI and and work is still the still moving ahead to to improve that support.
00:25:33 [W] So now we do support automatic automatic configuration of of peers.
00:25:38 [W] In were Pier is basically a site last but not least the stretch cluster kubenetes the stretch kubenetes cluster, which is essentially one cluster.
00:25:49 [W] Obviously this works along with with steps TSI in and work is still the still moving ahead to to improve that support.
00:26:00 [W] now we do support automatic automatic configuration of of peers.
00:26:05 [W] In war Pier is basically a site last but not least the stretch cluster kubenetes the stretch kubenetes cluster, which is essentially one cluster.
00:26:16 [W] that is stretched one kubenetes cluster is stretched across different zones. So the use case typically is that I have two data centers, but I only have two I don't have three data centers which will be really ideal, but I only have
00:26:33 [W] If Enzo's so the use case typically is that I have two data centers, but I only have two I don't have three data centers which will be really ideal.
00:26:42 [W] But I only have two and because staff is corn-based election based.
00:26:49 [W] We need to have some Quorum resolution when we do certain operations in order to maintain the stability of the coaster.
00:27:00 [W] So if we had three sides and it will be really easy what have each month on one side and done but because I only have two and my third side is really limited.
00:27:12 [W] It can be a VM running on the cloud or or it can be in the admin desk something like this.
00:27:20 [W] We have to had a solution for serve to work a bit smoothly with latency.
00:27:30 [W] Higher latency speaker and we have implemented this new mode in Rook. So now you can have a stretched kubenetes cluster and an orbital Zone, which will only contain one month to
00:27:45 [W] Act as a member of the Quorum and perform elections and pursue decision-making. All of the other zones will have storage of edible and I guess that's it for today.
00:27:57 [W] So it is really easy to get involved as soon as you reach robot iot n you will have links to every were basically the dark the slack Channel and how to contribute obviously on
00:28:12 [W] Thank you for your attention.
00:28:05 [W] Thanks for being here. Today was a pleasure talking with everybody and hopefully we will see everybody in person next time.
00:28:13 [W] Have a good day and stay safe.
00:28:15 [W] Bye.
00:28:16 [W] Thank you.
00:28:17 [W] It's been great to be with you.
00:28:18 [W] Hopefully you've learned a little bit about rook and we hope to see you in weaveworks lack or other community. Thanks.
00:28:43 [W] I wanted to pull one in the question.
00:28:47 [W] One of the questions off the stack to answer how Rook ancef compared with distributed storage like Longhorn, so I guess to compare sort of directly to Longhorn.
00:29:03 [W] We Rook with Seth is able to provide.
00:29:11 [W] both or all of block file system and object storage to kubernative applications running in the cluster block storage as as much as Travis and
00:29:26 [W] With distributed storage like Longhorn, so I guess to compare sort of directly to Longhorn.
00:29:33 [W] We Rook with Seth is able to provide.
00:29:41 [W] both or all of block file system and object storage to kubernative applications running in the cluster block storage as as much as Travis and
00:30:22 [W] And or Longhorn only provides block storage.
00:30:28 [W] While all of them are open source, I think Seth is able to ride more flexibility in the storage type there. We also likes f-free dates even kubernative by a while and it has been tested in Enterprise.
00:30:43 [W] very large environments
00:30:46 [W] For for a long time including one of the largest clusters we know of at CERN the research organization.
00:30:56 [W] And it is yeah, it's very well tested.
00:30:59 [W] It's very stable.
00:30:59 [W] The data will be safe with Seth and there's also been a lot of time to provide for performance optimizations and Seth and I don't think Longhorn is really started focusing on
00:31:15 [W] That are been as well tested broadly.
00:31:18 [W] Yeah, thanks.
00:31:19 [W] I think I'll give it up to Travis or Alexander for the next question.
00:31:29 [W] Yeah, well, I'm on a screen.
00:31:32 [W] well talk to you want to talk about self-service data replication?
00:31:44 [W] And so one of the question was basically what happens if like there's no dead last in the cluster and force F.
00:31:56 [W] It's just one node gone. Like Seth will take care of replicating the data based off the chosen replication size to another node.
00:32:05 [W] that let's say you have five nodes and one of your notes goes down several take care and replicate the data that was on this fifth node to the add-on iot.
00:32:14 [W] Again, so that this replication sizes is guaranteed again.
00:32:18 [W] .
00:32:24 [W] I think we might be having some slight technical issues to Travis is unable to be seen so that's more or less for the questions if you have any other
00:32:39 [W] D we have to Rook slack if you want to go there and also the cncf slyke for the cube con right now.
00:32:44 [W] There was a rook Channel if you have any questions, so feel free to ask there. And yeah.
00:32:56 [W] Then anything else say Blaine or maybe Travis?
00:33:02 [W] Are you well?
00:33:08 [W] Well then thanks for thanks for being here.
00:33:11 [W] Thanks for watching Safe Haven.
