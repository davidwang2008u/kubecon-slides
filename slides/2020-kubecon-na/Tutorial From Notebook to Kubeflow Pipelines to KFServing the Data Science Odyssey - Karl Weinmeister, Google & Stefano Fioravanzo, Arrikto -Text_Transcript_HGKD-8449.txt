Tutorial: From Notebook to Kubeflow Pipelines to KFServing: the Data Science Odyssey: HGKD-8449 - events@cncf.io - Friday, November 20, 2020 3:11 PM - 59 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, my name is Karl Wilhelm meister, and I'm joined today by Stefano fear of on zoo.
00:00:30 [W] Hi, my name is Karl Wilhelm meister, and I'm joined today by Stefano fear of on zoo.
00:00:35 [W] We're going to talk about the data science Odyssey from a notebook all the way through to serving the model and everything in between we're going to cover a complete data science workflow and introduced several products that help along the way.
00:00:51 [W] Let's get started.
00:00:53 [W] The primary use case we're going to cover this hyper parameter optimization, which is the ability to search across a wide variety of parameters to find the parameters will create the best model.
00:01:07 [W] In general in this session.
00:01:09 [W] We're going to learn about simplifying your hyper parameter tuning as well as you're serving workflows with intuitive you eyes.
00:01:17 [W] The benefit is that you will accelerate your time to production with reduced training time and reduced time to build a process. You're going to get to the answers that you want faster and to deployment faster and finally we're going
00:01:33 [W] to review eyes
00:01:35 [W] The benefit is that you will accelerate your time to production with reduced training time and reduced time to build a process. You're going to get to the answers that you want faster and to deployment faster and finally
00:01:50 [W] Action between the teams that build the model in the teams that deploy the model.
00:01:56 [W] First what's kubeflow?
00:01:57 [W] So kubeflow is an open source project aimed at making emelda ployment simple portable across on-prem multiple cloudevents and because it takes advantage of kubernative
00:02:13 [W] So that you can run distributed training jobs. And so that you can serve your models in a way that can handle the wide variety of views that you might have.
00:02:25 [W] The use case is we're going to cover today cover quite a variety.
00:02:29 [W] The first is that we're going to cover a complex mlc system and scale. We're going to cover more than what they might call it a toy data set where it's small it fits in one virtual machine.
00:02:42 [W] We're going to talk about handling a more complex scenario.
00:02:46 [W] We're going to talk about rapid experimentation hyper parameter tuning.
00:02:51 [W] Seeing how all this can work in both hybrid and will declare workloads.
00:02:55 [W] And finally, you might see how these traditional Concepts and software development of continuous integration and deployment might apply in the machine learning world if you think about it, there's a lot of similarities there Concepts
00:03:11 [W] Sting the input data testing the output of your model making sure there's not a regression so that your model is better than it was before if you want to deploy it all of these things.
00:03:19 [W] You can codify into a pipeline that's reproducible and definable within your notebook.
00:03:29 [W] So the kubeflow platform has multiple layers.
00:03:33 [W] Let's walk through each of those first.
00:03:36 [W] You can bring whatever framework that you're comfortable with whatever machine learning library you've learned or want to learn you can use on kubeflow.
00:03:46 [W] There are a variety of what we call operators.
00:03:50 [W] They'll allow you to run your training jobs insert your models.
00:03:54 [W] In this layer, we see the heart of kubeflow which is a set of components for capabilities, like notebooks kale, which is going to help convert the
00:04:09 [W] The pipeline itself which helps to orchestrate your workflow Piper parameter tuning which is called cotton and several other components. We won't get into all these right now.
00:04:11 [W] You also see a variety of serving components as well as capabilities for logging and monitoring the whole stack of things that are necessary.
00:04:22 [W] And all of this is built on top of kubernative so that you're able to run your workload across multiple environments. And the way the kubeflow is designed.
00:04:33 [W] It will come naturally if you're comfortable with kubernative Concepts.
00:04:39 [W] All right, let's walk through the workflow.
00:04:41 [W] We're going to discuss in more detail today.
00:04:43 [W] So the first step is in the jupyter notebook itself, which is available in the kubeflow environment in this notebook.
00:04:51 [W] You're going to start with identifying the problem and some data exploration and Analysis next you choose an algorithm code your model.
00:05:01 [W] The next step is experimentation and model training.
00:05:06 [W] We're going to tune the model parameters.
00:05:09 [W] And then finally we'll be able to serve it and that will take us all the way through the key steps of the process.
00:05:14 [W] Let's look at how Kuma flow works.
00:05:17 [W] So let's start with the user interface.
00:05:22 [W] Here's a picture of the user interface.
00:05:24 [W] Pipelines experiments and so forth are available to you. You also have a dashboard that shows up where you see recent assets of worked with and shortcuts to common things that you might need to do in addition
00:05:30 [W] Face the command line is another way to interact with kubeflow.
00:05:20 [W] So there are actually two command lines.
00:05:24 [W] Do you might want to use so first there's the standard kubernative command line that allows you to do things like check the status of a trading job and check the pods that are running maybe get some logs things
00:05:39 [W] You also have the kubeflow command line utility which allows you to take templates that are EML files and apply them to customize your configuration
00:05:53 [W] Apis and sdks, so if you're going to build a model like one, you're going to create a hyper parameter tuning job. These capabilities are available through an SDK.
00:06:03 [W] And the final thing I'm going to mention before I turn it over to Stefano is that MLK code is one small part of the process. What we're seeing here in this diagram is from a famous paper about
00:06:18 [W] Machine learning systems, but what I want to point out is that these are things that you need to do when your code goes into production.
00:06:24 [W] It's inevitable.
00:06:26 [W] You will need to think about monitoring logging testing managing your resources making sure that you have a reproducible environment to help to build and manage that by working with the
00:06:41 [W] today you're able to work on top of that and leverage it so that you could focus on the data science as well as putting together a process in a pipeline that matches the goals of your business or your project
00:06:52 [W] Leverage all the great work that's been happening in this project.
00:06:53 [W] So with that I'm going to turn it over to Stephen.
00:06:56 [W] Thank you Carla for the great introduction to kubeflow.
00:06:59 [W] Now that you have a general idea of the architecture of kubeflow and all of the components that it provides you might wonder how you can actually use it to develop and then continually improve and validate machine learning models using
00:07:14 [W] And all of the components that it provides you might wonder how you can actually use it to develop and then continually improve and validate machine learning models using jupyter. Notebooks kale kubeflow pipelines
00:07:30 [W] Kill kubeflow pipelines and Brock, so this is exactly what we're going to do in this tutorial.
00:07:38 [W] But first, let's have a high level overview over the end-to-end workflow that we are going to implement.
00:07:44 [W] So first the the end user you will start from a jupyter notebook.
00:07:50 [W] So a local environment where you can develop your models or your training algorithms Etc. And then once you're done, all you need to do is to
00:08:00 [W] use scale to annotate notebook cells to convert this notebook into a scalable Pipeline and afterwards you can spin up a hyper parameter tuning job to run,
00:08:16 [W] Sort of thousands of parallel pipe lines. Once that's done. You can use again. Ko2 select the best model out of this massive workload and then
00:08:31 [W] Undp I serve this model from for everyone to use each step of the way is tracked by MLM D. So you can have an end-to-end need lineage of the workflow.
00:08:40 [W] Also each step is backed by BBC's and rocks Rock takes a snapshot of these PCS to have a complete time machine of your workflow.
00:08:53 [W] This is our agenda will start from deploy. Minikube F so are locally our development environment and then you'll learn how to convert notebooks to pipelines and then how to scale up this workflow with
00:09:09 [W] They're tuning and afterwards how to serve the best model.
00:09:10 [W] So check out this URL which will redirect you to a code lab where you can follow all of the steps of this tutorial at your own pace.
00:09:23 [W] Let's start by deploy. Minikube, F minikube have is our own portable and opinionated kubeflow distribution.
00:09:32 [W] So minikube have rounds seamlessly on gcp or on your laptop or on any on premises infrastructure.
00:09:39 [W] Minikube. F is a single node v m as a single note kubernative VM that runs kubernative trance kubeflow alongside rocks data management platform. It is super easy.
00:09:53 [W] Deploy, and in just 15 minutes, you have everything ready to go.
00:09:57 [W] Let's see how easy it is to deploy me clear for gcp. So I'm in the console of my gcp project and what I need to do to create a new Mini f is to head over the marketplace.
00:10:13 [W] So I'm clicking on Marketplace.
00:10:11 [W] and then search
00:10:13 [W] for minikube f
00:10:15 [W] here it is.
00:10:18 [W] Let's click and launch.
00:10:25 [W] All I need to do is to provide the name.
00:10:33 [W] And that's it.
00:10:36 [W] After clicking deploy, the deployment procedure will take roughly 15 minutes and you will be able to monitor all of the resources that we are deploying inside the virtual machine.
00:10:49 [W] For now, I'm just going to use a mini KF. I have already deployed. So once you start the deployment procedure, you will see this page and once it's complete this URL will be available to you.
00:11:01 [W] So I'm copying the password of my newly generated user and clicking on this link and I will be redirected.
00:11:12 [W] To make kubeflow homepage.
00:11:16 [W] So now that our minikube if-- is ready to use.
00:11:19 [W] Let's start with the fun part.
00:11:20 [W] That is how to convert The Notebook into a pipeline. But first, let's talk a little bit about kubeflow pipelines pipelines is the one of the most important components in the kubeflow platform.
00:11:33 [W] Through the same repeated steps, whether they be data processing data transformation model training and then serving and so on and so forth.
00:11:30 [W] So in this tutorial, we will try to simplify as much as possible the deployment in creation of this kind of workloads and how to make them completely reproducible will do this with kale and rock now
00:11:45 [W] Into a pipeline has several benefits. First of all, the notebook allows us to clearly Define.
00:11:41 [W] What is the structure of the resulting pipeline?
00:11:44 [W] And since we have multiple cells we can easily parallelize and isolate them.
00:11:51 [W] Also. Once you have a pipeline, you can apply data versioning and even different Hardware requirements for running them like running the training step in GPU and data processing in a CPU.
00:12:07 [W] Let's look at how the workflow changes when you apply when you use kale and Rock So before that, what you would need to do to create a kubeflow pipeline is to write your machine learning code review test it locally and then you would need to
00:12:22 [W] A specific DSL code to construct and Define the pipeline and then build the docker images to package your code and all of your dependencies and then build and upload the pipeline once
00:12:33 [W] If you bump into any bug or if you want to amend your code in any way, it would need to go all the way back to building new Docker images and then to redeploy the pipeline but now with Kaylin rock this workflow gets dramatically
00:12:41 [W] Deploy, the pipeline, but now with Kaylin Rock This World flow gets dramatically simplified because you just need to develop on your notebook tag your notebook cells with a very simple UI that you'll see later.
00:12:54 [W] tag your notebook sales with a very simple UI that you'll see later and then run the pipeline with the click of a button without any Docker image building and you can imagine how this workflow dramatically improves
00:13:10 [W] Workflow dramatically improves your your speed to production and iteration.
00:13:19 [W] Let's also talk about data management and how rock integrates with kubeflow.
00:13:24 [W] So this is a tf-x paper from 2017 the talks about high level component you view of a machine learning platform so we can see here where the tf-x component the
00:13:39 [W] Breastfeeding the overall platform and in our case. This is where kubeflow comes easy to provide this kind of components in a containerized way kubeflow also provides the integrated front
00:13:54 [W] You know containerized way kubeflow also provides the integrated front end to manage deploy and monitor all of the various applications. All of these is orchestrated by kubernative now.
00:14:08 [W] Orchestrated by kubernative now when you write vitess pipeline or a machine application on top of kubeflow.
00:14:15 [W] You need to you need some storage. So in general you would write a pipeline that is specific that interact specifically with a vendor API based on the kind of story technology that you're using
00:14:31 [W] Rectal comes in and provides a general purpose storage layer so that you can write pipelines that are not specific that don't have to interact with the specific storageos just access Superfast local storage
00:14:46 [W] That well, we've extended kubeflow to be data where and specifically to use kubernative Primitives PVCs persistent volume claims in all of its applications.
00:14:55 [W] We do this by integrated with the container storage interface and sitting on the side of the critical eye Opeth in this way.
00:15:04 [W] You can read and write data super fast from local from local data from local volumes.
00:15:12 [W] And in the meanwhile a tree Rock can take snapshots of your volumes immutable snapshots and share them across locations via an object store so you can reproduce your environment or
00:15:27 [W] Okay. So now we are ready to put our hands on a notebook and convert it to a pipeline.
00:15:34 [W] This is kubeflow Central dashboard the place where all of the kubeflow has component come together and what I can navigate between them as you can see on my left.
00:15:45 [W] I have notebooks volumes models snapshot pipelines and then experiments and runs where we group together objects coming from different applications that belong to the same place like
00:16:00 [W] Or hyper parameter tuning experiments as a data scientist. The first thing I want to do is to create my own development environment. So I'm going to create a new notebook server.
00:16:07 [W] It is a matter just of just a matter of assigning a name and in a matter of seconds.
00:16:14 [W] I'll have a fully full Jupiter lab environment ready to use.
00:16:49 [W] Great.
00:16:49 [W] So the first thing I want to do is to clone the cave repository to pull the example that we're going to run.
00:17:01 [W] So get blown HTTP get hub kubeflow.
00:17:10 [W] Cale Cale
00:17:14 [W] Great.
00:17:15 [W] Folder the examples folder and then the open box in cackle competition folder.
00:17:21 [W] So we created this notebook to work on the open boxing cackle challenge as we wanted to tackle the real world problem.
00:17:29 [W] The challenge is about trying to locate the weak spot of a messenger RNA structure to help create a stable vaccine this notebook contains a typical data science pipeline starting from data processing
00:17:44 [W] model training and then evaluation
00:17:42 [W] I won't go too much into detail. So what these notebook is actually doing because we don't care right now, but you will be able to play with it after the demo.
00:17:52 [W] So the first thing I want to do is to verify that they have all my dependencies available.
00:17:59 [W] So let's run the Imports apparently need to install some libraries.
00:18:05 [W] So let's do just that.
00:18:12 [W] Notice how I'm installing libraries here on the Fly.
00:18:17 [W] And you will see later what this means.
00:18:24 [W] Okay, so now everything should be running smoothly.
00:18:32 [W] Now since I know my notebook is working.
00:18:35 [W] I want to convert it to a pipeline with kale.
00:18:39 [W] So I'll head over here on the left on the kale panel and enable it you can see a bunch of colors and badges pop up in The Notebook.
00:18:47 [W] On the pipeline step and its dependencies.
00:18:46 [W] For example here the pre-processing datastax depends on the load datastax.
00:19:02 [W] That's all we need and if I click the compile and run button chaosmesh.
00:19:10 [W] Analyzes The Notebook validates it and then start to take a rock snapshot of the current environment to completely reproduce the environment.
00:19:23 [W] We are developing on so since I just installed on the Fly some python libraries, my pipeline will run regardless seamlessly without having to build new Docker images.
00:19:38 [W] Snapchat tip to to rock snapshots
00:19:30 [W] Then I can follow this deep links to see what is going on if I click here.
00:19:36 [W] I am redirected.
00:19:41 [W] To the kubeflow pipelines run page where I can see the new round that starts.
00:19:48 [W] Since this run will take a few minutes to complete. I will head over to an experiment I run previously and show you.
00:19:58 [W] The resulting pipeline after a few minutes of computation as you can see. We have a four-step pipeline.
00:20:08 [W] Each step corresponds to a notation in the in the original notebook.
00:20:14 [W] So multiple cells have been packaged inside this step by KO which is running in his in the exact same original environment with all of your dependencies.
00:20:27 [W] Also K creates machine learning method at executions for each pipeline step.
00:20:33 [W] This allows us to track and linkerd other entities that either belong or relate to the step itself.
00:20:39 [W] Some examples are the parent.
00:20:42 [W] Kfp run and artifacts that are consumed and produced by the step.
00:20:46 [W] You Sean that was created by KO.
00:20:38 [W] I can click here to go to the specific page.
00:20:43 [W] And notice for example how the Run ID. So the KF Tirana led the parent of this step execution is clickable.
00:20:53 [W] This is because we are standardizing on using Global you our eyes to reference and Link resources across kubeflow plication, and we are extending the kubeflow wise to interpret these unique identifiers your respectively to where the
00:21:08 [W] So clicking here will redirect me to the original run page.
00:21:16 [W] Likewise going back to the execution page I can scroll down and see that we took a rock snapshot.
00:21:26 [W] At the end of this step, so we are linking with kale this snapshot to the step execution and we can even navigate to The Rock UI.
00:21:40 [W] Now that we have converted a notebook into a single drum pipeline.
00:21:45 [W] We want to scale this up and optimize our model with Hyper parameter tuning.
00:21:49 [W] So, how do we do that?
00:21:51 [W] Well, we could start tinkering manually with the parameters in the notebook and run manually multiple pipelines like just like with it now and then compare the metrics and choose the best model
00:22:06 [W] Have converted a notebook into a single run pipeline.
00:22:04 [W] We want to scale this up and optimize our model with Hyper parameter tuning.
00:22:09 [W] So how do we do that? Well, we could start tinkering manually with the parameters in the notebook and run manually multiple pipelines like just like with it now and then compare the
00:22:47 [W] Got Deep to automate this process so calcium is the official kubeflow hyperparameters Junior. It supports several machine learning Frameworks from tens flow MX met by torch and others
00:23:02 [W] Flexible and we've integrated it with Gayle to run it from a notebook.
00:23:06 [W] So what we can do is we can go back to the notebook configure inputs and outputs in a pipeline so we can create a pipeline that's accepts input parameters and produces metrics that
00:23:21 [W] The to optimize over the resulting pipelines and then steal from The Notebook select the input hyper parameter tuning space the search algorithms and the goal afterwards just
00:23:34 [W] Don't directly from The Notebook we can create and submit then you cut them job.
00:23:38 [W] Let's see how we can do that.
00:23:41 [W] We are back to our original notebook.
00:23:44 [W] So the previous pipeline completed successfully and now we want to optimize it with Hyper parameter tuning as we said before we need two things a pipeline with inputs and outputs.
00:23:59 [W] Kubeflow pipelines.
00:23:58 [W] It's possible to create pipelines that have input parameters so that can vary between pipeline runs and output Matrix with kale creating such a pipeline is very
00:24:13 [W] You just need to create a notebook cell with some variables assignment and then annotated with the pipeline parameters annotation with this K. We make sure that the resulting pipeline will be parameterized with these values.
00:24:29 [W] with these values
00:24:29 [W] and then to create a pipeline metric.
00:24:32 [W] All I need to do is to select to choose which variable I want to basically print to Output from a pipeline in this case.
00:24:42 [W] I'll I'm choosing the validation loss here produced by my training procedure.
00:24:48 [W] and then on the bottom of the notebook I can just
00:24:55 [W] Print the validation laws and I'm not a tease cell with the pipeline Matrix annotation.
00:25:04 [W] And this will make sure that the pipeline will output a kubeflow pipeline metric.
00:25:10 [W] now if I want to start the hyper parameter tuning job to spin up hundreds of pipelines will I need to do is to enable this toggle and open up
00:25:22 [W] this cat dialogue that K provides
00:25:26 [W] you can notice how kale and ready recognize all of the variables that we've dug with the pipeline parameters and notation we have already prefilled this dialogue before but you could
00:25:41 [W] Ranges and list. However, you want it in all of the input parameters.
00:25:49 [W] Also, you can choose between various search algorithms and then the search objective that in this case.
00:25:55 [W] It's just the validation loss we chose before and we want to minimize it.
00:26:04 [W] So just like that with defined a hyper parameter tuning job and by clicking the compile and run button care will basically again validate The Notebook convert it and built it into a kubeflow pipeline Rock.
00:26:19 [W] Is taking now a snapshot of the current environment to reproduce the current state in the pipelines and then kale also creates and submits a new cat experiment where each cutting
00:26:12 [W] respond to a pipeline run
00:26:05 [W] Also, we can see here a live view over the current state of the cat experiment.
00:26:13 [W] So as soon as new trials and new rounds pop up and then complete you can monitor here directly from The Notebook.
00:26:27 [W] Now I can also click in this link and navigate to the khatib UI.
00:26:38 [W] This is a caterpillar why we have built from scratch following the design patterns that you may find already in other kubeflow plication.
00:26:50 [W] Within we've improved over the existing Kathy be y to show much more and much more detailed information over the status and various details of the experiments
00:27:05 [W] These experiment we take a lot of time to run.
00:27:06 [W] Let me show you something I run before so I'm going here on the left selecting experiments and then HP tuning to go to the homepage of our new category.
00:27:18 [W] Why as you can see I have my new experiment running and I can also see something around before we 50 trials and at a glance I can also see what was the best.
00:27:33 [W] Metric and the corresponding configure input configuration.
00:27:38 [W] Let's go in and see.
00:27:42 [W] So our new UI also exposes the state of the entire experiment with this nice graph where you can see color coded all of the trials that have executed and their parameter configurations.
00:27:58 [W] This plot is interactive so that you can also basically explore how the various parameter configurations behaved and how they
00:28:13 [W] They basically influence the output of the experiment.
00:28:11 [W] You can see at a glance.
00:28:12 [W] What was the best tryout configuration?
00:28:16 [W] What's the current state of the experiment?
00:28:20 [W] and then a list of all the trials and when you hover on a row you see the specific trial and its configuration in the graph?
00:28:33 [W] By scrolling down we can see at a glance also, which one was the best trial this one highlighted?
00:28:46 [W] Here it is.
00:28:49 [W] If I click on these pipeline icon here on the right, I will be redirected to the corresponding kubeflow pipeline run.
00:29:00 [W] This is because each trial corresponds to a specific pipeline run.
00:29:05 [W] So we want you to be able to navigate between you eyes seamlessly and Link together all of the entities.
00:29:15 [W] If I click on configure here, I can I also have some new khatib related entries and navigate back to the original khatib experiment.
00:29:27 [W] Objects and entities across kubeflow link together.
00:29:21 [W] Let's go back to the pipeline.
00:29:23 [W] So this is the pipeline that performed best in my original experiment.
00:29:30 [W] You can also see that there are these two icons.
00:29:34 [W] This means that these two steps have been cashed. In fact when running a cat experiment not all it's not all steps need to be rerun across across Pipelines.
00:29:47 [W] The first step in this pipeline that consumes the input parameters is model training.
00:29:53 [W] So we actually don't need to reload and reprocess the same data over and over again.
00:29:59 [W] So with Rock and pdc's we're actually just skipping these executions and starting from here from from that process data snapshot and let's actually go see visually
00:30:14 [W] The input parameters is model training.
00:29:54 [W] So we actually don't need to reload and reprocess the same data over and over again.
00:29:59 [W] So with Rock and pdc's we're actually just skipping these executions and starting from here from from that process data snapshot and let's actually go see visually
00:30:30 [W] MLM D sin scale is logging input and output artifacts for each step.
00:30:37 [W] We could go look here at the artifacts that the specific Rock artifacts that are associated to this step.
00:30:44 [W] So by clicking here, I can navigate to The Rock snapshot artifact saved into a Melody then to the lineage Explorer.
00:30:57 [W] And here this nice visualization allows me to understand that this rock snapshot artifact was produced by many many steps and consumed by many others
00:31:12 [W] Is that all of these pipelines are using the same step and so they were being cached now that we've run a massive hyperparameters during job. We want to take the best model and serve it with
00:31:27 [W] KF serving is kubeflow those component for serving models into production.
00:31:32 [W] It is based on K native and it allows serverless inferencing on kubernative.
00:31:37 [W] So keep serving provides several obstructions on top of various machine learning Frameworks and provides quite a few features like Canary deployments and scale to zero and much more.
00:31:50 [W] So what we want to do is to select the best trial of the previous cat experiment and restore a notebook out of a snapshot of that spiffe of that pipeline.
00:32:00 [W] So in this way will use Rock to restore the notebook from the best trained model and have the modern directly into notebook memory.
00:32:11 [W] Then we'll use a very convenient Kayla pi to serve this model in general creating inference Services is quite a bit tedious.
00:32:20 [W] As you can as you need to submit new CRS and maybe even build Docker images if you're using pre-processing Transformers, you will see how these old becomes much much easier with kale.
00:32:34 [W] Let's see how it's done.
00:32:37 [W] I'm back to the previous hyper parameter tuning experiment.
00:32:41 [W] and
00:32:42 [W] choose the last step in the pipeline.
00:32:45 [W] This is because I want to restore a notebook with the state after the model has been trained.
00:32:56 [W] So I'm heading over to visualisations where kale has produced a bunch of artifacts.
00:33:03 [W] So here I can see an artifact corresponding to the first snapshot taken before the step execution and then another artifact corresponding to a snapshot taken after the step execution.
00:33:19 [W] Bonding to a snapshot taken after the step execution.
00:33:23 [W] Let's take the first one.
00:33:25 [W] So I'll open this link which will redirect me to the Rock UI and specifically to these snapshot page.
00:33:37 [W] I'll copy this link, which I can take back to center dashboard.
00:33:45 [W] I'll open up notebooks.
00:33:48 [W] New server and I can copy here.
00:33:54 [W] This special URL which will make sure that my notebook is restored from this snapshot.
00:34:01 [W] Let's go this actually, let's call this surfing
00:34:14 [W] now that my notebook is up when I click on connect something interesting happens.
00:34:19 [W] So kale notices that we are restoring and notebook from a snapshot.
00:34:26 [W] So what it will do is it will automatically open up the original notebook and start restoring a marshalling data so that the current in the
00:34:42 [W] He's exactly the one that I would have found at that specific point in time in the in the pipeline execution.
00:34:50 [W] This means that I will find my model in memory.
00:34:56 [W] I will have in my python memory the best model trained out of the hyper parameter tuning experiment.
00:35:06 [W] That's it kale has completely completed the resuming of The Notebook.
00:35:14 [W] I can create a new sale here and verify that model. Actually, it's here.
00:35:21 [W] I haven't done anything. I just created a new notebook out of a snapshot and here it is my model in memory.
00:35:29 [W] Okay.
00:35:30 [W] So now I have my best model here in the notebook and I want to serve it. So K provides a very simple and convenient API to serve the model.
00:35:45 [W] So we can import kale.
00:35:35 [W] com serve with eels.
00:35:40 [W] import sir
00:35:42 [W] and now I want to use this function to serve on my model and it's just as easy as saying serve model, but then I also
00:35:58 [W] wound
00:35:50 [W] to pass some pre-processing function since KF serving supports both predictors and Transformers.
00:35:58 [W] We have a very handy function here in the notebook, which pre-process has features for us so we can then pass just raw data to the corresponding model server.
00:36:11 [W] Let me go here and redefine the function and the tokenizer. Does that this function uses?
00:36:25 [W] So I'm back down to the bottom of the notebook and I want to tell my API that the corresponding model server will have also to create a Transformer.
00:36:39 [W] Which will need to execute these.
00:36:44 [W] Process features function and I will need to pass as well.
00:36:54 [W] my tokina is er
00:36:57 [W] so that kale knows that the needs to package this object alongside dysfunction.
00:37:07 [W] So now that I'm running this we can see that ko-hoh recognizes the type of the model.
00:37:13 [W] It dumps it.
00:37:14 [W] After this, it starts. It will start taking Rock snapshot and afterwards creating a new inference service.
00:38:04 [W] Now that the inference service user is ready we can print.
00:38:11 [W] this object to see where it is served and by clicking here, we can navigate to our new models UI so we've built a brand new UI
00:38:26 [W] Bo's the entire state of KF serving to Monitor and expose all of the inference services that you may deploy as you can see here.
00:38:18 [W] I have an overview over where this model is exposed the fact that I deployed both the predictor and a Transformer and the entire State
00:38:33 [W] where these model is exposed the fact that I deployed both the predictor and a Transformer and the entire State I can have more details here
00:38:45 [W] have more details here about the various resources the fact that we are using a PVC where it is mounted and then we can also have a look at the various metrics exposed
00:39:00 [W] The specific pods that are running one for the predictor and one for the Transformer.
00:39:10 [W] These are live updating metrics that come from the underlying k- resources.
00:39:21 [W] We can also see logs.
00:39:23 [W] These are live logs coming from the predictor and the Transformer and we'll see how these update once we send the prediction and also the yamuna definition.
00:39:36 [W] And to have an overview of all the models.
00:39:38 [W] We have a nice home page where we can have a list we can see a list with summary of all era of all are running model servers.
00:39:50 [W] okay, so let's go back to the notebook and actually send try to hit this model and get a prediction so I will want to create a
00:40:08 [W] A data some a data structure to send to them to my model server for this.
00:40:15 [W] I am creating a dictionary with an instances key, which is a standard way to send data to attention for the server.
00:40:26 [W] servers
00:40:28 [W] okay, so let's go back to the notebook and actually send try to hit this model and get a prediction so I will want to create a
00:40:47 [W] Data some a data structure to send to them to my model server for this.
00:40:53 [W] I am creating a dictionary with an instances key, which is a standard way to send data to attention for the server and I'm going to pass our
00:41:39 [W] unprocessed EX
00:41:42 [W] public test data that is this is some this is data that we've defined before in the notebook. And that was actually a restored automatically by KO.
00:41:53 [W] So this is our data something that we want to to send.
00:42:01 [W] And then to send a request to our model server.
00:42:07 [W] It's just as easy as saying chaosmesh are going to predict data.
00:42:15 [W] Actually, it was KFC server.
00:42:19 [W] Okay, so sending a request and let's head here to the locks page and you can see live that our Transformer is actually getting some input data and process data and
00:42:34 [W] He's doing some magic and then the data is processed.
00:42:40 [W] All this happened without having to build any kind of Docker image.
00:42:46 [W] So kale was able to detect bars and package the input processing function and it's related assets create a new Transformer.
00:42:57 [W] I initialize it here and then use it.
00:43:03 [W] To actually process the raw data to be passed to the to the predictor and here
00:43:18 [W] My predictions should be quite a big response.
00:43:11 [W] So let's give Jupiter lab a bit of time to process it.
00:43:15 [W] Here it is.
00:43:19 [W] That was the last part of our tutorial.
00:43:21 [W] So now it's time to summarize what we've learned today.
00:43:25 [W] So you learn you have learned how to install and deploy minikube yet in a super easy way and in no time and then how to use a notebook to
00:43:40 [W] Don't and then using kale annotated and deploy it and convert it to a scalable pipeline afterwards how to use scale to even scale more this workload using hyper parameter during and
00:43:45 [W] how to use scale to even scale more this workloads using hyper parameter during and run tens or hundreds of pipelines using caching afterwards if learn how using Rock
00:44:00 [W] Afterwards if learn how using Rock and the snapshots that rock takes to reproduce every step of the way. You can restore a notebook from the pipeline to its previous state.
00:44:13 [W] So in this case, we took the best model train them out of the hyper parameter tuning job to find the model ready to use in the notebooks memory afterwards. We used a convenient to use Kale API.
00:44:28 [W] To serve this model directive from The Notebook to make it production-ready.
00:44:34 [W] All of this work though was backed by MLM D so that you could have a complete lineage of your work.
00:44:42 [W] So we've also seen some cool new you eyes like the Kata py and the models UI this you eyes are not open source yet, but we'll work in the next weeks and months to make sure that they will find
00:44:57 [W] Kubeflow also note that this is a pre-recording and we are still heavily working on this workflow.
00:45:05 [W] So by the time you see this video some of the features might change or might see Improvement.
00:45:10 [W] This is just a small sample of community contributions that with dammit erecto.
00:45:18 [W] We are investing a lot of time and effort into making kubeflow a great platform for machine learning. So from Jupiter manager, too.
00:45:26 [W] volume support minikube AF and authorization manifest installations across the board
00:45:34 [W] Now kubeflow is a beacon Vibrant Community backed by many both large and small companies.
00:45:43 [W] So if you feel like you want to join as a developer or as an end user here is a list of channels and public places that you can join and talk with us.
00:45:57 [W] Now all the new things that you've seen today are the the product of a joint effort of our internal team a Torito.
00:46:08 [W] So I want to really thank all of my colleagues who have contributed tons and tons of work to this.
00:46:13 [W] So from Ely has Dimitri's chemo Nats apostolos.
00:46:18 [W] konstantinos and Kris really? Thank you.
00:46:20 [W] Until the end of this tutorial so now we are ready to answer all of your questions.
00:46:41 [W] Hello, I guess we are live now and I hope you can hear me. Everyone calls are too.
00:46:51 [W] Okay, can you give your car?
00:46:53 [W] Okay.
00:46:57 [W] Can you hear me car right?
00:47:00 [W] Yes.
00:47:03 [W] Okay, great.
00:47:05 [W] So I hope you enjoyed this tutorial as you can see we started answering some of the questions in the Q&A chat.
00:47:15 [W] And if you have more questions we are here for you and note that we are also available on select channels on kubeflow tutorial on keep going tutorial on keep Connery kudo.
00:47:30 [W] She learned in you can find us there.
00:47:38 [W] If you want to replay this tutorial, you can head over to the original website and you will find all of the links to find the slides and links to provision
00:47:53 [W] from both gcp and AWS
00:47:58 [W] and maybe I could help answer one of the questions about gpus Stefano might have some other thoughts.
00:48:05 [W] I think that largely depends on what you know cloud or on-premise environment you're using when you can create a node pool that gpus and kubeflow can
00:48:20 [W] Cool. However, you have it configured a Google Cloud that I'm most familiar with.
00:48:33 [W] Be able to set tag so that you specify a certain GPU. So anyway, largely its abstracted away and handled by the kubernative infrastructure, but you can sort of Target specific gpus you want to use.
00:49:06 [W] In about the human in the loop human review component.
00:49:11 [W] That's a very good question.
00:49:13 [W] I haven't personally implemented that but I'm just thinking of some potential Solutions here.
00:49:19 [W] It's a very flexible framework.
00:49:21 [W] You could potentially have a step that waits for some kind of output from another process and continues the workflow when some flag is set. You can potentially also bridgecrew.
00:49:35 [W] In about the human in the loop human review component.
00:49:40 [W] That's a very good question.
00:49:41 [W] I have it personally implemented that but I'm just thinking of some potential Solutions here.
00:49:48 [W] It's a very flexible framework.
00:49:49 [W] You could potentially have a step that waits for some kind of output from another process and continues the workflow when some flag is set. You could potentially also bridgecrew.
00:50:04 [W] At the workflow into a couple steps may be one that goes all the way through to the human review process and then a second workflow that might get triggered. Once the human review is continued maybe stepping up with some other
00:50:52 [W] Maybe stepping out some other ideas.
00:51:02 [W] So we cover my I think I missed some of your throat now. That is he right. All right. My connection was very very good.
00:51:17 [W] There was another question about performance comparison with tf-x.
00:51:22 [W] I don't have any stats on that.
00:51:26 [W] I do know that you know, the kubeflow pipelines infrastructure is very aligned with tf-x.
00:51:35 [W] You know, you're there are components that allow you to invoke components like, you know schema Jen statistics General that that's in the tf-x framework, so I haven't done
00:51:47 [W] head-to-head comparisons though
00:51:54 [W] Then I see a question about vagrant minikube from vagrant. And yes anything minikube from background will be updated very very soon.
00:52:04 [W] to the same version that we are now offering on this EP and AWS and
00:52:11 [W] Main concern is to allow users not to worry at all about Cube cattle.
00:52:19 [W] So with kale we're basically providing apis and processes to abstract away from Cube Cato and low-level kubernative interaction.
00:52:35 [W] But then to cattle is available in the notebook if you want to to explore the the kubernative objects yourself.
00:52:52 [W] SKF be using Argo tekton in the background So currently give peas using Argo even though there is a undergoing effort to support tekton as well.
00:53:08 [W] And eventually kfb will be able to work seamlessly with both of them in the future.
00:53:24 [W] And then there's another model here I go out of at least with Wonder too.
00:53:32 [W] No, but we will definitely push it Upstream in the coming months and it might be part of 123 or later version. We still we still don't know.
00:53:47 [W] In the coming months and it might be part of 123 or later version. We still we still don't know.
00:53:57 [W] At least Carl go ahead.
00:53:58 [W] Well now I was just answering the question about integrating.
00:54:04 [W] Kfp runs within existing CIP line there, you know with the SDK or able to trigger runs so you could potentially, you know, run your existing CIP wine and
00:54:19 [W] Cri-o pipeline there, you know with SDK or able to trigger runs so you could potentially, you know, run your existing CIP wine.
00:54:31 [W] And as one of those steps call the kfp pipeline that continue
00:54:42 [W] I think that's all the questions for now, but they are coming in here.
00:54:51 [W] Yeah, I'm just wait a little bit more. We still have some work time.
00:55:42 [W] Okay. So question if it's possible to use cave without Rock.
00:56:24 [W] Okay. So question if it's possible to use cave without Rock.
00:56:33 [W] Yes rock is not a prerequisite to using kale. You can experiment with kale in your own kubeflow cluster note that
00:56:49 [W] Without Rocky won't be able to seamlessly transition from a notebook to a pipeline with snapshots because that's where Rock comes in. So you would need to in some way build your own Docker images or
00:57:14 [W] kubeflow cluster know that
00:57:19 [W] without Rocky won't be able to seamlessly transition from a notebook to a pipeline with snapshots because that's where Rock comes in. So you would need to in some way build your own Docker images or
00:57:35 [W] Code and data from your notebook to the pipeline.
00:57:40 [W] And this process is made seamless by Rock in this case.
00:57:57 [W] I think we're close to running out of time if this wraps up automatically you can find us in the slack Channel. We're going to be in the number-two coupon tutorials the channel after this.
00:58:08 [W] I hope it was all helpful for everyone.
00:58:11 [W] Really appreciate you attending on this Friday.
00:58:15 [W] Let's see.
00:58:18 [W] We have a couple more questions. I don't know Stefano if you want to maybe answer them here or Slack.
00:58:28 [W] I would say let's let's move off the conversation to slack so we can have
00:58:35 [W] threads and more lively conversation going
00:58:41 [W] because I guess it's good.
00:58:42 [W] This will close any time soon.
00:58:44 [W] Yeah.
00:58:51 [W] So, please do all of the unanswered questions post them on slack and we'll address them there.
00:58:59 [W] It was a it was a pleasure and thanks for attending.
