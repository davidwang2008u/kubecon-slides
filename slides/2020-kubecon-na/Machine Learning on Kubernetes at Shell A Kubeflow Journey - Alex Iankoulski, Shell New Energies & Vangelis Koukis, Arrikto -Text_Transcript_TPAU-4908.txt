Machine Learning on Kubernetes at Shell: A Kubeflow Journey: TPAU-4908 - events@cncf.io - Wednesday, November 18, 2020 3:52 PM - 33 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Welcome to our joint presentation that was prepared for you through a collaboration between shell and a rectal here.
00:00:07 [W] And a technical leader for mlr orchestration that shall new energies and I'm also a doctor captain and I'm Gil is cookies CTO and co-founder at a little before we get into the content.
00:00:19 [W] I am required to show you this disclaimer slide which points out that when were discussing Nats zero emissions.
00:00:25 [W] We use emissions or estimations and projections. The actual results will depend on many factors including the choices that society and our customers make
00:00:35 [W] In today's presentation, we will first take a look at the business context and use cases.
00:00:41 [W] We aspire to solve then we'll discuss the technical challenges associated with these use cases.
00:00:45 [W] We will share some lessons learned.
00:00:47 [W] We will dive deeper into a few of the technical details.
00:00:51 [W] And of course, we'll look forward to showing you an interactive demo of an end-to-end machine learning workflow and last but not least. We'll have a live Q&A session.
00:01:02 [W] Let me start by introducing shall Global we're a group of energy companies with more than 80,000 employees in over 70 countries.
00:01:10 [W] We use Advanced Technologies to innovate and help build a sustainable energy future in 2015 in Paris France 195 countries signed an accord to reduce carbon emissions in order to limit the rise of global
00:01:25 [W] live Q&A session
00:01:25 [W] Let me start by introducing shell Global.
00:01:28 [W] We're a group of energy companies with more than 80,000 employees in over 70 countries.
00:01:33 [W] We use Advanced Technologies to innovate and help build a sustainable energy future in 2015 in Paris France 195 countries signed an accord to reduce carbon emissions in order to limit the rise
00:02:02 [W] Under 2 degrees Celsius above pre-industrial levels to achieve this goal Humanity must drastically reduce greenhouse gas emissions reaching a point of nats zero emissions within the second half of this century.
00:02:16 [W] Experts agree that Global energy demand is likely to double by 2050 compared to the demand in the year 2000 and the same time greenhouse gas emissions must be drastically reduced in order to get climate change under control.
00:02:30 [W] That is why shell has set itself an ambition to become by 2050 or sooner a net zero emissions energy business.
00:02:39 [W] This is accompanied by our ambition to provide a reliable electricity Supply to a hundred million people.
00:02:46 [W] in the developing World by 2030
00:02:50 [W] the world's Energy System is changing shell is investing in more lower carbon technology.
00:02:56 [W] This includes Renewables such as wind and solar and your Mobility options such as electric vehicle charging and hydrogen and an interconnected power grid shell is investing up to 2 billion dollars a year in its new energies business,
00:03:12 [W] Is from developing more and cleaner energy solutions.
00:03:15 [W] I work at Shell new energies building the energy platform and digital foundation for the business.
00:03:21 [W] I invite you to follow this link on the slide here for a short introduction video.
00:03:28 [W] In the context of shell new energy is digital Foundation.
00:03:32 [W] There are several areas where machine learning can shine when applied to solving business challenges.
00:03:37 [W] Some of these areas are listed here.
00:03:39 [W] They range from operational organization to energy trading to increasing value for our customers where there is machine learning.
00:03:47 [W] There is Need for machine learning orchestration. And with that come on number of technical challenges that we have been working on solving together with a rectal
00:03:58 [W] Next let us discuss some of these technical challenges and our approach to solving them.
00:04:04 [W] In the next couple of slides will enumerate nine different groups of charges.
00:04:09 [W] I'll elaborate on the context and ask my co-presenter to comment on our Solutions infrastructure needs to be Cloud native, but also cloudevents snyk, so we don't have to rewrite code if our platform needs to run on a different cloud
00:04:24 [W] Odds and the sum and the same time deployments need to be reproducible auditable and reversible because we want to know exactly what runs in our systems and how it got there scale needs to cover the entire Spectrum from micro
00:04:26 [W] Our software must run well in a single host laptop and as well as a large scale on the ground tooling should be webassembly don't require anyone to install software locally and self-service.
00:04:32 [W] so we can keep our Ops teams focused on automation rather than repetitive manual tasks compute must be treated as an ephemeral resource that is available when needed but goes away when not in use the workloads.
00:04:47 [W] It's running on these compute resources must be resilient and reproducible able to serve their purpose. Even if underlying infrastructure changes over to you can goes so should gain to us with these challenges and here the solutions we came up with and
00:05:02 [W] It's the workloads running on these Computer Resources must be resilient and reproducible able to serve their purpose even if underlying infrastructure changes over to your van Gogh's so she'll gain to us with these challenges and here the
00:05:28 [W] For infrastructure, we opted for other provision kubernative clusters and use the kubernative CPI everywhere.
00:05:34 [W] The kubernative CPI is a common language.
00:05:37 [W] We used to working straight workloads regardless of the clouds region were speaking to then we build on kubeflow as the de facto way of running machine learning workloads on top of kubernative and more specifically the Arabic Telco flows stack which combines kubeflow with
00:05:52 [W] The management software will talk more about drug later on.
00:05:56 [W] How do we Deploy kubeflow on top of different kubernative clusters?
00:06:01 [W] We follow a gitops based methodology everything starts from a git repository.
00:06:07 [W] We have opted to steer away from the KF cutting tool so we can deploy in the simplest kubernative knative way of applying manifest.
00:06:15 [W] And hence. We can support seamless upgrades with robotics on scale kubernative can run from a single laptop.
00:06:23 [W] example, there's micro kubernetes Oracle delivers.
00:06:26 [W] Own sigil no deployment of kubeflow called minikube flow of air encourage you to try it out with shell we run on managed kubernative Services more specifically eks on AWS where we cannot steal seamlessly either up or down
00:06:41 [W] On on to link, we followed devops practices and try to shift left users run their own code servers. For example Visual Studio code.
00:06:47 [W] They run their own Jupiter labs.
00:06:48 [W] They manage their own code and I get repository for example provided by gitlab eventually the run their own workloads on kubeflow and finally on compute being able to run with reputable results means using
00:07:04 [W] Images inside kubernative spots, but this is not the end to install and reproducibility because what about your actual data the things that you use as input more on this later on so more challenges Alex
00:07:12 [W] Things that you use as input more on this later on so more challenges Alex sure storage is easy fast and cost efficient so we can decouple compute and storage challenges data should be secured
00:07:22 [W] Needs to be fast and cost efficient so we can decouple compute and storage challenges data should be secured all times yet available for authorized users and version so we know what changes happen over time and can scroll back in time as
00:07:39 [W] available for authorized users and version so we know what changes happen over time and can scroll back in time as needed security must be end-to-end enterprise-grade an integrated with our corporate identities and finally
00:07:54 [W] Transparent and non-disruptive to our users work. We also want for any user to be able to easily orchestrate reproducible workloads as opposed to relying on a dedicated orchestration engineer.
00:08:05 [W] So for storage we moved from a shared file system over NFS that is EFS on AWS local Superfast mounted file systems provided by Rock.
00:08:18 [W] We will focus more on this in the next slide, but the basic idea is local storage is
00:08:23 [W] Super flexible your conscience taxes local files and the performance is orders of magnitude faster.
00:08:30 [W] But what about data management?
00:08:30 [W] So rock sits on the side of this local storage and gives you thousands of pointing down in snapshots think of a time machine.
00:08:38 [W] say we snapshot once every 10 minutes for your notebook servers so you can go back in time and reproduce the data for all experiments you run or a snapshot happening at each step of a pipeline so you can know exactly what series
00:08:53 [W] Events led to the creation of a specific model and then you can investigate any biases this functionality gives end-to-end reproducibility for workloads on security. We have implemented single sign-on single log out centralized
00:09:09 [W] With radiation against different namespaces.
00:09:09 [W] What we generally do is we create a private named space per user but we can also support shared name spaces where different users share access to the same set of resources.
00:09:18 [W] For example, the same pipeline runs and finally on orchestration WE implement mlrs.
00:09:25 [W] We combine the power of get kol are open source to which converts notebooks into production pipelines and then orchestrates hundreds or thousands of these pipelines for hyperparameters.
00:09:36 [W] Link and eventually serving the glue that brings all this together is the metadata record at each phase of the workflow.
00:09:45 [W] If you want to know more about analogs a rectal has a joint workshop with Google named from notebook to pipelines to give serving the data science Odyssey.
00:09:56 [W] It's on November 20th.
00:09:58 [W] Ten past twelve Pacific and I encourage you to attend it.
00:10:05 [W] So let's take a deeper dive into three of the points.
00:10:09 [W] We just mentioned gitops paste deployments storage and orchestration of end-to-end workflows.
00:10:17 [W] What is gitops here's a deployer on our left hand side and here's a kubernative cluster where they want to deploy kubeflow on the right hand side gitops is all about a git repository it sits in the middle the deployer commits
00:10:32 [W] To the cluster as llamo manifests and they only apply committed manifests to their kubernative cluster.
00:10:29 [W] They can use standard kubeacademy apply or even customize.
00:10:34 [W] We'll talk more about customized later on.
00:10:35 [W] Why is this important? Because they treat their infrastructure as code the state of their infrastructure corresponds to a commit in the repository and their infrastructure goes from commit to commit
00:10:49 [W] but most of the time the actual manifests come from a vendor in this case.
00:10:54 [W] It's us a rectal who produce the electric influenced stackrox.
00:11:19 [W] Issues like a because we use customized with them on top of the vendor commits. Eventually. The deployer user is customized to combine these manifests into the final desired state which they apply onto the kubernative cluster
00:11:34 [W] Makes upgrades a breeze.
00:11:22 [W] Let's assume the vendor is at version V2 and the deployment has committed deployment specific configuration as cognates the one on top of V 2 at some point the vendor produces V3 the deployer pools
00:11:38 [W] is there changes so they now sit on top of the three as a new Comet the one prime they have essentially upgraded their infrastructure and can now reapply
00:11:44 [W] next let's talk about storage and data management shell used to run over DFS on AWS and managed file system / UF / NFS this solution has two main drawbacks.
00:11:57 [W] There is no data management.
00:11:59 [W] No backups.
00:12:00 [W] anyone can change the shared state. So there really is no way to reproduce able to reproduce.
00:12:05 [W] I'm sorry an experiment after a while because the data has moved on and secondly performance suffers Baseline performance.
00:12:14 [W] EFS depends on its size and it's 50 kilobytes per second per gigabyte.
00:12:20 [W] So let's use a hundred gigabyte EFS file system as an example compare these two running with rock which uses the local storage as it comes with your instances rocks Souls the reproducibility problem by
00:12:35 [W] Did seen application consistent snapshot so you can go back in time and reproduce the results.
00:12:41 [W] Every snapshot is essentially a git commit for your data.
00:12:45 [W] Not just your code Rock archives the snapshot into object storage.
00:12:49 [W] For example, x 3 in the case of Amazon on performance if we compare a standard M5 d dot for extra-large instance.
00:12:57 [W] The difference is huge local nvme gives you 16 times the iot rations per second for reads more than
00:13:04 [W] 400 times the IOP rotations per second for rights bandwidth is 18 to 21 times better and the aggregate numbers scale with a number of instances.
00:13:18 [W] Let's take a minute to talk about how Rock Works underneath.
00:13:20 [W] Here's a single kubernative note Rock runs as a pod when the side of your workload also running as Parts these parts have a direct path path a via the colonel to local storage.
00:13:35 [W] Seats on the side it monitors the iot relations. So you can retrieve the change data and produce a new snapshot which it archives into S3 this path be at a later time rock can restore data from this snapshot path. See
00:13:43 [W] This like this is rock running going to kubernative cluster on multiple nodes all coordinating access to an object store within the same region.
00:13:45 [W] But it really becomes interesting when you look at Rock Running on multiple regions as independent clusters.
00:13:51 [W] In this case each one of the kubernative Clusters accesses our local independent objects.
00:13:57 [W] So let's look at this example where we rock runs on Amazon's own top left a Google Cloud Zone bottom left on Trend top, right even on a laptop bottom right
00:14:12 [W] These ridges together the other Rock registry and it allows them to synchronize their snapshots in a peer-to-peer fashion over the Blue Links.
00:14:14 [W] So why would you need to synchronize your snapshots?
00:14:17 [W] that is your data commits because generally different parts of our data science workflow run on different locations. For example, the experiment on one location local in this case. Then you move to the cloud for example
00:14:32 [W] It's game then you run inference we production and this happens at multiple locations.
00:14:30 [W] We are Rito have extended kubeflow.
00:14:33 [W] So it works with kubernetes volumes directly underneath and then we take care of synchronizing data commits. All of these volumes across locations.
00:14:42 [W] And with this let's move to Alex so we can talk about security and isolation.
00:14:51 [W] Thanks, Ryan Harris Enterprise standards require that all services and applications are secured.
00:14:57 [W] We need to use the same user identity throughout all secured assets. Therefore.
00:15:03 [W] We've implemented single sign-on one lesson. We learned in the process is that it is not necessary to integrate all of our applications with the Enterprise identity provider instead. We can use a self-hosted IDC provider which in turn federates
00:15:18 [W] IDP we use gitlab, but other oid see providers can be used as needed in the case of kubeflow.
00:15:25 [W] This means that we are able to provide each user their own isolated namespace as well as a shared name space where users can collaborate on projects to have end-to-end enterprise-grade security the journey
00:15:40 [W] Further in addition to single sign-on we have implemented a single log out functionality which allows us to log out directly from our application screens Alaskan girl is to explain how we're able to do this within kubeflow and then dive into the
00:15:40 [W] Thanks, Alex.
00:15:38 [W] So we have extended kubeflow.
00:15:42 [W] So it becomes one more all IDC client via component. We call the oid see of service. This component integrates closely with is Theo and more specifically the envelope rocks inside is still
00:15:57 [W] You to learn more about this architecture by following the kubeflow docks and the number of blog posts we have made.
00:16:04 [W] Finally, let's look at an end-to-end workflow made possible with are confused step.
00:16:10 [W] Here's a data scientist and they have a description of their mlperf line inside that Jupiter lab notebook each one of the cells or a group of cells corresponds to a different step of the pipeline for example data pre-processing.
00:16:25 [W] building model training model evaluation steps 1 to 4
00:16:28 [W] kale are open source to packages and notebook or even a python script which is what will demo later on into a kubeflow pipeline.
00:16:36 [W] Here is the compiled kubeflow pipeline.
00:16:40 [W] Kale, then orchestrates this compiled pipeline for hyper parameter tuning. It spawns hundreds or thousands of runs to find the best combination of hyperparameters.
00:16:52 [W] Finally, it chooses the best model produced from hyper parameter tuning and it serves it via kubeflow serving note how we maintain metadata for each part of the process
00:17:07 [W] MMD componentconfig
00:17:05 [W] also note how every step uses Rock provided volumes to maintain all of its input and output data.
00:17:12 [W] This is important because Rock and then snapshot these volumes at each step of the workflow for each one of the individual pipeline runs thinly and then it can maintain thousands of snapshots.
00:17:26 [W] This means we can reproduce reproduce.
00:17:29 [W] I'm sorry each and every one of these pipe and runs forever.
00:17:36 [W] So, why did we choose to work with good form?
00:17:41 [W] Well, the reason is to choose kubeflow are obvious. It runs natively on kubernative in a scalable and reliable way.
00:17:48 [W] It is secure and integrates with external identity providers.
00:17:51 [W] It runs both on your laptop and in the cloud you can use the open source version or purchase a commercial version with Enterprise support.
00:17:59 [W] It integrates with the way you currently do your work stream lines and accelerates your mly Rook flows one example would like to share here is a case when we needed to build 10.
00:18:11 [W] And models normally the time required to do this work manually would involve about two weeks of coding and four weeks of execution with kubeflow.
00:18:19 [W] We were able to write the code in less than a day and build all models within 2 hours seeing is believing. So let us jump into my favorite part of this presentation.
00:18:30 [W] Work stream lines and accelerates your mly workflows one example would like to share here is a case when we needed to build 10,000 models normally the time required to do this work manually would involve about two weeks of coding and
00:19:03 [W] We will use roleplay to demonstrate an end-to-end machine learning pipeline orchestration workloads.
00:19:31 [W] The short of it is that the state of California has a goal to generate sixty percent of its electrical energy from renewable sources by the year 2030 and become the first carbon neutral state in the US by 2045.
00:19:45 [W] I'm really excited about this and built a project that predicts what percentage of California's energy will be generated from renewable sources over the next 30 Years.
00:19:57 [W] You can see all the details in the git repo that I shared with you.
00:20:01 [W] Wow. That's pretty cool.
00:20:03 [W] How can I help I need this project to be orchestrated as a machine-learning pipeline so I can run it when I want to check on the progress towards achieving these carbon neutrality goals.
00:20:15 [W] I've heard that we need to sit together and iterate on building a container so you can then build the pipeline then use the pipeline tufin.
00:20:27 [W] Is not working we need to go back to step 1. I'm really nervous because this sounds like it will take a while and I need to be done before 6 p.m.
00:20:35 [W] Today.
00:20:35 [W] Well, don't worry because what you just described is how we used to build orchestrations, but you don't have to do this anymore with the new are weak to kubeflow stack.
00:20:48 [W] can do everything very quickly by yourself.
00:20:52 [W] Oh, that sounds great.
00:20:54 [W] Would you walk me through it? Sure.
00:20:56 [W] It's easy.
00:20:56 [W] So let's start by logging onto kubeflow using your single sign-on to do that.
00:21:06 [W] Okay.
00:21:08 [W] So now are you looking there you are.
00:21:14 [W] There's a shared namespace top left. The shared namespace is kubeflow could con. Yes, so we share it between us then go to notebooks.
00:21:29 [W] You can always create a new server, but I've already created one. I've closed your repo. So let's connect to it.
00:21:40 [W] Your goal should be there.
00:21:41 [W] I see that my code is already here.
00:21:45 [W] So have a look at it.
00:21:47 [W] What is this pipeline?
00:21:53 [W] choose the changes you need to make
00:22:05 [W] okay.
00:22:08 [W] Let's take a look at this.
00:22:11 [W] So what I see here is that you've imported Pipeline and step decorators from the KL SDK and then just use them in the code to annotate my functions. Nothing else.
00:22:26 [W] You just annotate the functions in your existing code what you already run?
00:22:32 [W] All right, but wait. Well, this will my existing code still work the old way. I usually just run it locally.
00:22:41 [W] That's all you need to do.
00:22:48 [W] All right, that's that.
00:22:51 [W] So at this point your code runs locally inside your Jupiter lab.
00:23:01 [W] Clay is just run it locally. That's all you need to do.
00:23:10 [W] All right, that's that.
00:23:13 [W] So at this point your code runs locally inside your Jupiter map.
00:23:40 [W] I see that it looks a little different but I think that's because it's being run by a kale.
00:23:44 [W] That's pretty cool it ran and what do I need to do to run this in kubeflow?
00:23:49 [W] Just run the exact same thing but add - - kfp at the end of the command line This Is It
00:23:59 [W] Okay.
00:24:01 [W] Seems to be running.
00:24:01 [W] What is the joint no running against kubeflow pipelines at this point kale compiles your code into a kubeflow pipeline and it pushes it to cook for pipelines. It uses Rock to take a snapshot of your Jupiter lab volumes automatically,
00:24:17 [W] Add - - kfp at the end of the command line.
00:24:21 [W] This is it.
00:24:24 [W] Okay.
00:24:26 [W] Seems to be running.
00:24:27 [W] What is he doing on running against kubeflow pipelines at this point kale compiles your code into a kubeflow pipeline and it pushes it to kubeflow pipelines.
00:24:37 [W] It uses Rock to take a snapshot of your Jupiter lab volumes automatically.
00:24:42 [W] So your pipeline can be completely reproducible in terms of both your code and your data you can actually go to kubeflow and see the result see the pipe and run and I'd like to know more about how
00:24:56 [W] Actually solves the problem, right? So let's take a look at the Run.
00:25:05 [W] I see that the pipeline was generated here and I recognize all of the steps from my workflow. So first we scrape the data from Kaiser's.
00:25:20 [W] Public report on energy usage and then we process that data.
00:25:25 [W] We split the data set into training and testing set then we use a few techniques to build models and rank them and then we train the best model with the full data set
00:25:41 [W] Build models and rank them and then we train the best model with the full data set and predict 30 years into the future.
00:25:38 [W] Let's see.
00:25:40 [W] Let's see the actual divine right? So this is the actual pipeline the one that you just submitted running.
00:25:47 [W] I like this a lot.
00:25:48 [W] I feel like I'm in complete control of running my code the same way outside of kubeflow and inside kubeflow and the only thing I need to do to control that is add that - - kfp flag to my command exactly.
00:26:25 [W] Cool. Now that the pipeline around is finished we can check the results.
00:26:34 [W] What do these results mean?
00:26:38 [W] Well, the model seems to show that the goals are pretty ambitious.
00:26:41 [W] But the margin of error is very large because we're predicting pretty far into the future.
00:26:47 [W] I think the outcome will depend on the little Stripes that are done every day towards achieving California's renewable energy goals, it would be interesting to watch and see how this prediction changes over time and that's why I'll schedule this pipeline to
00:27:02 [W] Every month, I'm jealous.
00:27:04 [W] I can't believe how easy this whole process was and I'm so happy. It didn't take much time at all.
00:27:11 [W] I'll be orchestrating all my MLG workflows only this way going forward. Thank you so much Alex. I think you can now log off and we call this project done way ahead of your headline.
00:27:30 [W] and this is the kind of experience our team's Love Today We Share challenges and Lessons Learned while building scalable and highly available mlperf structure applied to real-world use cases we discussed how kubeflow
00:27:45 [W] The erectile Stackhouse us solve these challenges and demonstrated of data science workflow from Zero to Hero made possible by kubernative and kubeflow.
00:27:47 [W] We want to say a big. Thank you to our teams.
00:27:52 [W] Quickly share a few references and let you know that you can reach us offline here with any follow-up questions, but now we have a few minutes to take live questions from the audience. Thank you.
00:28:20 [W] Hello, we have a queue of some questions here.
00:28:28 [W] I'm going to start reading the ones from high priority.
00:28:35 [W] So let's first talk about question from Ricardo Roca.
00:28:44 [W] He's asking are the different kubeflow instances in each cloud in your example, totally independent with similar name spaces or configs pushed to each via guitar amps.
00:29:00 [W] So I think I can take this.
00:29:03 [W] One strong high priority. So let's first talk about question from Ricardo Roca.
00:29:01 [W] He's asking are the different kubeflow instances in each cloud in your example. Totally independent.
00:29:09 [W] With similar name spaces or configs pushed to each via guitar amps.
00:29:18 [W] So I think I can take this.
00:29:33 [W] We use Rock to manage data on all deployments so we can support workflows that span different regions in a seamless way.
00:29:50 [W] so, I don't know if the other attendees heard the entire answer then go is their audio cut out a little bit, but
00:30:07 [W] so, I don't know if the other attendees heard the entire answer bengalis are audio cut out a little bit, but
00:30:22 [W] Questions we can answer in detail later on on the slack Channel.
00:30:28 [W] I'm going to move on to the next question, which is from Leonard.
00:30:34 [W] Okiya.
00:30:35 [W] Do you specify step dependencies in the decorators when you're annotating the functions?
00:30:46 [W] So I hope my audio is good.
00:30:48 [W] If not, they'll switch but let's see what happens. So
00:31:11 [W] Okay, it appears that we do have some technical challenges with the Audio I can take this question in the meantime while Vangelis comes back online.
00:31:27 [W] Dependencies between steps are determined automatically by the functions that you specify in your
00:31:41 [W] Method and you do not have to specify those in the steps.
00:31:41 [W] However, using the kale plug in the cable extension for Jupiter lab there is capability that allows you to specify which steps should be executing
00:31:56 [W] it after others that way defining your entire Grant find draining your pipeline in general k l will try to parallelize
00:32:10 [W] Workloads and make your pipeline as efficient as possible.
00:32:26 [W] So
00:32:29 [W] I am going to answer the next question from Gabriel Barbosa.
00:32:36 [W] It's possible it.
00:32:38 [W] is it possible to run by local notebook to kubeflow and the answer is yes, the only
00:32:51 [W] The only requirement for that would be to have access to the kubeflow API the example we showed was for a local notebook that
00:33:06 [W] The kubeflow cluster is able to submit the kubeflow pipeline but doing that externally is possible as well.
00:33:13 [W] So we are at the time and there are several more questions that we couldn't answer.
00:33:21 [W] However, we will continue answering questions on the slack Channel.
