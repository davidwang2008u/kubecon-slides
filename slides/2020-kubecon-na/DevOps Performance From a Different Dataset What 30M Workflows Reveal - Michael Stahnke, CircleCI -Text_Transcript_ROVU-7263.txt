DevOps Performance From a Different Dataset: What 30M Workflows Reveal: ROVU-7263 - events@cncf.io - Wednesday, November 18, 2020 3:52 PM - 43 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everybody.
00:00:00 [W] I'm Michael stanky.
00:00:01 [W] I run the platform engineering organization at Circle C. I and I want to talk about what we've learned from looking at the Circle C i data about how teens are performing.
00:00:11 [W] So I'm Mike.
00:00:13 [W] We know what's the problems that we're trying to ask about?
00:00:15 [W] What are we trying to learn a bit about the data diving in sync with the raw data tells us and then a little more about insight about how we look at that data and what we can derive from it what we can maybe hypothesize about it and talk about Trends.
00:00:28 [W] We're seeing in the industry and other influencing factors.
00:00:30 [W] So beginning with the setup.
00:00:33 [W] setup. There's a lot of devops reports out there that talk about high performing teams and what these metrics are and what happens and to me those have always been fascinating. They've been some of the best research available in our
00:00:43 [W] Field I've helped author a number of these at this point.
00:00:46 [W] I worked at puppet for a long time and they were the people that started this and I worked with them on a number of these questions and Analysis and kind of how teams are working.
00:00:56 [W] And what makes a team High performing versus that seems maybe medium performing or a team that's less successful. And you know, you asked a whole bunch of questions and you learn a whole lot about the way teamwork teams work.
00:01:07 [W] Well, that is an interesting way to approach this.
00:01:11 [W] But now that I'm at Circle C II had a whole new opportunity of how to do team analysis. And that's looking at actual Behavior looking at data.
00:01:19 [W] And so the way that I think about this is this is performance derived versus performance described.
00:01:24 [W] You describe your performance when you're filling out a survey you say. Well, here's how we behave or here's how my team behaves whereas when it's derived. I can say when I look at the data. Here's what your team is doing and here's how they're actually working.
00:01:36 [W] We learn a whole lot about the wave team work teams work.
00:01:28 [W] Well, that is an interesting way to approach this.
00:01:32 [W] But now that I'm at Circle C II had a whole new opportunity of how to do team analysis. And that's looking at actual Behavior looking at data. And so the way that I think about this is this is performance derived versus performance described.
00:01:45 [W] You describe your performance when you're filling out a survey you say.
00:01:49 [W] Well, here's how we behave or here's how my team behaves whereas when it's derived. I can say when I look at the data.
00:01:54 [W] Tahir's what your team is doing and here's how they're actually working and those things don't always agree with each other one to one and that doesn't mean that anybody is lying or that they're not telling the truth or that they're, you know, perhaps looking at something with rose-colored glasses or whatever.
00:02:08 [W] It means that sometimes people have recency bias, or they have a well I'm not thinking about this other project. I'm only thinking about this main project or you know, whatever the case may be. And so we kind of try to I guess normalize out that
00:02:30 [W] Um, you know, perhaps looking at something with rose-colored glasses or whatever.
00:02:34 [W] It means that sometimes people have recency bias, or they have a well I'm not thinking about this other project.
00:02:41 [W] I'm only thinking about this main project or you know, whatever the case may be. And so we kind of try to I guess normalize out that data a little bit and look at what's really going on org-wide versus just maybe a specific data point.
00:02:54 [W] So in this dataset, we have 44 thousand organizations that are in this data set.
00:03:00 [W] Using circleci, I to pull from a hundred and sixty thousand projects and that makes this about a thousand times larger than any state of devops survey that's existed over the years.
00:03:10 [W] And so this is just a much much larger data set which should mean that we can get a little more strong signal from it and a little less, you know, if there's four people that work at one company or 20 that work at one company to fill out something that they can skew the data and one way or another.
00:03:24 [W] don't think that actually happens in those surveys because again, the data analysis is very thorough and professional.
00:03:30 [W] All and they usually can sort that kind of thing out. But this is just a larger dataset overall.
00:03:35 [W] The other thing that we can do this year is look at what's changed year over year.
00:03:39 [W] We started this this question set last year where we were asking a bunch of questions in our data science people were slicing and dicing it and trying to figure out the answers.
00:03:47 [W] And so we have some data from last year and even this year.
00:03:50 [W] We reran the data from last year to make sure that we had processed it correctly and everything.
00:03:54 [W] And so we have some trends that we can see of what's changed this year versus last year. And if you're like all of us, I guess your 2020 has turned out quite differently than 2019 ran and so you may see that there's some of those factors
00:04:06 [W] Show up in the data.
00:04:07 [W] Thorough and professional and they usually can sort those kind of thing out. But this is just a larger dataset overall.
00:04:14 [W] The other thing that we can do this year is look at what's changed year over year.
00:04:17 [W] We started this this question set last year where we were asking a bunch of questions and our data science people were slicing and dicing it and trying to figure out the answers.
00:04:26 [W] And so we have some data from last year and even this year. We reran the data from last year to make sure that we had processed it correctly and everything.
00:04:32 [W] And so we have some trends that we can see of what's changed this year versus last year. And if you're like all of us, I guess your 2020 has turned out quite differently than 2019 ran and so you may see that there's some of those factors
00:04:44 [W] Show up in the data.
00:04:47 [W] So we you know, we do 30 days increments and datasets for some of the trending will do multiple 3D data set so you can see month over month and a couple spots, you know, our organizations.
00:04:57 [W] We've grown a little bit in 2020 our number of projects in the sample grew a little bit in 2020, and that's basically just because of platform growth overall for circleci. I
00:05:07 [W] Traditionally there are four metrics that are used in the devops surveys as kind of the the indicators of high performing teams.
00:05:15 [W] And that's your deployment frequency. Your recovery from failures change failure rate and your lead time to change and those are generally, you know, kind of the big four metrics if you're high in those if you cluster in those you can be in the high performing group and that indicates those are characteristics of high performers.
00:05:35 [W] 20 are a number of projects in the sample grew a little bit in 2020 and that's basically just because of platform growth overall for Circle. See I
00:05:44 [W] traditionally there are four metrics that are used in the devops surveys as kind of the the indicators of high performing teams.
00:05:52 [W] And that's your deployment frequency. Your recovery from failures change failure rate and your lead time to change and those are generally, you know kind of the big four metrics if you're high in those if you cluster in those you can get in the high performing group and that indicates those are characteristics of high performers.
00:06:08 [W] Okay, and so, how do we look at that now if you map that into a CI World? Well, if we have deployment frequency the questions really how often do you initiate your work? And so this is how often do you initiate a pipeline and that becomes throughput
00:06:23 [W] Deployment frequency. The question is really how often do you initiate your work? And so this is how often do you initiate a pipeline and that becomes throughput in our in our nominal Ledger then we have the time to change that has what's your pipeline duration?
00:06:37 [W] We just shorten that to duration your change failure rate, which is you know, your pipeline failure rate.
00:06:43 [W] We actually go to success rate instead of failure rate because it's more fun to talk about successes and failures and then mean time to recovery just becomes recovery time in our world.
00:06:54 [W] So that was a little bit about the setup now.
00:06:56 [W] I'm going to get into the data. This will be a lot of data coming at you pretty quickly, but then we'll get to the insights.
00:07:02 [W] So throughput is what we start with which is how often do you push code that triggers?
00:07:06 [W] See? I most projects are configured to run and push to it with upon push to a get server.
00:07:12 [W] Now this can happen.
00:07:13 [W] This could be one commits pushed to a get server or this could be dozens of commits saying a single pull requests.
00:07:18 [W] It's you know, a lot of people like to say they run tests on commit but usually it's actually a run on push.
00:07:24 [W] And so with throughput you can see that you know people at the 5th percentile are running point zero three jobs per day.
00:07:31 [W] Well, that's not very many and then people at the 90th percentile or kicking off at workflow 16 times per day.
00:07:38 [W] Okay, that makes a little more sense. But when you think about kind of the narrative that you know, High performing teams are deploying dozens of times a day or hundreds of times a day.
00:07:47 [W] I would say that our data don't really support that now if you're in that 95th percentile and above you do start to see that where people are in this
00:07:54 [W] It's you know, a lot of people like to say they run tests on commit but usually it's actually a run on push.
00:08:00 [W] And so with throughput you can see that you know people at the 5th percentile are running point zero three jobs per day.
00:08:07 [W] Well, that's not very many and then people at the 90th percentile or kicking off at workflow 16 times per day.
00:08:14 [W] Okay, that makes a little more sense. But when you think about kind of the narrative that you know, High performing teams are deploying dozens of times a day or hundreds of times a day.
00:08:23 [W] day, I would say that our data don't really support that now if you're in that 95th percentile and above you do start to see that where people are in the
00:08:30 [W] 30s and and I mean well well above that you can get the several hundred at the 98th our 99th percentile but on average, you know, people are deploying maybe eight eight times a day and if you look at that mean at the bottom there and that's that's just
00:09:01 [W] Deploying with the eight eight times a day and if you look at that mean at the bottom there and that's that's just running running a throughput that might not even be a deploy every time that could be just doing validation or Todd pushes.
00:09:13 [W] So people are not pushing code perhaps as often as they talk about all the time.
00:09:17 [W] And so yeah, this analysis is kind of most projects are not doing not deploying dozens of times per day.
00:09:23 [W] So why does that become such a popular narrative? Well, a lot of times in the surveys they ask, you know, what's the primary epsagon?
00:09:31 [W] Application or service you work on and it could be that a single project you have is deployed a lot or it could be that you're single application. Your platform is made up of dozens and dozens of projects. And so while any one of those projects may only be running, you know, 128 times
00:09:46 [W] In aggregate, you're still deploying 60 or 70 times a day.
00:09:50 [W] And for instance if you were going to look at circles the eye that is exactly what you would see you would see that our platform has dozens of repositories that make up what becomes Circle C i and we deploy about 60 to 80 times a day.
00:10:02 [W] would say on average. But any one of those applications may only be changing once or twice a day.
00:10:09 [W] And then you can see that like the difference between 2019 and 2020 isn't a lot but you can tell that the 90th percentile and the 95th percentile have really increased the amount of see either running and to me what this tells me is the people
00:10:24 [W] Are getting more out of it this year than they were last year and I think that that's that's a good thing and it's not surprising but you know and really across the board people haven't are just using a little bit more see I but it's not necessarily the most significant trend.
00:10:37 [W] So those leveraging see I will are doing so even more so and if you had already made the investment to see I think I think you're leaning into that investment more heavily and you're seeing the Returns on it.
00:10:47 [W] The other thing that we see there are fewer developers pushing coworker pushing cold worldwide in 2020. And there were in 2019, which I think again if you think of all the factors going into 2020 that isn't too surprising now if you look at duration
00:11:02 [W] That's how long does it take to get results in?
00:11:04 [W] So this is basically time after code push. How long does it take me to get signal on what's going on for my code?
00:11:11 [W] 5% of our builds finish in less than 12 seconds.
00:11:13 [W] And when I say 5% that's that's about 500,000 in the sample of this exact sample that I pulled and so that's not a small amount but less than 12 seconds you think what happens in 12 seconds?
00:11:26 [W] 12 seconds? Well, it could be a number of things that could be you get a very quick failure.
00:11:30 [W] It could be the entire workflow is something like I'm putting a static file on to say in S3 bucket, or I'm you know, modifying some documentation.
00:11:41 [W] What's going on for my code five percent of our builds finish in less than 12 seconds.
00:11:42 [W] And when I say 5% that's that's about 500,000 in the sample of this exact sample that I pulled and so that's not a small amount but less than 12 seconds. You think we'll what happens in 12 seconds.
00:11:55 [W] Well, it could be a number of things that could be you get a very quick failure.
00:11:59 [W] It could be the entire workflow is something like I'm putting a static file on to say an S3 bucket, or I'm you know, modifying some documentation.
00:12:10 [W] It could be very simple things.
00:12:11 [W] It could be your running unit tests that have been really well optimized but 5% of our builds are finishing in less than 12 seconds, but then you can see that a lot of our builds, you know, they increase quite a bit from there, but 50% of the bills are under that four-minute mile Mark in the
00:12:46 [W] In less than 12 seconds, but then you can see that a lot of our builds, you know, they increased quite a bit from there. But 50% of the bills are under that four-minute Mark in the 95th percentile you're at at 34 minutes and our average is right around 25 minutes because you
00:13:12 [W] At 34 minutes and our average is right around 25 minutes because you know you start looking at the outliers that are in there and pulling that number up.
00:13:20 [W] I think most of most of our workflows time out after about five hours.
00:13:25 [W] So that's about the longest you can go but they're out there a little bit scattershot and a little bit around the world here. But the the 4-minute Mark is the one that I really want to want to hold in on and that's half of all builds finish in under four minutes.
00:13:40 [W] teams that have an SLO around how long it takes to build their software and if things go above their SLO, they will stop engineering and start start engineering how their tests are happening whether they need to parallelize more whether they need to be more
00:13:54 [W] He tests or which ones can take longer or speed up a test.
00:13:55 [W] And so if things take longer than four or five minutes, they may throw a flag and say hey, it's time to re-engineer our test Suite because what matters to us is cycle time and how fast we can give signal back to developers.
00:14:06 [W] And so some teams work very very hard to keep that build time under you know for five minutes and other teams say 25 or 30 minutes is great for me.
00:14:14 [W] I know what a place I used to work 25 or 30 minutes would have been out would have been outstanding to get signal back because sometimes it was seven.
00:14:21 [W] Or 8 hours, and so it all depends on your perspective about what you're doing how detailed and thorough your tests are you know, maybe the number of platforms or permutations your testing things like that?
00:14:35 [W] So in 2020, you can see that there's most of the average durations are the most the durations that across the percentiles are a little bit larger in 2020 and that might be that there's more tests happening. Perhaps more investment into validation in certain segments may be more complicated
00:14:50 [W] Tailed and throw your tests are you know, maybe the number of platforms or permutations your testing things like that?
00:14:57 [W] So in 2020, you can see that there's most of the average durations or the the durations that across the percentiles are a little bit larger in 2020 and that might be that there's more tests happening. Perhaps more investment into validation in certain segments may be more complicated
00:15:29 [W] Things like that, but there are a little more usage going on in 2020 than there was in 2019.
00:15:34 [W] So all pipelines are running just a tad longer with one notable exception and that's that the average time for a pipeline is was actually longer in 2019 than in 2020 and what this tells me is maybe the outliers are are longer but the
00:15:49 [W] Average time is shorter.
00:15:50 [W] So people have been spending time optimizing and trying to get their themselves at the lower end of that that measurement that 24.6 minutes to twenty six point seven six minutes is about an eight percent change. So it's about the average about eight percent faster Euro here.
00:16:08 [W] Next is success rate.
00:16:11 [W] So how often does your pipeline complete complete complete with a green status is more or less.
00:16:17 [W] What this this metric is trying to measure.
00:16:20 [W] And as you can see we have in the lower end of the percentiles 0 it never returns green and in the higher ends a hundred percent where it's always returning green. And so some of our data samples dabbles see I bet they don't necessarily get a working build. This
00:16:35 [W] Riding with their own individual project. It could be a side tool. It could be you know, they never get a valid configuration and they look at it once and walk away and then others of our sample see no failures within a month.
00:16:47 [W] It could be they have a really good test Suite they really know what they're doing.
00:16:51 [W] They have discipline development practices.
00:16:52 [W] It could be they have few tests that they have tests that just say return true so they can never fail and we see all those types of things in our workflows, but you do see that some tests are some organizations have workflows that
00:17:05 [W] I fail and so what you look at overall is the the overall success rate on average is you know, it's 54 percent in the mean and at the 50th percentile 61% and what I found really interesting was this really didn't change from
00:17:20 [W] Organizations have workflows that never fail.
00:17:16 [W] And so what you look at overall is the the overall success rate on average is you know, it's 54 percent in the mean and at the 50th percentile 61% and what I found really interesting was this really didn't change from 2019 at all.
00:17:30 [W] You can see there's a 1% difference at the 50th percentile, but we wanted to look into this a little bit more and see well, okay.
00:17:36 [W] Is that true?
00:17:37 [W] this really that static year-over-year? And if you do look at the little bit the 50th 75th and 85th percentile did things are increasing a little bit so
00:17:46 [W] The people that are using I guess that that percentile are have increased their behavior a little bit.
00:17:52 [W] They're having a little bit higher success rates than what they were a year ago.
00:17:54 [W] I'm not sure it's that significant but it does seem like people that are really leaning into the sea ice base are making sure they're they're getting more out of it than what they were.
00:18:01 [W] The last is recovery time in terms of the data. And that's the time a pipeline sits in a failure state.
00:18:10 [W] So in 2020 what we see is, you know, the shortest end is about two minutes and that can happen in a couple different ways things could happen where you have a developer perhaps that pushes a config and right away realizes they pushing air and they re push a config right
00:18:25 [W] It could be you have to developers and one pushes a bag config and one question is a good one.
00:18:29 [W] And so the bad one finishes and the good one finishes and now it looks like that pipeline is recovered.
00:18:32 [W] And then you can also see that some of these are way way longer in terms of recovery time. If you want to capture 95% of all Builds on the system, how long is the recovery time is three point four days, which is very different than two minutes.
00:18:46 [W] but on average if you look at this mean at fourteen point eight five hours what that basically tells me is sometimes people want to build they stop their work day and they check at the next morning because that's about the amount of time you're away from away from your desk on an average weekday
00:19:01 [W] How they go find the what happened the next day which again, I think describes Behavior a little differently than what most of those devops surveys do in terms of how fast can you recover from a failure or how fast do you recover from problems?
00:19:13 [W] And you know, it could be that people look at that question and think more about production problems than about development pipeline is being read. But of course in a classic continuous delivery methodology a pipeline being read shouldn't be treated any different than a production level outage from a developer point of view.
00:19:28 [W] So and again recovery time can be for multiple contributors running in parallel.
00:19:29 [W] And we talked about already and then once you get to a certain Gap, you can kind of see there's this overnight waiting period that is likely happening and that's that's people that are you know, they kick off a build maybe toward the end of their day and they check it in the morning or they decided to get with it in the morning.
00:19:48 [W] In terms of you know, the 2020 impact here at recovery time went down in every category and one of the things that I wonder about is that because more people are at home and more people are paying attention.
00:19:59 [W] because there are fewer distractions. I'm not positive on that but it does look like the 2020 is slightly better than 2019 was in terms of how long pipeline sit in their Red State.
00:20:11 [W] Yeah, so they have improved year over year.
00:20:14 [W] And so now I want to really get to the insights, which I think is the most interesting part of this rather than just read you data on a screen.
00:20:20 [W] Let's talk about what this data tells us a little bit more.
00:20:23 [W] So what development practices definitely work like what can we pull out of this?
00:20:28 [W] Well, I can tell you that success rate does not correlate with company size at all.
00:20:33 [W] We looked at company sizes and team sizes and Order sizes and tried to look it. Okay, if
00:20:39 [W] your this size team versus this time to size team.
00:20:42 [W] Do you have a higher success rate?
00:20:43 [W] Shin is that because there are fewer distractions.
00:20:42 [W] I'm not positive on that but it does look like the 2020 is slightly better than 2019 was in terms of how long pipeline sit in their Red State.
00:20:52 [W] Yeah, so they have improved year over year.
00:20:55 [W] And so now I want to really get to the insights, which I think is the most interesting part of this rather than just read your data on a screen.
00:21:01 [W] Let's talk about what this data tells us a little bit more.
00:21:04 [W] So what development practices definitely work like what can we pull out of this?
00:21:10 [W] Well, I can tell you that success rate does not correlate with company's size at all.
00:21:14 [W] We looked at company sizes and and team sizes and org sizes and tried to look at okay, if you're this size team versus the
00:21:21 [W] Time T sighs team.
00:21:23 [W] Do you have a higher success rate?
00:21:24 [W] The answer was really pretty much know.
00:21:27 [W] So the only thing that we learned for sure was that if you're a team of one your success rates are lower than if you're not a team of one duration is as also longest for a team of one and so this might be that if you're the only one
00:22:40 [W] Duration is as also longest for a team of one.
00:22:46 [W] And so this might be that if you're the only one consuming the output of your software project.
00:22:51 [W] you may not be trying to make it faster or better because there's no one else relying upon you it could be these are all personal projects or one-off tools for your organization and not things that other people are relying upon but a team of one is definitely not
00:23:27 [W] Relying upon but a team of one is definitely not I guess that those that's always the longest that we see.
00:23:36 [W] Recovery time decreases with an increase team size. And so the more people looking at the build the factory recovery time is which makes sense. But it's only up to a point in my life to that point was around 200, which was actually much larger than I would I would have guessed but the
00:23:51 [W] involve to that point was around 200, which was actually much larger than I would I would have guessed but the vast majority of our data samples sits there with teams between kind of that 529 Mark and not much above that every now and then you get projects that
00:24:06 [W] teams between kind of that 529 Mark and not much above that every now and then you get projects that are maybe the core project that has many teams contribute to and so you get many more contributors, you know where it could be in the hundreds
00:24:21 [W] You and so you get many more contributors, you know where it could be in the hundreds because perhaps the entire engineering department or even people outside the engineering department still contribute to the main core project, but that sample is a little bit smaller than kind of just the across the board all the different projects.
00:24:36 [W] Smaller than kind of just the across-the-board all the different projects.
00:24:40 [W] The longest recovery times are again from teams of one. And so again, if you're working on this yourself, maybe you maybe you're working on your own tool you get a red build and you think well, whatever I'll work on it later and that could be ten days later.
00:24:52 [W] It could be two weeks later.
00:24:53 [W] It could be, you know, three hours later.
00:24:55 [W] It just depends on kind of when that fits back into your schedule.
00:25:02 [W] So in every way that we can measure performance for a team is better when you have more than one contributor, which I guess you can say Mike that doesn't surprise me you were talking about teams. If you have a team your team is probably gonna perform better and that does make sense.
00:25:15 [W] But if you're chewing things on your own, basically, it doesn't look like you will have as good of outcomes. And so I guess one of the things that tells me is you shouldn't be that 10x developer who sits in a corner and says I can do this all myself because you won't actually have as good of outcomes and
00:25:30 [W] And we can show that based on the data sets.
00:25:33 [W] Ultimately software is collaborative.
00:25:36 [W] that's one of the things that we were able to pull out of this pretty clearly is that you know, when you have more than one person or multiple people more than two people even as that number grows you get better outcomes on everything having to do with software delivery.
00:25:50 [W] The next question that I asked was is this don't Deploy on Friday a real thing.
00:25:55 [W] Who sits in a corner and says I can do this all myself because you won't actually have his good outcomes and we can we can show that based on the data sets.
00:26:02 [W] Ultimately software is collaborative.
00:26:05 [W] And that's one of the things that we were able to pull out of this pretty clearly is that you know, when you have more than one person or multiple people more than two people even as that number grows you get better outcomes on everything having to do with the software delivery.
00:26:19 [W] The next question that I asked was is this don't Deploy on Friday a real thing.
00:26:24 [W] This has been kind of an old school level of thinking I think that goes back, you know to the classic operations days before devops was a big movement was what we don't make changes at certain times.
00:26:35 [W] That's the best way to be stable. And don't Deploy on a Friday became quite a real thing overall.
00:26:40 [W] And so I wanted to could I see that in any of our data?
00:26:43 [W] Well, we see 70% Less throughput on weekends and that
00:26:49 [W] Is that there's a whole lot less happening on our on circleci.
00:26:52 [W] I on a weekend and there is during the week and it also means my AWS bill goes down quite a bit.
00:26:56 [W] So that's that's something that you look forward to but then on on Fridays we see 11% less throughput than we would say comparative to Tuesday Wednesday or Thursday and I pull off Tuesday Wednesday Thursday
00:27:11 [W] That's happening on our on circleci.
00:27:13 [W] I on a weekend and there is during the week and it also means my AWS bill goes down quite a bit.
00:27:17 [W] So that's something that you look forward to but then on on Fridays we see 11% less throughput then we would say comparative to Tuesday Wednesday or Thursday and I pull off Tuesday Wednesday Thursday
00:27:33 [W] These are all week days no matter what time zone you're in whereas Monday and Friday you end up having pieces of weekend for every other slice.
00:27:40 [W] I met some port in the world, but we also see nine percent lower traffic on a Monday.
00:27:46 [W] And so if you remember there was 11 percent lower on a Friday and nine percent lower on a Monday and what's really tells me is that it's about the same between Monday and Friday. So I don't think people are holding back on pushing code on Fridays, you know, a lot of times people do take
00:28:01 [W] Days off if they're trying to you know extend a long weekend and that probably accounts for most of that drop. The other thing that does account for that drop is that Mondays and Fridays are you know again different time zones around the world, you know, we use
00:28:15 [W] And Fridays are you know again different time zones around the world, you know, we use UTC for all of our logs and all of our data Gathering and stuff like that. But you know at midnight UTC Monday morning is still Sunday evening in North America. So there's
00:28:26 [W] At midnight UTC Monday morning is still Sunday evening in North America.
00:28:31 [W] So there's not a lot of work happening from that segment.
00:28:33 [W] It's mostly other parts of the world at that time.
00:28:41 [W] Another interesting set of questions that I love to ask is what do we see about languages and language Trends going on in this data set, you know, are there certain languages that are that lend themselves to have better outcomes across the sea ice is them across teams and
00:28:56 [W] Take these with a grain of salt because anytime I put up a chart of languages.
00:29:00 [W] It's half of it's just to provoke you to think about what your what your language choices are and why and the other half is to make sure that you can argue with your other develop and friends about what language choices you made in why?
00:29:11 [W] So language is in our sample, when you look at, you know, the giant sample that I said that was a hundred and sixty thousand projects.
00:29:17 [W] Here's basically the breakdown anything below zero point three percent just wasn't even counted because this is the top 20, but you know, we have a lot of dynamic languages.
00:29:29 [W] We have a lot of statically typed languages.
00:29:30 [W] We have a lot of more markup and markdown type languages.
00:29:33 [W] know, several different styles of language in our sample here on throughput. This is basically how often you're running on.
00:29:41 [W] Circleci, I you know Ruby is our is the thing that's happened in the most often that could be because their durations are rather slow or rather quickly you get feedback very quickly.
00:29:49 [W] It could be there's just a lot of Ruby projects. It could be there's a number of different things and you can see that this is just how often different things happen. Now the dockerfile being at 20 it was a little bit surprising to me because we do have a lot of people that build Docker images via see I but it could also be that that doesn't
00:30:05 [W] Set off and it happens less often than a lot of the other software projects.
00:30:07 [W] I guess that's because Doctor Who would be the final artifact versus perhaps an intermediate artifact of compiling your go program and then placing it the docker Docker container things like that on the success rate at the 50th percentile.
00:30:21 [W] This is the order.
00:30:23 [W] I find it interesting that view CSS and she'll show up at the top.
00:30:27 [W] I don't see a lot of tests for CSS and even in Shell you don't see a lot of tests.
00:30:31 [W] Usually it's kind of just actions and so those
00:30:33 [W] Actions May succeed regardless of kind of what happens a lot of shell Exit Zero unless you're very intentional about how you write things. Some people probably do use, you know, like a bash automated test Suite bats or whatever, but most people I think are just using shell to complete a
00:30:49 [W] while over there and it's going to return 0 regardless and a lot of cases and then dockerfile also usually succeeds in that you build a Docker container whether or not the container with what you wanted or not, maybe something that you need to be requires further introspection, but
00:30:52 [W] Real language that I think that has a lot of testing up and down all the way through would be probably go at number 8 and what I find fascinating here is that's the first, you know, that's a strongly typed language and then you get into many dynamically typed languages and then you get back into the rest of statically
00:30:59 [W] Lower and so you see, you know, 16 starts with Java again where you get back into static typing and that I'm really surprised that go it seems like and you'll see this in a couple of other slides go actually clusters more with other
00:31:13 [W] I'm really surprised at go.
00:31:13 [W] It seems like in you'll see this in a couple of the other slides go actually clusters more with other Dynamic languages, even though it has more of the statically typed language, you know properties going for it.
00:31:25 [W] Yeah recovery time number one is go which was a little bit surprising to me as well.
00:31:29 [W] But apparently the people that are writing go programs or paying very close attention to their pipelines and making sure they're have good things are happening the longer things the things that more at the bottom the recovery time being slower for some of the some of the stack. We type stuff that makes sense to me because
00:31:44 [W] Those are paying very close attention to their pipelines and making sure they're have good things are happening. The longer things the things that more at the bottom the recovery time being slower for some of the some of the stack. We type stuff that makes sense to me because their durations are a little bit longer
00:32:07 [W] A little bit longer and a lot of times your recovery time is longer if your duration is longer, if you know your pipelines going to take 35 minutes to run you may step away from your desk.
00:32:16 [W] You may go to another call another meeting and then you don't see the result for a while or you don't get back to working on the result for a while.
00:32:22 [W] Whereas if you're going to get real results in three minutes, you may be able to correct them very quickly. And so your correction time will also kind of correlate with that.
00:32:31 [W] In terms of duration things that happen the fastest our shell scripts and knative cl and CSS and then you get into the JavaScript stuff which does happen very quickly and then go and then, you know, you have kind of those Dynamic languages again and you get back into the static languages toward the end.
00:32:46 [W] Know what this tells us is is not a lot.
00:32:49 [W] I think they kind of cluster together. But I think that just the behavior of the go programmers is what I would say is a little bit of novelist and I call out there a little bit in last year's analysis.
00:32:59 [W] I can tell you that PHP showed up in a really good light in most of these metrics which was really odd to me and I remember making recommendations of everybody using PHP which I didn't believe but thought was rather amusing and this year it shows up much more like how I expected to group
00:33:14 [W] Ruby and things like that. So this year the data seems to be a little clearer on how these whole group together.
00:33:17 [W] One of the other things we talked about was Branch information because we you know, a lot of times people build off of a feature branch and then they merged into Mainline.
00:33:24 [W] How does that all work in? The first question that came up to me for 2020 was have people really worked on renaming their branches from Master.
00:33:32 [W] You know, there was a lot of talk in the social unrest days throughout this year that Masters just it's harmful language and we want to remove that from our computer science curriculum as much as we can and all of our usage so get have announced they were going to change the
00:33:47 [W] Branch away from Master as did get upstream was very aware.
00:33:51 [W] I was made so that you can have different default names easily and not just use master. So did this actually decrease this year?
00:33:59 [W] The answer is no it really didn't in any significant way and I played yet in here because I don't think GitHub has completed all the changes to make it so that any new project doesn't have master or maybe they did with new projects, but they haven't done anything retrofitting existing projects.
00:34:14 [W] A little more of that work you'll see it take off a lot more. But right now we don't see any significant movement in people moving away from a branch called master.
00:34:20 [W] Another Branch Branch topic is that teams are innovating in experimenting on feature branches.
00:34:25 [W] And so you can tell this because the success rate on the default branch is much higher than on default branches. And so the default branch in a lot of cases is main line or main or master and the the success rate there is significantly higher than what it
00:34:40 [W] Because the success rate on the default branch is much higher than on default branches. And so the default branch in a lot of cases is main line or main or master and the the success rate there is significantly higher than what it would be on some of those
00:34:52 [W] On some of those branches that lead into it, you know the branches you would rather be making a poor request to emerge request on things like that.
00:35:00 [W] So we have about an 80% success rate on on the on the main branch versus a it's a hundred percent above the 75th percentile.
00:35:11 [W] It's so basically success is happening a whole lot on the mainline Branch.
00:35:16 [W] Whereas it's only 58% for that non-default branch and what that tells me is people are using the ci/cd stem to get signal at times. And so maybe they're merging a thing where they like, I'm not sure this is going to work. I'm not positive, but I'll let the ci/cd some tell me if this works or it'll run maybe a complicated.
00:35:30 [W] Esther I just can't simulate it on my workstation to easily. But either way what this really supports is that things are happening outside of Mainline. They're getting merged into Mainline and that's how we're figuring.
00:35:42 [W] That's how people are getting signal and they're also taking really good care to keep Mainline clean and keeping it green, which is really great when you have multiple developers working on something. This is just following, you know, the practices of trunk space development. They're really want the trunk to me clean all the way through
00:35:59 [W] The duration on the default branch is faster at every single percentile than it is on the feature branches by default or by on feature branches at large.
00:36:08 [W] And so what this basically means is branches that are not in Main Line generally finish lower than branches that are main line. And again that makes sense because there's new experimentation going on perhaps your tests are flaky perhaps they need to be optimized but why is they get into Mainline
00:36:24 [W] And also recovery time is lower.
00:36:20 [W] Whilst will be lower than what is not on the default Branch every single percentile.
00:36:24 [W] So when you're on your feature branches, your recovery time is significantly longer than what it is on the main line Branch at every single every single percentile.
00:36:33 [W] The last set of questions that I was really interested in from our data team and from the other people that helped put this together was what can we see?
00:36:40 [W] What can we what if we learned about a global pandemic on team performance?
00:36:43 [W] What does that really told us from our data set and as you start to graphing we pulled graph from 2019 all the way through 2020 and we try to do, you know, 30 day increments on samples. But as you can see there are dips that
00:36:58 [W] In the March and April time frame or changes, I guess I would say in this case throughput went up in March and April are Peak throughput, you know, looks like at least at the 95th percentile was in April of 2020.
00:37:08 [W] Generally after that since April through puts been around flat at most of the percentiles.
00:37:15 [W] So it does fall just a tiny bit at least at the 95th percentile in duration.
00:37:21 [W] We see that you know, a lot of things went up in March and April and to me the way that I thought about this and tried to rationalize this was you know, perhaps there's a bunch of developers that were at conferences that were on vacation that were
00:37:36 [W] From the computer I guess is probably the right way to put it that return to the computer in March and they started working on things.
00:37:34 [W] Maybe they're adding more testing.
00:37:36 [W] You have more people directly working on the code bases.
00:37:39 [W] And so your duration goes up and then you see a little bit of optimization after that. Perhaps people were like, hey, I don't like when my test take longer to return.
00:37:46 [W] I'm going to go work on that make sure that doesn't happen cuc duration increased a little bit in February and then that increase accelerated in March and then decreased in April, which I think
00:37:56 [W] you know, we can basically say that the hypothesis here is that more tests were happening in March driving up the duration and in April there was more of a concentrated effort on optimization and that kind of adds up when you think about the way that people were kind of Sheltering in their homes.
00:38:11 [W] They're working on things and then it was like, wow we're adjusting to this.
00:38:14 [W] Okay.
00:38:15 [W] I'm going to work on this.
00:38:15 [W] I'm going to make this a real thing.
00:38:16 [W] I'm going to make it good success rates also increased in you know, during the pandemic as everybody went home and sheltered in place. They
00:38:26 [W] Got higher success rates and part of that again could be that they were paying more attention to the direct things that their company was doing.
00:38:32 [W] They weren't on the road.
00:38:33 [W] They weren't as distracted.
00:38:35 [W] They weren't as sidetracked.
00:38:36 [W] I do love the 25th percentile just falling off there. And I wonder if that's just like 2020 in a graph.
00:38:41 [W] It's just like no I can't anymore or if it's you know, those are projects that are not core to the business.
00:38:47 [W] Have a clear narrative there. My hypothesis is that that is non-core business inside projects and tools that were just not essential and they're no longer getting the attention. They once got the success rates were the highest on record for
00:38:58 [W] Kind of when you had full work force at home fully engaged on getting longer days things like that.
00:38:59 [W] So the hypothesis there is that people working hard on Core Business stability in March and April.
00:39:05 [W] A lot of companies were going through reject reduction in Workforce and other types of economic, you know things where they were like wow with this this little side thing.
00:39:14 [W] I don't know they want to dabble with that anymore.
00:39:16 [W] make sure the core of our business is clean and works in this table and I think we saw that show itself through our data through from developers and then
00:39:24 [W] Recovery time, you know, we can see that it hasn't changed significantly throughout the pandemic period but it did drop a little bit and we saw that in some of the 2019 to 2020 data earlier, you know, generally the 95th percentile is dropped quite a bit some of the people that were really
00:39:39 [W] At the recovery times, but recovery time has been improving since April.
00:39:45 [W] In order with the longest recovery time have definitely improved in that that was kind of indicated by this top line that green bar there where when you had the longest recovery time that has increased or decreased significantly. And again, this is a this is a golf not bowling thing where you want to be the lowest score to win.
00:40:00 [W] So people are recovering faster and faster than what they used to and the hypothesis here is that people have fewer distractions when they're working at home.
00:40:02 [W] They don't get pulled into meetings or go to lunch or go grab a coffee and then forget about it and then come back.
00:40:06 [W] Oh, I was working on that pipeline.
00:40:07 [W] I should probably go fix it.
00:40:09 [W] There's probably a little bit less of that happening when I say if you were distractions. It's a course for some values of distraction. Some of us have very new distractions to work with whether that's children at home or taking care of loved ones or worrying about sickness or just Doom scrolling all day about what's going on there.
00:40:23 [W] There's a lot that can be distracting but I do think that people generally are a little bit more in front of the computer and paying a little more attention to what their their signals are on their delivery.
00:40:34 [W] And so I want to take you into a couple of final thoughts as we've dug through this, excuse me final thoughts here when mapped against devops survey data see I use errs at the 50th percentile show between medium and high performers at an org level.
00:40:49 [W] People generally are a little bit more in front of the computer and paying a little more attention to what their their signals are on their delivery.
00:40:53 [W] And so I want to take you into a couple of final thoughts as we've dug through this, excuse me final thoughts here when mapped against devops survey data see I use errs at the 50th percentile show between medium and high performers at an
00:41:25 [W] And so what that tells us is that you know, when you look at kind of the classic layout of how are you doing?
00:41:31 [W] And what are the data sets? If your average at using CIF your at the 50th percentile generally you're going to be right on the borderline between medium and high performer.
00:41:41 [W] And the more you're using it the more it will push you up into high performance and that's high performance in terms of the way the devops survey measures it and in the way that Circle C. I measure is that in terms of your CI engagement. And so you can kind of correlate the CIA engagement correlates very highly with dynatrace.
00:41:55 [W] Devops engagement overall or devops success overall.
00:41:58 [W] So if you're using if your average it using a CI platform, you'll be right in the line and people that are really leaning into this have better outcomes in all four of our critical metrics and the four critical metrics that these devops surveys have been using for years and so
00:42:13 [W] It's a critical part of being on your delivery path and making sure that you can deliver software when you want to and making sure that your team is high performing the last takeaway that I that I really loved was that more collaborators means better outcomes. The more people you have working on this stuff the better
00:42:28 [W] You know part of it to me is I think you just care about your developer experience for other developers. If they have to wait 10 minutes for tests from they could rate five people work on it.
00:42:32 [W] Make sure they're only waiting five and if the you know, the pipeline's read well, somebody else needs it to be green to do their work.
00:42:38 [W] You're going to make sure it's green and I think that adding collaborators is really great thing.
00:42:41 [W] So I hope you found this interesting, you know, it's definitely a dive into the data.
00:42:45 [W] is kind of a cursory survey of the whole set of data because there's a lot you can go into there's a lot of questions you can dig into there's a lot of questions you can ask of the data team they can slice and dice.
00:42:55 [W] But if this is interesting to you, I will remind everybody we are hiring we have been hiring like mad over at Circle C i and we still have many many positions open and engineering and data.
00:43:04 [W] So if this stuff is interesting to you or you're welcome to apply and I just want to say thank you. I'm again, I'm Michael stanky.
00:43:10 [W] I'm the VP of platform over at Circle C. I you can find me at styra on basically any internet service that you care to find me on and I also want to give a special thanks to Ron and Melissa who helped gather all this data and sliced and diced it to answer a lot of my questions about it. I did not.
00:43:24 [W] This all on my own. I am not a data scientist, but luckily I know people who are so thank you so much.
