Building a Global Supercomputer with Virtual Kubelet: SFON-7297 - events@cncf.io - Thursday, November 19, 2020 2:56 PM - 37 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone.
00:00:01 [W] This is the talk on the duration in kubernative and Mitre Mission. I work for University of California, San Diego.
00:00:08 [W] Out there how our project came to the current state it's in and in the second part Adrian the creator of admiralty will describe how ignorant is working. Also.
00:00:22 [W] Also, we will have the demo on how technology works.
00:00:28 [W] So our project is called PRP Pacific research platform and it grew from a project measuring Network performance.
00:00:39 [W] So the initial problem was that the University of California, San Diego had multiple campuses connected with what's called science DMZ scientific
00:00:54 [W] Projects measuring Network performance.
00:00:46 [W] So the initial problem was that University of California, San Diego had multiple campuses connected with what's called science DMZ scientific Network 10200 gigabits per second. So pretty fast.
00:01:02 [W] And sometimes this some segments of the network doesn't perform the way they are supposed to end PRP was measuring Network performance by sending test data between those locations that was done manually at some point.
00:01:18 [W] Measuring Network performance by sending test data between those locations that was done manually at some point.
00:01:26 [W] There were just more nodes that PRP could handle manually and there was an idea to deploy kubernative on top so that kubernative would orchestrate those measurement software
00:01:42 [W] Word grenadiers were realized that now we have a cluster of nodes.
00:01:47 [W] We just occasionally send data but mostly sitting idle and we started providing those Hardware resources to scientists to just do their computations and we saw exponential
00:02:02 [W] Our cluster because it was really easy to onboard our our system bring in the container run it and get free access to the resources for scientific use
00:02:12 [W] sir Knight of us
00:02:11 [W] At some point more projects started the knative hardware to us.
00:02:16 [W] We partnered with internet to open science grid the knative some notes more networks were coming to us and this is how
00:02:31 [W] yes, so we have seen Nick Network in California providing us the internet access and there is a bunch of regional networks that also donate
00:02:44 [W] Across there. So basically we have National kubernative cluster aggregating a bunch of computing resources at some point. We started bringing in GPU nodes storage
00:02:54 [W] So on so now our cluster is pretty big and at some point it started growing internationally.
00:03:00 [W] So an international scale we have nodes in Europe in Asian Pacific region region or criminals in Singapore, Korea Guam, Australia.
00:03:18 [W] And I were talking about knows themselves. Those are called fuel in the notes.
00:03:25 [W] / IO Network appliances, so those nodes are optimized for fast networks, so they can actually leverage those speeds and some knows can fit up to
00:03:41 [W] Use and some notes have a bunch of hard disk, which we use for storage for our self cluster.
00:03:50 [W] So when we talk about Federation current state is that we have our PRP noto's cluster National for us and some International nodes.
00:04:06 [W] Now it has 7,000 CPU cores more than 500 gpus 2.5 petabyte-scale.
00:04:24 [W] And we spawned to AWS cloud.
00:04:28 [W] So all those clusters are separate. It's really hard to integrate all that Hardware in one and we need iteration to
00:04:43 [W] Also, we partnered with several other smaller clusters and there is a federation already established between those locations.
00:04:45 [W] So basically all clusters can finally talk to each other but are controlled by different organizations and different people.
00:04:53 [W] That's the huge benefit of admiralty because of our other projects are seeing Federation as one cluster controlling others. It's not one level finops.
00:05:06 [W] duration between them
00:05:09 [W] And what we are looking forward to is when expense supercomputer is deployed.
00:05:17 [W] It's also going to have the kubernative cluster as a part of it and admiralty will be used to figure eight expense with natos and other questions and that will bring theoretical.
00:05:32 [W] Expansion 200,000 course, of course, we will not get all of them. But we that's a pretty big increase in what we figure eight with.
00:05:36 [W] So how can we use resources between those clusters? As I said most of these clusters belong to different organizations who want to set their own rules, so we cannot iteration cannot dictate
00:05:52 [W] CS for writing stuff in different clusters and scientists who use those clusters usually have access to only one or several namespaces.
00:05:50 [W] They again don't control the whole cluster and so figuration should allow working on namespaces level when people don't have to bug the admin to figure eight, but they can establish figurations between
00:06:05 [W] a piece of the cluster that the control
00:06:03 [W] Additional to those small clusters, which are on-prem as I said, we figure eight with clouds Amazon Microsoft Azure Google Cloud if we need some resources from
00:06:18 [W] We have some people workloads or we want specialized Hardware.
00:06:13 [W] We can always figure eight with those by simply running some temporary clustering those and then establishing the Federation.
00:06:24 [W] So, how do we use Federation now?
00:06:29 [W] Gitlab CI is what we started with.
00:06:33 [W] It's super convenient to like when we need to build a container on some specialized Hardware like are more windows instead of manually creating a gitlab runner somewhere else or
00:06:49 [W] This note in our because the richest Kata rate and that would be in the demo.
00:06:49 [W] So now we did it with Windows and arm and our gitlab by just taking the job can build the container in some other coaster and this container will be stored in gitlab registry
00:07:05 [W] Touring right now.
00:06:59 [W] We have a bunch of monitoring ports and our large cluster. But goal is that any other cluster that wants to have automated monitoring can just join our Federation and get all the
00:07:14 [W] Jobs bursting if some cluster has some unused resources and another one is overloaded.
00:07:18 [W] It's always useful to just burst another questor and requesters can share their Computing resources without actually reattaching note. So that's super Convenient Medical Data
00:07:33 [W] It's overloaded.
00:07:32 [W] It's always useful to just burst and other costs are and clusters can share their Computing resources without actually at the reattaching note. So that's super Convenient Medical Data uses
00:08:22 [W] Some data can is highly protected it cannot leave some plaster, but this data can be used for some computation and then results of this computation can be shared
00:08:37 [W] It's annoying moist.
00:08:39 [W] So figuration and again can let you spawn your Computing job in another cluster get some product anonymize it get it back.
00:08:51 [W] So this is super convenient special devices Internet of Things. Some devices are really small and Tiny and they can just join a big cluster and just monitor Imports will kill it. So again, this
00:09:06 [W] Sort of figure eight and some iot devices are again controlled by some other people and figuration helps establish those connections without getting
00:09:18 [W] access to the specialized Hardware
00:09:17 [W] Future of figuration we are now working on the project that will allow us to create on-demand layer to a fast pass around the world.
00:09:30 [W] So there is a that's a project we serve net.
00:09:33 [W] It's called an essay auto goal. And this project already can create a layer 2 connections and tear them down on request.
00:09:46 [W] And what we are right now is a special ci/cd and toward greater which will allow pods that spawn in some remote location to create a path for example to storage and some other location and this path will exist while the
00:10:01 [W] And what we are right now is a special ci/cd and operator which will allow parts that spawn in some remote location to create a path for example to storage and some other location and this path will exist while
00:10:23 [W] And then this path path will disappear and Federation will help us to run this across several kubernative costumes.
00:10:33 [W] So if some quester wants to talk to another cluster, it just creates a layer to connection and that will be controlled by a standard kubernative say Pi. So this is really great project that we're working on.
00:10:50 [W] This is a demo on figuration in this demo.
00:10:53 [W] I will show how duration Works in our production cluster.
00:10:57 [W] I will not go through details of establishing the connection.
00:11:02 [W] It's all covered in documentation.
00:11:04 [W] And this is supposed to be a short demo. So now we're looking at the nodes in our production cluster and we see that the first node.
00:11:14 [W] As the role of cluster and master this is virtual node created by admiralty.
00:11:21 [W] So basically this represents the whole remote cluster and iterated pause for run in this node, while the actual node is the load in this demo.
00:11:33 [W] I will show how our gitlab can spawn but in other clusters and get results from them.
00:11:41 [W] Node created by admiralty.
00:11:43 [W] So basically this represents the whole remote cluster and iterated pause for a run in this note while the actual node is the load in this demo.
00:11:55 [W] I will show how our gitlab can spawn but in other clusters and get results from them.
00:12:04 [W] So here is the jobs table of our gitlab these jobs are building container and putting it in gitlab registry.
00:12:16 [W] But while most projects are just building containers in our regular cluster. This project is setting up this architecture arm 64 10.
00:12:31 [W] On the jobs, which make them go to special runner in gitlab.
00:12:41 [W] Let's look at our gitlab namespace.
00:12:47 [W] So in this namespace will see that it's labeled with multi cluster scheduler enabled this will tell admiralty to look at these nice pace and wait for pots that we Mark as iterated
00:13:02 [W] in gitlab
00:13:07 [W] Let's look at our gitlab namespace.
00:13:13 [W] So in this namespace will see that it's labeled with multi cluster scheduler enabled this will tell admiralty to look at these nice pace and wait for pots that we Mark as iterated
00:13:46 [W] Understand space we will see several Runners Rhino radio on regular notes.
00:13:54 [W] Hookah, deployments there is already deployment that I created called gitlab Renner Federated.
00:14:01 [W] We can look at it.
00:14:06 [W] We will see that in template for parts.
00:14:09 [W] It has the multi cluster admiralty iot / elect annotation. So this again will tell admiralty that pods from this deployment should be Federated. So admiralty will decide where to send them to.
00:14:26 [W] But scale this deployment.
00:14:30 [W] two one
00:14:34 [W] and we will see that this new pod.
00:14:39 [W] You scheduled to run on this virtual node.
00:14:45 [W] So if you look at all parts, all of them are in our cluster, but this one pot is running somewhere else.
00:14:57 [W] We can look at our arm cluster.
00:15:04 [W] Again, this is gitlab namespace.
00:15:06 [W] But this is now removed cluster Federated with our large one.
00:15:11 [W] It's on remote node and in our large cluster, we only have the proxy for this odd.
00:15:20 [W] Let's switch back to our main quest.
00:15:22 [W] So this Runner already registered have to Medical in gitlab and if we rerun one of the jobs
00:15:38 [W] the runner will run it remotely.
00:15:43 [W] So if we get go again to remote cluster.
00:15:47 [W] We'll see that new.
00:15:51 [W] job started
00:15:58 [W] and it just finished.
00:16:01 [W] This is pretty quick.
00:16:04 [W] And the result of this job went into the local container registry in our gitlab.
00:16:12 [W] So this completed successfully this allows gitlab to run runners in Federated clusters without manually setting the amount and control them from one
00:16:27 [W] Hi, my name is Adrian.
00:16:26 [W] I'm the CEO of admiralty admiralty's the company behind the open source project the same they are multi is what makes the decentralized Federation presented by Dmitry possible.
00:16:36 [W] It's a multi-class to control plane.
00:16:38 [W] They use this common communities extension patterns. This talk will focus on future cubelet and the scheduler framework.
00:16:45 [W] What is virtual kubelet?
00:16:47 [W] This is a screenshot from virtual kubelet website.
00:16:51 [W] So like the kubenetes cubelet that runs on each node of your truck kublr instance presents itself as a virtual note, but instead of running containers on the local VM. It runs them on remote system.
00:17:06 [W] And there are 11 known providers to the state including admiralty.
00:17:11 [W] In the case of admiralty your tree kubelet represent remote clusters to help us understand the concepts that I'll explain later.
00:17:19 [W] Let's consider this example use case where user submits jobs in their own cluster cluster a but containers running other clusters DNC and so in cluster a
00:17:31 [W] The eventual notes that represent the other clusters the Box created by the kubernetes job controller.
00:17:37 [W] boxing networking sense
00:17:25 [W] I'm royalties proxy Bud scheduler schedules those spots to the virtual nodes and creates.
00:17:32 [W] candidate pods in the other clusters
00:17:37 [W] some of those candidates become delegate pods that are bound to real nodes.
00:17:43 [W] Those real nodes could actually be other virtual nodes.
00:17:46 [W] Can you imagine several levels of Inception?
00:17:51 [W] I don't know if he also includes a bunch of controllers to update but statuses make config map secrets and other dependencies photons.
00:18:00 [W] Notice that cluster a needs to talk to clusters B and C and also different ways to do that in a minute.
00:18:08 [W] Let's go back to visual cue blade and how Admiral team commits in your truck you blade has four main responsibilities.
00:18:16 [W] The first responsibility is to register a node object admiralty creates virtual nodes based on user created targets and cost to targets. Those are custom resource definitions.
00:18:30 [W] Now basically given name to Virtual node and refer to a secret that will be used to talk to the corresponding Target cluster. The second responsibility is heartbeat kubernative needs
00:18:45 [W] Notice that cluster a needs to talk to clusters B and C and also different ways to do that in a minute.
00:18:47 [W] Let's go back to visual cue blade and how Admiral team commits it.
00:18:51 [W] Your truck you blade has four main responsibilities.
00:18:55 [W] The first responsibility is to register a node object admiralty creates virtual nodes based on user created targets and cost to targets. Those are custom resource definitions.
00:19:09 [W] Now basically given name to Virtual node and refer to a secret that will be used to talk to the corresponding Target cluster. The second responsibility is heartbeat.
00:19:23 [W] Kubenetes needs to know that a node is healthy and regular intervals.
00:19:28 [W] Otherwise, I will evict the pods if the node is not responding. So if your child cubelet and admiralty checks to health of the target clusters and updates to condition of the know.
00:19:42 [W] The third and maybe the most important responsibility is to handle pause.
00:19:48 [W] So it's run the actual container. He's somewhere and in admiralty.
00:19:53 [W] This is most of the logic with multi cluster scheduling but status feedbag cross cluster garbage collection and so on.
00:20:02 [W] Now, of course, there are some other features like handling logs requests and exact requests and in admiralty skates, it's very simple, which is forward those requests to Target clusters kubenetes API.
00:20:19 [W] three Last Rose require keep configs to cold the target clusters
00:20:29 [W] So let's talk about that in admiralty clusters are connected in the control plane in one-to-one relationships and we say that the source and a Target cluster are connected when controllers in the source cluster can call the
00:20:45 [W] Other features like handling logs request and exact request and in admiralty's case. It's very simple.
00:20:53 [W] We just forward those requests to Target of clusters kubenetes API.
00:20:59 [W] three Last Rose require keep configs to cold the target clusters
00:21:09 [W] So let's talk about that in admiralty clusters are connected in the control plane in one-to-one relationships and we say that the source and a Target cluster are connected when controllers in the source cluster can call the
00:21:24 [W] routing authentication and authorization routing may require a VPN or tunnel if the Clusters on public or if they're in different V PC s-- authorization the last
00:21:39 [W] Blake coins are in different V PCS.
00:21:38 [W] authorization last one
00:21:42 [W] Is is quite straightforward?
00:21:43 [W] He uses our back resources in the Target cluster and that's very important to Dimitri and his colleagues and partners. They want costs our admins to be in control of who can do what in their clusters.
00:21:58 [W] And authentication can be done using different methods.
00:22:05 [W] the nice thing about having one to one relationships all connection cluster connections is that you can build any kind of topology cluster topology with those as long as it's a directed graph it's valid and so
00:22:20 [W] To - probably a management class. They're talking to many workloads clusters, or you can do Cloud bursting where a cluster has its own Target and the case of the research platform presented by Dimitri. We have a decentralized federation where
00:22:36 [W] There's no no leader.
00:22:39 [W] I said I would talk about cross cluster authentication this could.
00:22:43 [W] Be a talk full talk just by itself.
00:22:47 [W] So I'll go very quickly.
00:22:51 [W] The simplest way to achieve it is to take this service account token from Target cluster and Export it and save it in the source cluster included you could do the same thing with the certificates
00:23:06 [W] To achieve it is to take the service account token from Target cluster and Export it and save it in the source cluster including you could do the same thing with the certificates API instead of a token you would have
00:23:26 [W] Instead of a token.
00:23:27 [W] You would have a certificate the problem with those two methods is that you using your target clusters as an identity provider and now the source cluster so you need to distribute and rotate the secrets yourself.
00:23:43 [W] With other methods you can use an identity provider available in the source cluster to get you tokens or certificates and present those to the Target cluster.
00:23:55 [W] And that targeted cluster has to be able to recognize them.
00:23:59 [W] And so if you're in the cloud you can use cloud identity like a kubernative service account in the
00:24:07 [W] In a source cluster can impersonate like an AWS.
00:24:12 [W] I am row or Google cloud service account Hydro search will have renamed what could identify these machine identities and and then use that to connect to the
00:24:27 [W] And in a source cluster can impersonate like an AWS.
00:24:33 [W] I am row or Google cloud service account Hydro search Prince will have renamed workloads identities.
00:24:40 [W] Well Machine identities and and then use that to connect to the Target cluster if you're in control of the masternodes and you can change the API server Flags, that's great
00:25:03 [W] If you're in control of the masternodes and you can change the API server Flags, that's great because then you can use web hook token authentication then haproxy.
00:25:13 [W] if you need to have a solution that one-size-fits-all is you can use an impersonating proxy that uses kubenetes impersonation to
00:25:29 [W] With etiquette and you can check out the kubeflow IDC proxy project as a prime example and admiralty Cloud uses the same print the same Concepts.
00:25:46 [W] admiralty has to schedulers the proxy scheduler and the candidate scheduler proxy scheduler handles proxy pause on the source cluster side and candidate scheduler handles the
00:26:01 [W] was created by the proxy scheduler
00:25:59 [W] and that both built upon the scheduler framework.
00:26:06 [W] The steel framework is as great. It's a set of go language interfaces.
00:26:13 [W] that allows you to build your own scheduler while retaining all the features of the standard kubenetes scheduler and adding yours at various extension points in the scheduling cycle in The Binding cycle
00:26:28 [W] Features of the standard you're going to be scheduler and adding yours had various extension points in the scheduling cycle in The Binding cycle.
00:26:32 [W] And so the Dots here Mark where our guide multi to schedulers extend the kubenetes scheduler.
00:26:47 [W] You so you can for example add some filters like hug filter nodes, or you can wait before actually binding to a note and this is useful.
00:26:58 [W] In admiralty's multi cost of scheduling algorithm because two clusters talk to each other using annotations on polish a bronze.
00:27:10 [W] Let's let's see how this works.
00:27:14 [W] So in this sequence diagram, I have the source cluster on one side and very started clusters on the other side that have the same components with different timelines.
00:27:28 [W] When a sore spot is created and annotated to use admiralty as a multi cluster scheduler.
00:27:35 [W] It is mutated.
00:27:36 [W] We need to make a few changes to the Pod like a scheduler name changed scheduling constraints.
00:27:44 [W] so that the pot can tolerate the virtual node.
00:27:48 [W] We save the original scheduling constraints for later.
00:27:51 [W] We had a finalizer for cross cluster damage collection different things check the documentation for details.
00:27:59 [W] with different timelines
00:28:02 [W] when a sore spot is created and annotated to use admiralty as a multi cluster scheduler.
00:28:09 [W] It is mutated.
00:28:11 [W] We need to make a few changes to the pot like a scheduler name changed scheduling constraints.
00:28:18 [W] So that the Pod can tolerate the virtual node.
00:28:22 [W] We save the original scheduling constraints for later. We had a finalizer cross cluster damaged collection different things check the documentation for details.
00:28:33 [W] While the proxy but is being scheduled and the virtual nodes that it tolerates are filtered pods chaperones are created in the Target clusters that correspond to those original notes
00:29:01 [W] And the virtual nodes that it tolerates all filtered pods chaperones are created in the Target clusters that correspond to those ritual notes applied Chevron has two purposes.
00:29:16 [W] SQL of annotations the both schedulers annotate the pot chaperone to communicate if they annotated one of the other pod instead that would invalidate the cache of the scheduler. So it's
00:29:29 [W] Instead that would invalidate the cache of the scheduler. So it's best to use another component of another object for that.
00:29:36 [W] But also when they may be mostly and most important from a teacher perspective if the source to Target costume connection is interrupted and the Pod has been running
00:29:51 [W] The pot has been sorry, sir.
00:29:53 [W] Delegate pot has been running for a while, but is evicted because a note that runs on a real note that it runs on is being cordoned. For example or stopped responding.
00:30:06 [W] We need a way to recreate the pot and the so we need a local controller and that's the pot shop wrong. But chaperone is just about template that creates the pain.
00:30:17 [W] alright enough about how shameful so each project chapter on creates a poddisruptionbudgets.
00:30:21 [W] That looks exactly like the Pod shape prom and it includes the scheduling constraints that were safe in the annotations on the proxy poddisruptionbudgets.
00:30:47 [W] And the real no level in the Target clusters, and so for example here only to of the tired clusters that have free can find a node for the candidates their respective candidates.
00:31:01 [W] So they annotate
00:31:04 [W] the the pawnshop wrong the proxies Kudo sees that selects one of the two using some topology spread constraint or other other proxy poddisruptionbudgets scheduling constraints that you can add.
00:31:19 [W] and when one is when the highest-scoring node is selected our chapter on into corresponding targeted cluster is annotated again this time to Signal the candidates killer that it is allowed
00:31:34 [W] Find the candidate pot.
00:31:20 [W] That becomes a delegate pot.
00:31:24 [W] The proxy skater sees that binds to finally binds the proxy pot and the other candidate parts are deleted via the pot chaperones and garbage collection.
00:31:38 [W] In summary, it is possible to build a global kubernative cluster.
00:31:43 [W] You just need super fast networks and some custom built news. But even then three tree colleagues and partners found reasons to use multiple clusters. The Federation around not too loose has over 10,000 cores
00:31:58 [W] Soon be extended by an order of magnitude with the addition of the expense of supercomputer.
00:32:02 [W] Federation uses admiralty which itself uses virtual kubelet and the scheduler framework if you're interested if you're interested in joining the research platform around Nautilus contact them and if you want to
00:32:18 [W] Iteration check out admiralty.
00:32:12 [W] Thank you.
00:32:25 [W] Here, we have a couple of minutes for answering questions.
00:32:30 [W] Hi everyone Thanks for attending and don't hesitate to ask questions in the Q&A with a speaker box.
00:33:09 [W] Okay, we have several questions that just popped up.
00:33:15 [W] Up, just one question and somebody saying things they are.
00:33:21 [W] So how do you handle resource budgets for users? You want to answer that Dimitri?
00:33:38 [W] Came up several questions that just popped up.
00:33:43 [W] Up, just one question and somebody saying things they are.
00:33:50 [W] So how do you handle resource budgets for users? You want to answer that Dimitri?
00:34:14 [W] I don't think budgets are propagated for now.
00:34:18 [W] So that's another thing for admiralty to handle.
00:34:22 [W] Yeah, there would be two levels of resource budgets. Like you can you can have budgets on the source side and sew the and on the target side.
00:34:35 [W] so on the target set you control what the sources can create. How much too.
00:34:43 [W] Sort of skin. You know how many resources they can they can take and on the source that you can kind of control, um resources people can claim in the targets.
00:34:55 [W] So it kind of gives you a both admin skin the can have control on that.
00:35:01 [W] It's interesting interesting use case.
00:35:07 [W] other questions
00:35:10 [W] there's a question of how does animal to defer from SQL cluster Federation.
00:35:15 [W] So admiralty's a control plane application management platform that is compatible with is to Cluster Federation is your cluster Federation is a networking Federation.
00:35:27 [W] So the two concerns that are different orthogonal and they can work together.
00:35:32 [W] But how does animal to defer from is to Cluster Federation? So admiralty is a control plane application management platform that is compatible with is to Cluster Federation is your cluster Federation is
00:36:01 [W] Yeah, we have 30 seconds. I think yeah, I won't go to the question sometimes of them. It's like
00:36:09 [W] okay quickly config Maxim Secrets just follow the bots.
00:36:14 [W] So Telco pedo copied over from source to Target clusters.
00:36:26 [W] So to say the network outages.
00:36:30 [W] Normal handling, like boys will be recreated because correct.
00:36:38 [W] Each PC resource reservation.
00:36:41 [W] There are some plugins that allow HPC style of scheduling. So again, that's up to filter rated cluster how it handles the resources.
00:36:54 [W] Thank you everyone.
00:36:56 [W] That's a great questions and we'll win the SEC channel for further questions. Have a nice day.
