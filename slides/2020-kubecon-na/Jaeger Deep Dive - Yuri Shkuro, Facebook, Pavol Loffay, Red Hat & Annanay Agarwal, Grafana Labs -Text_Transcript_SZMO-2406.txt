Jaeger Deep Dive: SZMO-2406 - events@cncf.io - Thursday, November 19, 2020 2:57 PM - 37 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, welcome to Jaeger project deep dive.
00:00:02 [W] We have three speakers in this talk and we'll go over very many aspects of Jaeger projects but not all of them because the we have a short session and we need to pack a lot of content.
00:00:15 [W] Hi, welcome to Jaeger project deep dive.
00:00:17 [W] We have three speakers in this talk and we'll go over very many aspects of Jaeger projects but not all of them because the we have a short session and we need to pack a lot of content.
00:00:27 [W] My name is uh grow. I'm an engineer at Facebook.
00:00:31 [W] I'm a maintainer of Jaeger and as well as opentracing and opentelemetry projects. All three of them are members of cloud native competing foundation, and I published a book last year mastering distributed.
00:00:42 [W] tracing where you can find more information about the history of Jaeger history of opentracing and as well as kind of introduction into the distribute racing as a discipline first, and then I will give an introduction and then we will talk about
00:00:57 [W] Jaeger features and Jaeger architecture.
00:01:00 [W] I will talk about sampling because this is one area where people have often questions and puddle at the end.
00:01:07 [W] We'll talk about Jaeger integration with opentelemetry and deploying Jaeger is on kubernative. And so first anime take away with the intricate racing.
00:01:20 [W] Thank you Yuri.
00:01:20 [W] Hi everyone.
00:01:22 [W] My name is an inaugural.
00:01:23 [W] I'm a software developer at Griffon a labs and I'm a contributor to the Jaeger and opentelemetry projects today.
00:01:29 [W] We're going to take a look at what distributor tracing is and how it fits into our debugging workflow.
00:01:34 [W] We're also going to learn some Concepts and terminology.
00:01:36 [W] This is a photo of boobers internal architecture, which is generated by your and we can see that in amateur environment. The number of microservices can run very well into hundreds or even thousands each of the green nodes.
00:01:49 [W] Resent in this graph represents a micro service and the gray lines represent the communications between these microservices when we interact with the Uber app a single request of the Uber infrastructure may look something like this and this typically happens billions of times
00:02:04 [W] well into hundreds or even thousands each of the green nodes present in this graph represents a micro service and the gray lines represent the communications between these microservices when we interact with the Uber app a single request of the Uber infrastructure
00:02:30 [W] So what are some of the monitoring tools that we use to monitor such a complex architecture?
00:02:35 [W] Typically, we use a combination of metrics and logs metrics are great because they're aggregate table. They can be used to alert upon and they're a great way to get an overall picture into the performance of the system.
00:02:47 [W] This is a sample metric that I'm exposing from my application.
00:02:51 [W] It's a standard Prometheus metric which gives me the duration or the latency of a request to the system.
00:02:58 [W] And here I can see that this gives me a very high level picture saying that my app of the ice cream shop took 10 seconds of latency for a given request.
00:03:05 [W] However, if I want more fine-grained information about the system, then I can add more and more tags to it.
00:03:12 [W] But very quickly I run into this problem of cardinality explosion cardinality refers to the number of items present in a set and so in the metrics world, this means the total number of values that are given labels at can take as I add more and more labels
00:03:27 [W] More information about my application.
00:03:28 [W] I increase cardinality and this can lead to degraded performance and is also not cost effective.
00:03:36 [W] Logs are also a great way to check the health of a given service but under concurrent requests.
00:03:42 [W] It's really difficult to get the stack trace of a given of one particular request that pass through the service.
00:03:47 [W] And so really we need tracing because traces are like stacktrace debugging for distributed systems.
00:03:55 [W] And it also tells us a story about the system or or it tells us a story of the life cycle of a request passing through the system.
00:04:05 [W] Distributed tracing works on the concept of context propagation on the left. We can see that we have a very simple microservices architecture where an edge service a creates a unique ID for every inbound requests to it
00:04:20 [W] Makes an outfit makes a downstream request. It passes along this UniQue Ideas part of the context as the services do some Quantum of work and generate spans.
00:04:14 [W] They attach this unique ID to the span.
00:04:17 [W] Once they are emitted and Stitch together in the back end we can see the trace as shown on the right. This is formed with the help of the unique ID.
00:04:27 [W] So now let's look at some traces.
00:04:28 [W] So these races are generated from the sample hot rod application that shipped as part of the Jaeger repository. When we click on the system dependencies system architecture diagram or the dependencies diagram.
00:04:43 [W] We see that as a developer. This already gives me a very intuitive picture of the architecture of the system and the request flowing through it. So this shows me the different microservices that are involved and also shows me how many requests were made between
00:04:56 [W] these microservices
00:05:00 [W] next in Jaeger. We also have deep dependency graphs which may look similar to the system diagram at first, but the difference is that now they're filtered by the service which is highlighted.
00:05:11 [W] So in the system architecture we could have we could have calls to reddish from we could have calls from the driver to the redness which may not have originated from the front end.
00:05:22 [W] We can also switch to an operation view in the layout mode over here, which is what is shown here.
00:05:24 [W] Next when we select a trace, this is the typical Gantt chart view of the trace that we see on the left. We see that the services are arranged in a hierarchical manner which shows the transitive nature of requests as generated.
00:05:38 [W] At the top, we have a minimap, which is really useful if the trace has like a couple of thousands man's because we can highlight a given section and it will only show the spans for that duration.
00:05:50 [W] Next we see just with a simple look of this we can see that some of these operations were blocking for instance the front end server service, which called the customer is called the my SQL service probably was trying to retrieve customer information and
00:06:05 [W] Asians were blocked on the service on this request.
00:06:10 [W] Next we can see that the waterfall diagram over here clearly represents a sequential order of operations.
00:06:19 [W] This is also really useful for an application developer and they can look at they can look at the operations and say hey this should have been Paralyzed by is this running in sequence?
00:06:34 [W] We can also see that the parent-child relationships are you know encoded into this view we can see that the parent always encompasses the descends.
00:06:45 [W] Next when I click on a given span it generate some extra information in the form of tags and logs logs can be optionally indexed.
00:06:54 [W] We at the fauna Labs do not index the logs that are ingested into Jaeger, but here you can see that as part of the tags. I can add higher cardinality data. For instance like the SQL query itself.
00:07:07 [W] It would be very difficult to view the query if it was part of the service and operation itself, but here I can add higher cardinality.
00:07:14 [W] down
00:07:18 [W] this is a new view that's arrived in Jaeger, which is the trace div. We can see that the two traces being dipped our give shown at the top.
00:07:28 [W] there must have been something very different about this because even though they're hitting the same end point which is shown at the top and the difference is all of these nodes so we can see that they had a common parent which is the common Gateway and point that
00:07:37 [W] It but from here Trace be all the nodes highlighted in red were not present in Trace be so this is sort of like a visual div that we see in systems like get where the red nodes show what was absent.
00:07:43 [W] Sentient race be compared to trace a and the green node show what is present in trees? We compared to Tracy.
00:07:48 [W] So the operations right at the top over here were more or less common.
00:07:53 [W] And we can see that there's like a substantial div and this is probably because one of these was a success and the other was a failure.
00:08:01 [W] The other view that we have is to actually compare spanned durations.
00:08:06 [W] So in the first view in the previous diagram over here we could see this was more of a node wise view which showed which nodes are part of a given Trace compared to another whereas this view shows more of the latency differences
00:08:21 [W] Is and the darker the node the Starker or the more more contrast between the latency is in Tracy and race be so for instance. If I was a developer looking at these trays use for comparison,
00:08:36 [W] In Trace be this particular node.
00:08:35 [W] Might have been a problem because this seems to have elevated Layton sees come in Trace be compared to trace a and transitively all the other Upstream Services have sort of had a higher latency.
00:08:50 [W] If I hover on top of these it also shows me the difference in latency.
00:08:54 [W] So this is really useful for debugging purposes.
00:08:56 [W] Next. We're going to quickly browse through the Jaeger architecture.
00:09:04 [W] So Jaeger is not just a single binary.
00:09:08 [W] It's a collection of services which help in Trace data collection storage as well as querying and visualization.
00:09:15 [W] So on this broad spectrum on the left, we have client libraries, which are used to instrument the application and they're typically written in the same language as the application.
00:09:25 [W] And so the officially supported libraries are in goal and Java python or C++ and C sharp.
00:09:33 [W] While PHP and Ruby our community maintain libraries and on the right side.
00:09:38 [W] We have the visualization front end which is written in react.js. It's beautiful.
00:09:43 [W] And this and something that we already discussed.
00:09:45 [W] opentelemetry
00:09:48 [W] so a little history about Jaeger eger was inspired by Google's Dapper and open Zipkin.
00:09:54 [W] It was created by Uber in August of 2015 and finally open source in April of 2017 the same your Jaeger joint cncf as an incubating project and it graduated to a top-level cncf project and 2019.
00:10:12 [W] So this workflow shows what requests were part of what information is propagated as part of a regular HTTP call between services and how the trace data reaches the Jaeger back in when service is
00:10:27 [W] is an upstream services or call the downstream service be it adds instrumentation from its end to add the content to add the unique ID into the HTTP context headers and then passes it along to service be
00:10:42 [W] Always bees and it receives the HTTP request parsec out the context information and uses the same Trace ID in the spans that it creates.
00:10:39 [W] Finally the span data that is emitted from service a and service be reaches out of band to the Jaeger back and where it is stitched together to form a common Trace.
00:10:50 [W] In 2017 when Jagger was open sourced the architecture looks something like this on the left. We have the host or container that has been instrumental.
00:10:59 [W] It has the application and the Jaeger Clan which was used for instrumentation.
00:11:04 [W] The Jaeger client sends pans locally to the Jaeger agent sends pans to the Jaeger agent, which may be running locally either on the same machine or as a kubernative sidecar and from here the Jaeger agent.
00:11:19 [W] Send spans two Jaeger collector.
00:11:22 [W] The a good collector is more of a central component.
00:11:24 [W] Whereas a Jaeger agent may be deployed in multiple clusters.
00:11:27 [W] This is done for several reasons because the link between the Jaeger agent and Jaeger collector is cross DC and might break.
00:11:35 [W] So the Jaeger agent can do stuff like buffering and so on the Jaeger collector is more of a central component that sort of receives the spans denormalize has them can perform some additional cleansing of the
00:11:49 [W] data, for example removal of sensitive information and so on and then ingest this into a database
00:11:56 [W] Drawn on the span data ingested that can compute the dependencies tab that we discussed earlier and finally the Jaeger query which queries the database to help visualizing the spans in the UI.
00:12:06 [W] Another important point to note are the red lines in this graph which shows the flow of sampling information.
00:12:13 [W] The Jaeger collector is a central store for all sampling configuration.
00:12:17 [W] And this can be used to define per service sampling per operation sampling and so on the Jaeger client can pull the Jaeger agent which in turn can pull the Jaeger collector and receive sampling information without ever having to rotate conflict maps
00:12:32 [W] So this is really useful.
00:12:27 [W] another important change we made to the architecture was the introduction of Kafka what this means is that we've been able to deep up decouple the ingestion of spans from from the client to the ingestion of spans into the database the Jaeger collector
00:12:42 [W] is that we've been able to deep up decouple the ingestion of spans from from the client to the ingestion of spans into the database the Jaeger collector can induce pans and to Kafka and the egg or ingested can asynchronously consume them and
00:12:56 [W] Exam two Kafka and the Jaeger ingested can asynchronously consume them and insert them into the database.
00:13:02 [W] So if ever we receive a high volume of spans because of increased traffic they can be buffered in Kafka without overwhelming the database the film streaming jobs can also now in Q from Kafka and right dependency information into the database.
00:13:17 [W] Bees the rest of the architecture Remains the Same.
00:13:22 [W] Speaking of Technology stack Jaeger is written in go.
00:13:25 [W] It's a go back in for tracing data.
00:13:27 [W] It has a pluggable storage with support for Cassandra elastic search and Badger databases.
00:13:33 [W] It also has an in-memory store and it has a pluggable storage and uses hashicorp go plug in.
00:13:40 [W] So if there are experts and other databases, you can write a plug-in which can be plugged into the Jaeger collector.
00:13:49 [W] Jaeger uses a react.js front end which is really a feature Rich and beautiful Jaeger has opentracing instrumentation libraries.
00:13:59 [W] It's compatible with all opentracing instrumentation libraries.
00:14:02 [W] It also has strong integration with Kafka and Apache Flink for tracing Analytics.
00:14:09 [W] And with that I pass over to Yuri to talk about sampling in Jaeger. Thank you.
00:14:19 [W] Okay, let's talk about sampling in the distributed tracing.
00:14:23 [W] We use the jump sampling in the classical statistical sounds meaning that we try to select a subset of all individuals or traces from a population of all possible traces in order to estimate certain characteristics
00:14:38 [W] Or more specifically to reason about application before months based on those samples that we've selected the question is.
00:14:44 [W] why do we need to sample?
00:14:47 [W] There are several reasons?
00:14:47 [W] The first reason is that tracing generates a lot of information and storing all of it incurs significant storage costs.
00:14:56 [W] Here's the some napkin math assume that we have a tracing span of about 2 kilobytes on average and we have a server doing 10,000 lire per seconds, right? So that means we already generating.
00:15:08 [W] Megabytes per second of data now assume that we have a hundred instances of that service. So that's 2 gigabyte per second or 170 petabyte-scale day. And that just for one service if you architecture is complex.
00:15:21 [W] It may have hundreds or even thousands of services you can imagine how much data we could generate if we actually were sampling every single request.
00:15:28 [W] The other reason is that if we not sampling than the instrumentation that collects the data from the application by itself introduces performance.
00:15:38 [W] Head here's an example again. If we have a service doing 10,000 QPS than its we have roughly a hundred milliseconds microseconds per request to work with right. And so if the instrumentation takes like five microseconds, then that's already
00:15:53 [W] It on your compute costs and that like if you run in a very large Fleet that that's a significant amount and finally the third reason is that when we collect races that data is actually very repetitive and
00:15:53 [W] Actually, very repetitive and majority of traces look very same.
00:15:44 [W] They have the same shape roughly similar latency measurements and so storing all of them is kind of useless.
00:15:52 [W] We don't get any more insight see Forrester organ. And so that's why we sample however in distributed tracing specifically sampling has a slightly interesting aspect what is called
00:16:07 [W] do so called consistent sampling and what we mean by that is that if we collect spans four trays, then we should either collect all of them across the whole architecture for that given request or we should collect none of them right because
00:16:22 [W] What is shown in this diagram here?
00:16:21 [W] Let's assume we had a system on the left and then we started randomly making sampling this year in this part of the requests and the three nodes happened to sample and the other two didn't and so we got a trace which looks like on the right, but
00:16:36 [W] The broken we have the sum of the nose, they came without a parent and so it's hard to reason about these traces. And so now they're not as useful as if we sample consistently and we get the whole Trace every time or we don't get the whole Trace at all, which
00:16:50 [W] And as far as specific sampling techniques, there are two primary consistent sampling techniques that I used in the industry head based is like most most popular.
00:17:05 [W] Today's of Google's Dapper paper all the modern tracing system support and recently in the last few years.
00:17:10 [W] tell based sampling started appearing as another popular techniques and I'll talk about both of them.
00:17:16 [W] So let's talk about head by sampling or also called upfront sampling the approach. There is very simple when we start a new Trace, let's say when we generate a brand new Trace idea because the incoming request didn't have any Trace ID then we make a sampling.
00:17:32 [W] Decision that that same time and we capture that sampling decision in the trace context which is propagated throughout the request right this way.
00:17:40 [W] We guarantee that the trace is consistent to sample as long as all the SDK is on the path of the call graph respect that something decision and capture the data accordingly that implementation has minimal overhead when the trace is not
00:17:55 [W] Again, we propagate the flag saying don't collect anything and so all the calls to the trace in the SDK become no opened a very cheap and so we don't affect performance by that and they don't collect any data.
00:18:03 [W] It's also fairly easy to implement because if you think about it, the the code is really you just make a probabilistic decision or some other algorithm to decide when to sample and then you just pass it around and all the other Downstream is Decay. They just respected decision.
00:18:19 [W] Cover that approach also has a couple of drawbacks one is it's not as good as at capturing various anomalies.
00:18:25 [W] For example, let's say you looking at your metrics and you CU Chi 99 latency spiking suddenly. So you want to say okay you can I find some traces that represent that Spike. Well P99 already means one and a hundred and if we sampling with the rate of 1%
00:18:40 [W] With sampling with a rate of 1 percent. That means that our total probability of actually catching your P99 late in the trays is 1 in 10,000 right now if your traffic is very high to service that you have sufficient number of
00:18:51 [W] Captured even with that probability then you probably can get some example. The more rare the outliers are the less chance you actually capture them with the like uniform probabilistic sampling approach and the second big drawback of upfront sampling
00:19:07 [W] It cannot be reactive to how the request behave and architecture because the sampling decision needs to be made at the very beginning when we know nothing about that request.
00:19:16 [W] Maybe we know like which end point was he threated at best but nothing else and we definitely don't know what's going to happen to the request in a life cycle. But we already made the sampling decision and every Downstream service has two respected and so it's very hard
00:19:31 [W] You will know like which endpoint was hit right at best but nothing else and we definitely don't know what's going to happen to the request in the life cycle. But we already made the sampling decision and every Downstream service has two respected and so it's very
00:20:07 [W] Of react to errors in a front sampling, what about sampling had based sampling in Jaeger?
00:20:14 [W] So Jaeger is decays out-of-the-box support that and they come with a assortment of different Samplers such as like always on always off or probabilistic the common most commonly used or rate limiting meaning. Let's sample
00:20:29 [W] Our second no more things like that. Right and the benefit of that is those samples are very easy to implement.
00:20:34 [W] However, the downside of sort of configuring sampling in this way is that when when you have a service and you instantiate Tracer you have to give it a sample with a specific configuration, which
00:20:49 [W] If you have thousands or hundreds of services in our organization, then all those decisions are made by individual developers.
00:20:54 [W] They're kind of sticky because once you deploy that stays in production with that whatever probability or rate that was assigned and the developers, they usually don't know what affect individual sampling rate may have on your tracing back and can you trace it
00:21:09 [W] Been raised myself on your tracing back and can you trace him back and actually support that level of something right?
00:21:12 [W] So it seems like a disconnect between the interest of the back and then the capacity with and how the sampling is configured in the sdks.
00:21:20 [W] And so for that reason Jaeger SDK is actually default to different type of sample, which is called remote sampler and what that means is that it actually reads the configuration from the central tear from from the collectors in Jaeger
00:21:36 [W] Such that the configuration can be controlled in a central place. And then it's the team the transit racing back and can actually determine how much sampling for which service for which end point should be happening. Right and you can change it on the fly if you want to but the point is that you
00:21:51 [W] His for which end points should be happening right and you can change it on the fly if you want to but the point is that you centralize the configuration rather than having it all done at the edges of your traffic.
00:21:59 [W] Here's an example of a configuration for that type of sampling. So on the top left, we have a default sampling strategy which would apply to any service unless otherwise configured with something else right? So here we see
00:22:14 [W] You it says like everyone should use probabilistic.
00:22:17 [W] Something was 50% straight probability right except you can also provide some overrides for very specific operations.
00:22:24 [W] So let's say we don't want to sample anything on the health or on the metrics and point, right? And so here we give them probability of 0 on the right side.
00:22:33 [W] is a specific overrides that we can do per service. So if we know some services that let's say maybe our full service here is very like look,
00:22:44 [W] Q PS and so we give it a higher probability of sampling but on the other hand we can also override individual operations on that service. Now, let's talk about tale by something. So Intel based sampling as as the name implies,
00:22:59 [W] Is made at the end of the trace rather the beginning what that means is that when we make a decision when we actually already observed the whole trays and that decision made make can be a lot more intelligent because
00:23:14 [W] And that decision made make can be a lot more intelligent because we can look at latencies that we've seen on the trays we can see you have there been any errors?
00:23:15 [W] Maybe there's an unusual call graph shape etcetera so we can be very Advanced with that decision and only and basically either will instead of capturing samples uniformly we can say let's steer them towards anomalies late if you have a like a long
00:23:30 [W] children and only and basically either will instead of capturing samples uniformly we can say let's steer them towards anomalously if you have a like a long latency or if you have an error, let's always capture that example, so that gives us control over what kind of
00:23:48 [W] We have in there. Let's always capture that example. So that gives us control over what kind of data we can get into the backend.
00:23:55 [W] That means that we can also catch a number is much easier than with the upfront or had based sampling. And another benefit is that because we are getting all this data into collectors before we make a sampling decision.
00:24:09 [W] We actually dealing with a lot more data that on which we can run various aggregations.
00:24:13 [W] Let's say we want to compute some statistical aggregations of like what late
00:24:17 [W] Agency, we observable histograms.
00:24:20 [W] We see in and services, right?
00:24:21 [W] If we do the before samples that we just have more data to work with and those Aggregates will be more accurate than if we were doing them after the sampling especially if we're doing after sampling which is not uniform but like skewed towards anomalies
00:24:36 [W] Approach is that because we need to collect the whole Trace before we can make a sampling this usually it means we need to store it somewhere big right because traces are distributed and they are produced in the individual pieces from multiple services.
00:24:50 [W] So you kind of have to collect them all in one place.
00:24:52 [W] You have to hold on to that and until you receive all the data for that race, which is also unlike somewhat indeterminate time potentially and typically in modern.
00:25:06 [W] Systems that support tale by sampling this is done in never liked you start racing in memory.
00:25:10 [W] Then you make a sampling decision. And if the decision is no you just thrown them away expire from the membrane like because most of the requests are very short-lived you can actually scale that system fairly well to very large traffic of inbound traces
00:25:25 [W] The other downside of tell by sampling is that if the goal is actually to collect all the data upfront and then sample only so much so that our storage can support then all this collection upfront introduces an additional
00:25:34 [W] On the application itself, right as I mentioned in a napkin Mass before you can have up to five to like 8 percent overhead on very high QPS Services. If you essentially collect data on every single request
00:25:41 [W] I have to like 8 percent overhead on very high Q Pi Services. If you essentially collect data on every single request and Export it into collector tear before you make the sampling decision.
00:25:51 [W] So it's a bit expensive in terms of Jaeger support.
00:25:56 [W] So Jaeger as we'll talk later is moving towards building most of the Jaeger beckons on top of opentelemetry collector and opentelemetry collected does have a logic for
00:26:08 [W] For tail based sampling.
00:26:10 [W] All right, you can configure various sampling roles already based on bike latency or certain tags like the are flags.
00:26:15 [W] Unfortunately at this time.
00:26:18 [W] It's only in a single node mode. So sort of you you can run a single service and soon send all the traces there, then it will work. But if if your traffic as such so large that that service cannot scale to it and you need to run multiple
00:26:33 [W] Then it will work. But if if your traffic as such so large that that service cannot scale to it and you need to run multiple collectors.
00:26:40 [W] Normally they are stateless.
00:26:41 [W] So it is not a problem. But in case of a tail based sampling, they are become stateful and there needs to be some sort of a shard in solution which is right now not available, but it will be available in the future.
00:26:51 [W] They're already prototypes in opentelemetry.
00:26:53 [W] is all about something and now Pablo will talk about
00:27:03 [W] Jaeger and opentelemetry integration
00:27:07 [W] Hello everybody.
00:27:08 [W] My name is Paolo VI. I'm software engineer at traceable Ai, and I'm Cora maintainer of Jaeger project and contributor to opentelemetry and opentracing projects in this section.
00:27:19 [W] I will talk about Jaeger and opentelemetry integration.
00:27:24 [W] Before we deep dive into opentelemetry.
00:27:26 [W] Let me quickly talk about opentracing so we better understand opentelemetry as the next evolution of opentracing and data collection libraries in general.
00:27:38 [W] So on this slide we see basically two parts on the bottom. There is our tracing infrastructure collecting disability traces from the user application.
00:27:48 [W] And on the top, there is user application process that is instrumented with opentracing opentracing is as a specification that tells what kind of data should be collected from the
00:28:04 [W] Databases and so on but it is also an instrumentation API that sits in between user application code and the tracing Library implementation.
00:28:16 [W] The tracing Library implementation is basically the implementation of the opentracing API.
00:28:21 [W] So this architecture allows us to change tracing system without changing all the instrumentation points that are embedded into our PC Frameworks, but also in our application code.
00:28:37 [W] However, there is one downside that we want to do that. We still have to recompile and redeploy our application.
00:28:46 [W] And this might be a problem if we have you know, dozens maybe hundreds or even thousands of microservices.
00:28:52 [W] It can be very costly thing to do.
00:28:54 [W] The problem is opentracing.
00:28:56 [W] It doesn't Define any data format. So all the tracing implementations these different data formats.
00:29:06 [W] So let's have a look at the opentelemetry.
00:29:11 [W] So BBC basically the same architecture as from the as on the previous slide, but opentelemetry is now now substitute the instrumentation a Bia API, but also the implementation of
00:29:26 [W] But we see also opentelemetry in the agent and collector.
00:29:20 [W] So the difference between opentracing opentelemetry is that opentelemetry defines the API but also defines the SDK the implementation of the API and it also defines a data format that is exported from the SDK
00:29:37 [W] This allows us to have an opentelemetry collector which accepts you know, the this data format and then can translate it to different data formats for different racing systems.
00:29:36 [W] And this pattern allows us to change tracing system without recompiling and redeploying our applications.
00:29:49 [W] So maybe little bit confusing for jaeger users is that we see opentelemetry logo in the agent and collector and this is purely Jaeger decision because in Jaeger project we have decided to
00:30:04 [W] Agent in the collector in gesture basically all our bacon components on top of opentelemetry collector.
00:30:08 [W] So this way all Jaeger bacon components will provide the same functionality that is available in The opentelemetry Collector and we will just add Jaeger specific functionality to it for example storage media mentation.
00:30:25 [W] So let's talk more about opentelemetry collector The Collector itself is written in go bowling.
00:30:31 [W] It's Jaeger back and components and in terms of Jaeger integration, we basically
00:30:40 [W] Rebased Auerbach and components on top of opentelemetry collector and we have added Jaeger specific functionality to it.
00:30:50 [W] So now we agree users will benefit from all the functionality that is available in The Collector, but they will still be able to use all the current functionality of Jaeger components.
00:31:05 [W] We also we also want to make it very easy for users to migrate to these components.
00:31:10 [W] So we will keep the current, you know architecture with agent collector in gesture and all-in-one and also probably the same configuration options.
00:31:20 [W] So let's talk more about opentelemetry collector The Collector itself is written in go blank.
00:31:20 [W] It's Jaeger back and components and in terms of Jaeger integration.
00:31:26 [W] We basically rebased our back-end components on top of opentelemetry collector and we have added Jaeger specific functionality to it.
00:31:38 [W] So now we agree users will benefit from all.
00:31:43 [W] All the functionality that is available in The Collector, but they will still be able to use all the current functionality of Jaeger components.
00:31:54 [W] We also we also want to make it very easy for users to migrate to these components.
00:31:59 [W] So we will keep the current, you know architecture with agent collector in gesture and all-in-one and also probably the same configuration options.
00:32:11 [W] If you are interested on our website, there is already a section for opentelemetry where you can read what kind of configuration options are provided. But also there are some guidelines and you can start using these new components right now.
00:34:51 [W] Raishin options are provided. But also there are some guidelines and you can start using these new components right now.
00:35:00 [W] So let's talk about Jaeger and opentelemetry sdks relationship.
00:35:06 [W] So opentelemetry sdks, they usually support Jaeger grpc exposure and Jaeger propagation format.
00:35:13 [W] So this basically allows you to use or to deploy services instrumented with opentelemetry into an ecosystem where you are using Jaeger clients.
00:35:26 [W] Then there is opentracing shin which is incomplete just basically and opentracing implementation that uses opentelemetry sdks. And this allows you to use all existing opentracing instrumentation libraries
00:35:41 [W] Use all existing opentracing instrumentation libraries with opentracing open Telemetry SDK.
00:35:49 [W] And last but not least Jaeger clients. They support w3c Trace context which is the default propagation format in opentelemetry as the case.
00:35:59 [W] So you will be able to use Jaeger clients in a new ecosystem with we're open till Victory sdks are used.
00:36:11 [W] Okay, let's move to different topic which is eager and kubernative.
00:36:16 [W] You could provides an excellent integration with kubernetes and you can deploy Jaeger into kubernative by using Helm charts plane kubernetes manifest files and also a grill Operator
00:36:31 [W] Most advanced method Harbor you can deploy Jaeger into kubernetes.
00:36:36 [W] So it follows the standard operator pattern where where first you have to deploy a girl operator create custom resource definition for it. And then you will be able to create custom resource
00:36:52 [W] Worry Define, what kind of parts of Jaeger deployment or how the Jaeger deployment should look like?
00:36:51 [W] So for example in the custom resource, you can Define that you just want all in one deployment or even the production deployment with the storage back-end.
00:37:03 [W] You could operator can also provision storage bag hands under some conditions.
00:37:09 [W] for example, if the cluster if you have deployed in the cluster Kafka or strimzi operator Jaeger operator will be able to how to provision a Kafka cluster for you.
00:37:24 [W] Okay, this is everything from my side and thank you very much for your attention.
00:37:31 [W] Thank you.
00:37:32 [W] Bubble.
00:37:32 [W] This is the end of our talk.
00:37:34 [W] These are the different ways you can get in touch with the Jaeger maintainers and Community.
00:37:38 [W] We have a bi-weekly meetings where you can dial in and participate in discussions and make sure to start the projects on GitHub developers like those Stars.
00:37:47 [W] Thank you so much for joining.
