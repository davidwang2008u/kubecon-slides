Building, Managing and Automating Clusters at Scale With Prow: MRDT-0769 - events@cncf.io - Friday, November 20, 2020 3:12 PM - 38 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, thank you for coming to my talk.
00:00:01 [W] I'm Mike's plane and we're going to be talking about building managing and automating clusters at scale with crafts.
00:00:31 [W] Hi, thank you for coming to my talk. I'm Mike's plain and we're going to be talking about building managing and automating clusters at scale with craft.
00:00:39 [W] So a little about me real quick.
00:00:41 [W] I can be found online and at Mike's plane. I work as a cloud infrastructure lead at Sonos previously.
00:00:48 [W] I've worked at Berkeley and PayPal where my container and kubernative is work first began.
00:00:53 [W] I've been using kubernative before the one dot ODS in various forms such as the good old kubernative May so's project and multiple Homegrown Solutions eventually this led to working with cops and become a cop's maintainer.
00:01:05 [W] also founded the Boston kubernative meet up.
00:01:09 [W] And hope to continue that when this is all over.
00:01:13 [W] So, it's Sonos.
00:01:14 [W] I want to talk a little bit about our journey into kubernative.
00:01:17 [W] So, let's see how we got here.
00:01:19 [W] application teams at Sonos build manage and deploy their services in production these teams often deploy multiple times per day now with over 29 million products around the world this leads to some very interesting Cloud infrastructure,
00:01:35 [W] And the kubermatic so let's see how we got here.
00:01:38 [W] application teams at Sonos build manage and deploy their services in production these teams often deploy multiple times per day now with over 29 million products around the world this leads to some very interesting Cloud infrastructure,
00:01:54 [W] To duplicate each other's code, which we really want to fix and speed up deploy times as well. And that's where kubernative comes in.
00:02:04 [W] So I often get this question.
00:02:06 [W] So I start by building a cluster, right?
00:02:08 [W] Not quite now let me explain. There's quite a few things to consider when you're building your kubernative clusters.
00:02:16 [W] Let's start with site or blocks and networking to other resources.
00:02:19 [W] You may need to Define them based on your company and your cultures set up. So you may need to make a lot of adjustments and changes their DNS is pretty critical for any service that wants to be externally accessible as well as security
00:02:34 [W] Duration for users and automation access you may want to enable our Alpha and beta it kubernetes API features as well that requires certain flags that you'll need to understand and version as well.
00:02:47 [W] Many times to have a successful cluster you require things like cluster monitoring logging and alerting.
00:02:53 [W] Auto scaling and node removal is pretty critical of my mind for kubernative.
00:02:57 [W] That way my team doesn't have to manage individual instances and we can focus on managing the Clusters in the automation behind it.
00:03:07 [W] The problems that I've heard run it people run into when they build clusters first is clusters become their source of Truth rather than putting it in code like a regular developer. Where would we try to focus on putting everything in the cluster?
00:03:22 [W] Cloud providers often sometimes also become the source of Truth when you use tags or certain automation that relies on their history you really then can't repeat that through the rest of your environments this relies on humans to decouple and manage the infrastructure
00:03:37 [W] Only doesn't scale very well. So let's get back to the basics. Here are our requirements and we went to go build our kubernative clusters.
00:03:33 [W] First of all, we wanted everything to live and get all the configuration can be checked in managed via Source control and pull requests.
00:03:41 [W] We wanted everything to be item potent.
00:03:43 [W] You can run it as many times as you need to to make sure that everything is up to date almost every cluster should be as in identical as possible.
00:03:51 [W] There will obviously be differences between some clusters here and there which allows us to try out things like different networking providers or other cluster resources. However, they need to be configured and
00:04:03 [W] Checked in the exact same way.
00:04:06 [W] That leads me to the configuration.
00:04:08 [W] We really want to be able to check in and manage individual configurations of clusters as we need to in general.
00:04:14 [W] We should be able to abstract. This across multiple clusters of the same type say production clusters versus test clusters.
00:04:21 [W] However, there are many times where you're going to want to automate and test specific configurations on specific clusters over time.
00:04:29 [W] We really wanted to embrace fast iteration much like modern application teams devops needs that as
00:04:36 [W] And our debit devops teams here at Sonos really focus on building kubernative in a way that we can move quickly.
00:04:42 [W] Robin to in the cloud all of our tools are version and all of our changes are deployed with code even API changes that may be difficult to manage. We try to write scripts and code to manage their upgrades across our environment environments.
00:04:57 [W] So let's talk about our tools first.
00:05:00 [W] We have cops. We did consider many other options at the time.
00:05:03 [W] However, cops has a large community and is multi-cloud.
00:05:07 [W] So it was a pretty easy pick for us.
00:05:10 [W] We then went on to terraform to build all the rest of our Cloud resources things like Route 53 zones S3 buckets.
00:05:17 [W] I am rules for things like low key and Thanos that you may want in your cluster complex networking changes and other custom tooling things that we might want to add on the Fly.
00:05:28 [W] Maybe we want to be able to schedule actions on our Auto scaling group so that all of them spin down on the weekends things that we don't really have to over engineer and we know how to do it in terraform so we can easily just add that whenever we would like
00:05:41 [W] The next bit we added is Helm file and Helm which drive a lot of our cluster components and how they run Helm is used to deploy to the Clusters while Helm file describes how we deploy to those clusters.
00:05:54 [W] This really gives us the flexibility to decouple in the future and deploy individual components as we see fit.
00:06:00 [W] We can quickly Implement workarounds for things like API changes or breaking Helm chart versions, that would be difficult unless we have this Automation in place.
00:06:09 [W] Things that we don't really have to over engineer and we know how to do it in terraform so we can easily just add that whenever we would like.
00:06:10 [W] The next bit we added is Helm file and Helm which drive a lot of our cluster components and how they run Helm is used to deploy to the Clusters while Helm file describes how we deploy to those clusters.
00:06:24 [W] This really gives us the flexibility to decouple in the future and deploy individual components as we see fit.
00:06:30 [W] We can quickly Implement workarounds for things like API changes or breaking Helm chart versions, that would be difficult unless we have this Automation in place.
00:06:41 [W] We also use y q and a bunch of custom scripts to help manage our different values that drive our clusters.
00:06:48 [W] These values grab specific bootstrap components that allow us to figure out which components we should run in. What cluster
00:06:58 [W] On top of that all of these tools are versioned.
00:07:02 [W] This allows us to upgrade individual components on individual clusters as we want tools are pulled down and execute a dynamically within the repo similar to tools like ASDF more on that in a little bit.
00:07:17 [W] So let's talk about bringing up a cluster.
00:07:23 [W] This is what bringing up a cluster at Sonos looks like on the left. You see where we run the source command this sets up our environment AWS credentials and secrets on the right.
00:07:34 [W] You see the actual workflow that is involved in bringing up and managing our clusters first. We run make cluster which you could break down into smaller pieces, like make cluster up or make Helm file sync, but one command will also suffice.
00:07:49 [W] First it templates and and manages the configuration. Then we spin up the cluster using cops Creator cops update depending on the type of workload. Then we terraform applied to add our additional changes to the cluster.
00:08:01 [W] Once the Custer is up and running.
00:08:04 [W] Talk about bringing up a cluster.
00:08:09 [W] This is what bringing up a cluster at so nose looks like on the left. You see where we run the source command this sets up our environment AWS credentials and secrets on the right.
00:08:20 [W] You see the actual workflow that is involved in bringing up and managing our clusters first. We run make cluster which you could break down into smaller pieces, like make cluster up or make Helm file sync, but one command will also suffice.
00:08:35 [W] First it templates and and manages the configuration. Then we spin up the cluster using cops Creator cops update depending on the type of workload. Then we terraform applied to add our additional changes to the cluster once the cluster is up
00:09:24 [W] It doesn't have a copy of a specific version tool.
00:09:26 [W] We will go download that tool and cache it locally when the Tool is needed this way.
00:09:31 [W] We ensure every human or CIA user has the exact tool versions. They need this is very useful when utilizing tools like cops or terraform that can contain significant changes between versions. This can also easily be cashed in CIB between runs.
00:09:48 [W] This is what it looks like in code.
00:09:50 [W] So simply you declare which cluster you would like to operate on you run the source command. That way we get our credentials and everything and then we run the make cluster command.
00:10:04 [W] This is what that's in command. Looks like when we run it and see I
00:10:07 [W] We'll get back to this a second if you're not familiar, but you may notice this looks much like a kubernative pots back.
00:10:13 [W] If you look in the center there, you'll see the command make cluster.
00:10:21 [W] Now you may say what about day to what happens after that?
00:10:25 [W] You have to worry about updates and upgrades things like cops kubernative itself. Terraform. Helm Helm charts keep CTL. That's a lot of things to manage. Right?
00:10:38 [W] What about adding new features things like metrics logging and security?
00:10:43 [W] How are we going to test all this and make sure all this stuff works.
00:10:47 [W] Are you feeling overwhelmed because we were to the best thing we figured we could do is add as much structure to this as humanly possible.
00:10:54 [W] The worry about updates and upgrades things like cops kubernative itself. Terraform Helm Helm charts keep CTL.
00:11:03 [W] That's a lot of things to manage. Right?
00:11:06 [W] What about adding new features things like metrics logging and security?
00:11:11 [W] How are we going to test all this and make sure all this stuff works.
00:11:15 [W] Are you feeling overwhelmed because we were to the best thing we figured we could do is add as much structure to this as humanly possible.
00:11:23 [W] That's where gitops comes in. It allows us to drive automation through pull requests and easily manage many small changes over time. We can run thousands of tests to make sure our code Works to the best of our ability.
00:11:38 [W] That's where proud comes in prowl is the kubernetes ci/cd Stan built and maintained by the kubernative Sig testing team built on similar principles to kubernative itself.
00:11:50 [W] Now this is a screen shot of what proud looks like.
00:11:53 [W] This is a simple cluster I spun up locally, but you can also see that at proud at Kate's dot IO.
00:12:01 [W] Up top, you see the jobs that have run recently in the the length of time.
00:12:04 [W] They've taken to run and down below. You can click into specific jobs if you're not familiar.
00:12:11 [W] Let's dive a little bit into proud self pro has job execution very similar to kubernative.
00:12:18 [W] It allows you to Define pods specs that it adds specific components due to make it much easier to automate it allows you have plugins and / bot commands in PRS, which makes it really easy to add additional tooling whenever you see fit everything for
00:12:33 [W] Little bit into proud self prowl has job execution very similar to kubernative.
00:12:38 [W] It allows you to Define pods specs that it adds specific components due to make it much easier to automate it allows you to plugins and / bot commands in PRS, which makes it really easy to add additional tooling whenever you see fit
00:12:54 [W] You can automate your prowl updates as well as the jobs themselves via changes in Source control.
00:13:00 [W] We really also like that you can have jobs to find inside the proud repo configuration itself as well as in the application repost that allows us to configure jobs that can make sure those other jobs that live outside the prowl repo itself are
00:13:15 [W] Because that allows us to configure jobs that can make sure those other jobs that live outside the prowl repo itself are properly defined.
00:13:20 [W] We also find the proud is very fast and scalable.
00:13:23 [W] This allows us to easily upgrade proud whenever we see fit and it runs in high availability, which is very useful in the cloudbees days is everyone would know.
00:13:33 [W] We also really appreciate the merge management feature which allows multiple pull request to be approved at the same time tested together and merge all at once below. You can see a couple links to Proud self and the GitHub link to the proud section under
00:13:50 [W] Now let's talk about the types of jobs that you can run in proud the main types of jobs.
00:13:54 [W] You can run our priests emits post cements and periodic table submits run on a pull request when that pull request is open. There's presubmit skate kicked off. You can also Define that certain priest of mints are required before in the the pr can merge versus not
00:14:10 [W] Post cements run on a branch or after emerge in general you defined the branch that you want those posts emits to run on and they will run as soon as code is pushed those branches and GitHub.
00:14:22 [W] Periodic are just what they sound like. They're periodic jobs that run on a cron much like cron jobs.
00:14:30 [W] So let's take a look at a presubmit example.
00:14:33 [W] This is an example that we took for from the test in for repo, but we use it to test our Gamal as well.
00:14:39 [W] You can simply see here it is to set to always run and it pulls down a llamo lint tool then we Define a config for that emmalin and we pass in a couple values files as well as folders for it to check.
00:14:56 [W] This is another example of a pretty cement job.
00:14:58 [W] This one's a little bit more complex.
00:15:00 [W] In this case.
00:15:01 [W] We have a script that we use to build and test our cluster configuration in a kind cluster. If you are not familiar with kind I would check it out.
00:15:11 [W] It's very great. It stands for kubernative and Docker and we use a very heavily to test our cluster configurations.
00:15:16 [W] To quickly build and deploy clusters and their configurations and get test results before waiting for a full ec2 cluster to spin up.
00:15:27 [W] Other priests submit jobs that we run we lint all of our Helm files.
00:15:32 [W] We lint all of our config files themselves.
00:15:33 [W] We also run a home file upgrade test which at the end of it.
00:15:38 [W] It'll actually output a diff of what the difference is between the before and the after of the change have that way. We can easily see what the changes are.
00:15:49 [W] We also have Tara form validation and we can really add jobs very quickly whenever we want.
00:15:55 [W] Now pull requests are tested in about five to ten minutes. And once they're approved their batch from errors using tide, which is a proud component now, we're not going to get into some of the components, but I'll send I'll give you some links at the very end of my talk pointing to some some great talks that we've had
00:16:10 [W] We also have Tara form validation and we can really add jobs very quickly whenever we want.
00:15:50 [W] Now pour quests are tested in about five to ten minutes. And once they're approved their batch from errors using tide, which is a proud component that we're not going to get into some of the components, but I'll send I'll give you some links at the very end of my talk pointing to some some great talks that we've had at
00:16:22 [W] now let's take a look at a post submit job you can see it has a mass concurrency of 1 which means it'll wait for the first job to finish you also can see that there's a run of changed this allows us to check if a given folder or specific file has changed and only run in
00:16:37 [W] You can see it also runs on our develop branch and runs the exact same commands as before.
00:16:43 [W] This is an example job that we run whenever we merge code into our kubernative automation repo from our from any branch into develop.
00:16:54 [W] Now you're probably saying that's great and all but how does that scale? Do you really want to upgrade all clusters when you're merging to a branch?
00:17:02 [W] once you want to get it in some way. Let's talk about a real world example.
00:17:07 [W] This pattern let us down the path of separating our configuration from our automation code.
00:17:11 [W] So let me explain first of all, we have a repo for Automation and tooling of our scripts. This follows get flow has releases with the develop Ranch for development and a main branch representing our major releases.
00:17:24 [W] We also have a repo for our cluster configs that can be managed with a separate Cadence that main branch represents what is running in our clusters?
00:17:34 [W] Now those cluster configurations now contain a gift shop that represents the code from our automation repo that is running in the cluster.
00:17:43 [W] This allows us to manually deploy specific code and specific branches for testing.
00:17:49 [W] It also allows us to hold off risky changes from reaching production and specific environments until our teams are ready similar to earlier.
00:17:58 [W] This is what one of our post submit proud jobs looks like first, you can see that we have a run of changed here.
00:18:04 [W] This allows us to trigger jobs only based on values changes and Shout file changes over here. You see the branch our main brands that this would run on.
00:18:15 [W] Of course, we'll need somewhere to send any of our errors in slack when they may occur now down here you'll see that we have our magic deploy hashicorp.
00:18:44 [W] So that sounds great and all but there's one more piece to this puzzle.
00:18:48 [W] Let's talk about the auto bumper. The auto bumper takes care of the pesky updates to the get shots in our config repo and we'll open PR's to update the cluster from time to time. Let's take a look.
00:19:02 [W] Now if you're familiar with the test in for repo under the kubernative Zork, you may have seen jobs like this.
00:19:08 [W] This runs a similar Auto Bumper To What We Run The Auto bumper runs to update specific values in specific files.
00:19:17 [W] So we extended the auto bumper to look for successful proud job deploys to our clusters. Once we have a successful deploy.
00:19:25 [W] We open a PR with a new gift shop if it's changed.
00:19:29 [W] Now if you're familiar with the test in for repo under the kubernetes orc you may have seen jobs like this.
00:19:13 [W] This runs a similar Auto Bumper To What We Run The Auto bumper runs to update specific values in specific files.
00:19:21 [W] So we extended the auto bumper to look for successful proud job deploys to our clusters. Once we have a successful deploy.
00:19:30 [W] We open a PR with a new gift shop if it's changed.
00:19:35 [W] These are the jobs associated with our config Repose pull requests these first three jobs run Helm lint on our configuration validate our terraform and Gambol in our values to make sure everything is set before we upgraded cluster
00:19:56 [W] These first three jobs run Helm lint on our configuration validate our terraform and yeah mole in our values to make sure everything is set before we upgrade a cluster.
00:20:07 [W] These are all defined in what's called in repo config in a DOT proud.
00:20:13 [W] am of file in the future. We hope to add additional jobs that will run cops update without applying changes and Helm upgrades as a dry run that way we can ensure those changes are prepped and ready to go.
00:20:25 [W] As well this job is to find that in our centralized proud repo and will confirm that our DOT product demo file in this config repo is formatted in configured properly.
00:20:38 [W] Sometimes if you miss configure a job, this may be the only job that successfully runs which will give you a heads-up that you may have a typo or a broken something.
00:20:51 [W] Finally we have tied which is a proud component type will wait for LG TM and approved labels from your teammates.
00:20:58 [W] This can be useful to let anyone in your or gal GTM a release if they feel it benefits them but must be approved by a defined approver from the owners file in order to Auto merge if changes happen in the Upstream Branch between the
00:21:13 [W] France was created and merged tide will ensure your tests are rerun with the post-merge config prior to actually merging.
00:21:22 [W] These Auto bumper jobs run as periodic some few times a day those jobs look for other successful kubernative cluster deploys. Once they are successful P ARS are opened and get shots are updated automatically
00:21:37 [W] Go take a look at those pull requests take a look at the tip of the kubernative automation code that has changed between the version updates. Then we will approve those pull requests the merge will occur and at that time the cluster will all
00:21:39 [W] Medically, this workflow is worked out really well for our teams at Sonos.
00:21:42 [W] So let's talk about some of our future goals first.
00:21:45 [W] We want to improve and contribute our otter bumper changes Upstream into test Improv.
00:21:51 [W] They apply to a various number of use cases and we think that other teams could really benefit.
00:21:57 [W] We also want to use test grid to view our test results and deploy configurations through our clusters.
00:22:06 [W] We also want additional slack Integrations such as announcing before deploys are going to occur.
00:22:15 [W] We also want to set up a job template. That way we can automatically generate all the proud jobs. We need for different use cases.
00:22:24 [W] We also want better contents of our poll requests. That way it's really easy for us to review changes that automation drives.
00:22:34 [W] Now here are a few resources that I think could be useful for you.
00:22:37 [W] The first is linked to the publicly accessible proud dashboard for kubernative. Next. You'll find the code test info repo where you'll find the actual prowl code. Third. I've open source of tool that will make it really easy to spin up a pearl cluster using kind
00:22:52 [W] Better contents of our poll requests that way it's really easy for us to review changes that automation drives.
00:22:58 [W] Now here are a few resources that I think could be useful for you.
00:23:02 [W] The first is linked to the publicly accessible proud dashboard for kubernative next.
00:23:07 [W] You'll find the code test info repo where you'll find the actual proud code. Third. I've open source of tool. That'll make it really easy to spin up a pearl cluster using kind locally.
00:23:18 [W] Finally there were some really great talks that have been given at previous Cube cons about prowl. I encourage you to take a look.
00:23:25 [W] Thank you for your time specifically at Soto's. I'd like to thank our team Scott. Makalah David Muckle Dan Miller medicine and Chris Celebi.
00:23:34 [W] Thank you for having me and coming to my talk now.
00:23:37 [W] Let's take some questions.
00:23:44 [W] Alright, like I like I said, I'm Mike sling.
00:23:50 [W] It's very weird to watch yourself.
00:23:52 [W] Give a presentation. You were pre-recorded a while ago, but let's take some questions.
00:23:56 [W] I answered a couple in chatbots.
00:23:57 [W] I'm going to try to go through some of these now, so let's start with there's a couple questions in here about other tools.
00:24:08 [W] So so let's start there. Did you consider other tools such as Argo and flux
00:24:14 [W] And then also, how is this different than Jenkins especially in see either some great questions.
00:24:20 [W] We did consider Argo in flux. We also did use Jenkins originally for a lot of this stuff and so notice we're very heavy Jacobs users across the board.
00:24:29 [W] there's a couple things here we both we like to proof of Concepts and things try some things out when we see there's value especially when it's gitops and we can kind of do a proven process and you know,
00:24:43 [W] And chatbots.
00:24:44 [W] I'm going to try to go through some of these now.
00:24:48 [W] So let's start with there's a couple questions in here about other tools.
00:24:54 [W] So let's start there.
00:24:56 [W] Did you consider other tools such as Argo and flux and then also how is this different than Jenkins especially in see either.
00:25:05 [W] Those are some great questions.
00:25:05 [W] We did consider Argo in flux. We also did use Jenkins originally for a lot of this stuff and Sonos were very,
00:25:13 [W] Heavy Jacobs users across the board.
00:25:15 [W] So there's a couple things here. We both we like to proof of Concepts and things try some things out when we see there's value especially when it's gitops and we can kind of do a proven process and you know, we can orchestrate
00:26:21 [W] um, we found proud to be very, you know, it had a lot of potential so we decided to give it a shot and and there's there's really not a lot different besides some of the gitops stuff that probably really good at
00:26:37 [W] Versus something like Jenkins or even Argo or flux. It really just was it's in use it's complete essentially, you know, and it's very active and when we initially looked at some of the other projects out there, they just
00:26:52 [W] Mmm, they were still investigating on a lot of the features that we wanted in terms of gitops and things like that.
00:26:57 [W] So I think long term we will probably look that direction.
00:27:02 [W] But right now a lot of the tooling that proud provides just made it simpler.
00:27:07 [W] We don't have to spend as much time thinking about, you know, click Ops or going and checking a job in Jenkins or what not focusing on writing the job and just silly yamo as much as I don't want to
00:27:20 [W] right.
00:27:21 [W] Yeah Moe anymore.
00:27:22 [W] All I have to do is, you know write some decent ammo just like I would in a container and it should work.
00:27:27 [W] So that's really why we just focused on that and then once Argo and flux probably mature a lot more in flux V2 looks very very exciting to me and a number of the components with Argo as well as
00:27:42 [W] That we've been po seeing will probably be the longer-term vision for this the type of work we do.
00:27:48 [W] All right.
00:27:49 [W] Let's talk about some cluster stuff.
00:27:52 [W] Alright, so what cluster division do you guys have right now?
00:27:55 [W] I have non prod QA Dev stage performance and prod.
00:28:01 [W] Yeah, we do a similar breakdown. We have what we call lower environments and upper environments most of our clusters, you know, we try to we try to designate specific clusters per region /
00:28:16 [W] As we call it and so what we built is everything. Our team doesn't really care about the environment for our developers.
00:28:19 [W] We focus on what tear they are to us because anything that we need to essentially provide an SLO and an SLA for our services that are you know, we consider we did the reverse of a lot of people tier 3
00:28:35 [W] He cares about and it that's it.
00:28:34 [W] No one else really cares tier two is everything internal, but we will generally not get paid for it during the day or excuse me overnight, you know while we, you know, outside business hours and then tear one which is most of our Production Services,
00:28:50 [W] those are those are our pride and our stage services that you know, that that's really our division from our team's perspective since we mostly focus on the cluster pieces of it, but in general a lot of our lower environments in terms of kubernative
00:29:02 [W] Share the same cluster perp is the one exception there where when we do performance testing.
00:29:06 [W] We try to move that two separate clusters or make sure that you know the workloads don't conflict with other tooling.
00:29:14 [W] All right.
00:29:15 [W] let's see.
00:29:17 [W] All right, so I'm just going to go down the list now since we have a couple more minutes was Cube ATM and on a an option to build your cluster before deciding to go with cops.
00:29:27 [W] question. I am a cop.
00:29:30 [W] It's maintainer. So that was the obvious choice.
00:29:33 [W] However, our team did look at a lot of the other options out there, you know, specifically the you know, the elephant in the room of e KS and a lot of great tooling there. We work very closely with a bunch of the team at Amazon
00:29:48 [W] Wing and considering some of the things we really wanted.
00:29:51 [W] We just decided to go with cops.
00:29:53 [W] We did a lot of testing and to be honest you baby m is great and there but there's a lot of pieces that you have to plumb in there and we viewed cops was the end to end piece that you know, maybe someday we'll use a lot more of the
00:30:08 [W] Mints and you know, we can simplify the ecosystem a lot.
00:30:07 [W] But for now, that's what we picked and in the one thing.
00:30:11 [W] I want to point out. There is we really try to focus on these components we can take out and remove as we need to so it's really not that hard. We might have to reformat some of our configuration and and redo some templates but it really shouldn't be hard for us to swap
00:30:26 [W] One into the other and and we've done that a couple times now with you know, I mentioned there's a there's a question about Helm charts and using Umbrella charts.
00:30:34 [W] And when we moved away from an umbrella chart approach and went towards using Helm files, we were able to swap out the components pretty easily that work pretty well for us.
00:30:45 [W] us. So we, you know, try to stay Nimble with something like cops and change it, you know over time if that makes sense. All right, let's
00:30:54 [W] It's see where does proud run is in a kubernative cluster?
00:30:59 [W] So then how do you manage that cluster?
00:31:01 [W] Okay, great question.
00:31:03 [W] It does run in a kubernative cluster.
00:31:06 [W] We have a specific build cluster that has you know, some elevated Privileges and things like that.
00:31:10 [W] And and and yeah, so so that proud can also manage that cluster.
00:31:17 [W] don't do a lot of automation for that cluster specifically because of the, you know, the risk associated with doing lots of upgrades and
00:31:24 [W] Changes, we basically apply the changes and then we want to roll over nodes and things like that.
00:31:29 [W] We do that more manually and then a planned approach our medium.
00:31:33 [W] It's kind of a hard problem to solve in terms of you know, how we manage that cluster.
00:31:40 [W] Some elevated Privileges and things like that.
00:31:40 [W] And and and yeah, so so that proud can also manage that cluster.
00:31:47 [W] We don't do a lot of automation for that cluster specifically because of the, you know, the risk associated with doing lots of upgrades and changes.
00:31:54 [W] We basically apply the changes and then we want to roll over nodes and things like that.
00:31:59 [W] We do that more manually and then a planned approach our medium.
00:32:03 [W] It's kind of a hard problem to solve in terms of you know,
00:32:08 [W] Oh how we manage that cluster.
00:32:10 [W] We have a couple ideas one is prowl has the idea of running its job separate from its work claim control playing excuse me. So in the future, we could have the control plane spin up jobs in a separate cluster that then go and upgrade the
00:32:56 [W] An upgrade the the first cluster the build cluster.
00:33:00 [W] There's also we put off building that because cops is considering adding components like that into cops itself.
00:33:08 [W] that could make it easy. But also there's a lot of other tools out there that are kind of doing some of this and starting to solve the problem. So we like using other people's Solutions out there. So we don't need to, you know, come up with our own so I know like the AWS no termination.
00:33:24 [W] Is doing a lot of work on being able to read from an SQL SQL or things like that.
00:33:29 [W] will probably end up taking an approach like that at some point where you know will utilize some component to actually do the upgrades of a cluster will rely on our alerts and monitoring to let us know how that is going and keep an eye on things.
00:33:44 [W] How do you compare to get Hub actions?
00:33:47 [W] That's a great question.
00:33:49 [W] It's straightforward. Right?
00:33:51 [W] So GitHub actions are awesome, and they're you know evolving very quickly and I think that they have a perfect use case.
00:33:59 [W] In fact, if you look at the repo that I posted at the end of my talk github.com Mike's planes / prowl up makes it pretty easy to spin up a proud cluster where you can hook up components or you know hook up GitHub.
00:34:14 [W] Web hooks into you know think proud cluster. I even use a GitHub action there because they're great.
00:34:20 [W] It's such a great concept.
00:34:22 [W] There are a lot of areas where they don't make sense though still at this point to me, you know, I don't feel comfortable opening up running, you know, the sum of the components that they have hope, you know, they've set up that allow them
00:34:37 [W] Herbs inside your environment. So a lot of the things we do we try to be conscious of our networking and a lot of things like that.
00:34:44 [W] So for now, we do a lot of Builds on site or in you know in the cloud and and it just doesn't make sense for some of the types of workloads.
00:34:53 [W] We want to handle but in the future it might I mean frankly if you've looked at the way GitHub is evolved over the years.
00:34:59 [W] They've done a ton of work and a lot of the ideas.
00:35:03 [W] You know also came along in the kubernative a sword, you know, things like owners files.
00:35:14 [W] So again, like I said, it's something that will probably evolve but you know, our whole goal was to use this tooling to you know, push our infrastructure for words.
00:35:25 [W] Let's see.
00:35:26 [W] Are you leaning on Cops for your upgrades the shell script in your posts? And the job is a little opaque.
00:35:32 [W] Can you shed any light there?
00:35:33 [W] Yes, we do rely on our cops clusters are cops CLI to do the actual upgrades at this point.
00:35:45 [W] We basically pretest different scripts to do jobs like this.
00:35:50 [W] So we have a script that does the update process a different one that does upgrade and then a another one.
00:35:55 [W] Yes, we basically do a rolling update the same way we do in cops just you know with a long-running process.
00:35:51 [W] But like I said that earlier answering the previous question, that's not something we want to do long term. It's just, you know kind of the way we've built it at this point.
00:36:00 [W] Do you tend to build new clusters for major changes and expect engineer's to migrate or do you always upgrade in place? Great question My Philosophy here is
00:36:15 [W] Plan two clusters should be able to be destroyed at any time, you know, obviously, you know, not taking production or production workloads and whatnot.
00:36:22 [W] But I really really plan to never keep a cluster for too long and you know, so we do, you know rip them down rename them things like that occasionally and it's pretty important in my mind to
00:36:37 [W] So we constantly are spinning up new clusters, whether it's even for development or testing and then we do try to make sure we can always upgrade things.
00:36:42 [W] But if you've been around kubernative for a while, you know, eventually there's you know some bit rot and it's helpful to kind of move beyond that and you know start with a fresh, you know control plane and everything. So we do a bit of both
00:36:57 [W] It's helpful to kind of move beyond that and you know start with a fresh, you know control plane and everything.
00:37:02 [W] So we do a bit of both I think long term. We hope to abstract that away from engineer.
00:37:10 [W] So the teams don't actually need to worry about what regions they're deploying to maybe we'll just deploy to every region for them and then you know, if we want to take down a cluster, you know, we can do that, you know with with little notice or
00:37:25 [W] Ocean doing all the hard work.
00:37:26 [W] So so yeah for now we work pretty closely with the engineering teams on solving problems like that.
00:37:37 [W] I think there might be a couple more questions.
00:37:40 [W] Oh, wow.
00:37:41 [W] How do you compare? Tekton?
00:37:44 [W] Yes, tekton pipelines and and triggers that's actually the other project that we've been looking at. I apologize.
00:37:50 [W] I didn't see the second page of these. Let's see. I think we probably need to wrap up in one moment. However, I think what we can do is take these questions and move them to the
00:38:06 [W] the the chat room and the cube Khan - operators to - que con - operators.
00:38:16 [W] I will continue answering these there.
00:38:17 [W] So feel free to post more questions and I'll be around for a bit.
00:38:21 [W] Thank you so much.
