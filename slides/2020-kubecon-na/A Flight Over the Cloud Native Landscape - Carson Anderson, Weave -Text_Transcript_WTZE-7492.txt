A Flight Over the Cloud Native Landscape: WTZE-7492 - events@cncf.io - Tuesday, November 17, 2020 1:02 PM - 45 minutes

Participant: wordly [W] English (US)
Participant: wordly [W0] English (US)
Participant: wordly [W1] English (US)

Transcription for wordly [W]

00:00:00 [W1] Hello and welcome to a flight over the cloud native landscape.
00:00:03 [W1] My name is Carson Anderson.
00:00:04 [W1] I work for we've not that we've not the one you're thinking, but we'll get there in a second.
00:00:08 [W1] You can find me on Twitter at Carson underscore Ops and on GitHub at carcinoid.
00:00:13 [W1] Both of those platforms if you want to see what I'm doing. In fact every presentation I've ever made all the artwork for this presentation of my others is always open source on GitHub.
00:00:21 [W1] So follow me there now before I go any further, I want to see what I mean when I said not that we've the wave that I work for is not the cloud knative vendor, you might be thinking about my weave is an end-user company and if you're curious about what we do, you can find us on get we've got cam I'm
00:00:36 [W1] Not the cloud knative vendor, you might be thinking about my weave is an end-user company. And if you're curious about what we do, you can find us on get we've.com.
00:00:30 [W1] I'm saying this because I want you to know that I'm going to teach you about a lot of different projects and I want you know, we have no stake.
00:00:35 [W1] I have no personal professional stake in any of these projects. So you're not getting any sport of specific vendor pitch here.
00:00:42 [W1] Now, what I want to do in this presentation is cover the 12 graduated and 21 incubating projects in the cncf. That's 33 projects total and I have less than 35 minutes.
00:00:52 [W1] It's to teach them all to you.
00:00:54 [W1] Now.
00:00:55 [W1] I'm not going to obviously in that amount of time be able to teach you everything about a project or cover all the features and nuances of Any Given project. But I want you to leave this presentation having a basic understanding of what these projects do and how they relate to each other and know which ones you're curious about might want to learn more about or might want to use your self.
00:01:11 [W1] So before we go further, let's say a big thanks to fifty dot IO for these characters.
00:01:16 [W1] I'm going to use them pretty liberally in the presentation because I find them to be a lot more interesting and fun than just having boxes called app one and up two.
00:01:24 [W1] So you're going to see these characters show up quite a bit specifically going to see fit p and when you see fit be it's I'm going to refer to a legacy application written in an unspecified language.
00:01:33 [W1] So could it be any language and we see Goldie it specifically a newer application written in go before we go into the projects themselves. Let's talk about
00:01:40 [W1] What it means to be graduated in the cncf, so if I've got a project and that project wants to be a graduated project. You have to pass a few things first.
00:01:48 [W1] You have to be receiving constant contribution from at least two different organizations into the project. That means that you know that if you're going to use a project that's graduated.
00:01:57 [W1] You're going to use a project that's graduated.
00:01:59 [W1] You're not dependent on one organization to maintain the project.
00:02:02 [W1] You also have to certify that you're passing the best practices for core infrastructure and fully open source software.
00:02:08 [W1] You have to pass a security audit publish some metadata around who governs the project who's in charge of the code and who's using the project once you do that you pass a supermajority vote from the cncf and you've become a graduated project.
00:02:23 [W1] Notice that never in that did I talk about something being quote unquote production-ready graduated status in the cncf.
00:02:31 [W1] You don't have to be graduated to be production-ready. There are plenty of incubating and even sandbox projects that might be production ready for you depending on what you're trying to get.
00:02:39 [W1] So don't be afraid to use the projects that are incubating or sandbox.
00:02:43 [W1] Although it is great to be a graduated project.
00:02:45 [W1] It's not necessarily just a measure of production Readiness, but rather a measure of openness and transparency.
00:02:52 [W1] Now let's go and dig into all these projects.
00:02:54 [W1] Could I have a lot to cover?
00:02:55 [W1] The first one I want to talk about is containerd e it's a Daemon like you might expect from the deep meaning.
00:03:00 [W1] it's a process that runs on your systems to help you manage containers. So you can use containerd e directly in your code to build container images and to manage containers, but most of us won't use containerd e directly most of us will use containerd e as part of something like Docker or
00:03:15 [W1] object so when most of us build a container image today, and we do that with Docker we're doing it with containerd e so, in fact, this project is so useful that is actually built into a lot of the public Cloud communities offerings and the k3s project which is in sandbox state that allows
00:03:25 [W1] all or mostly full-featured communities cluster all-in-one binary and that does that using containerd e so if you're curious about low-level container operations check out containerd e next is TUF TUF stands for the update framework and it's all about dealing with managing
00:03:33 [W1] Series of standards and tools that we can build into our code and use to deal with updates.
00:03:36 [W1] I'm being intentionally vague here because the tough standards don't prescribe to any one specific kind of update.
00:03:42 [W1] You might be they might be packaged updates or image updates. But anything that you might want to get regular updates from and verify the source of so that's tough.
00:03:49 [W1] I won't cover it much because most of us won't use tough.
00:03:51 [W1] We're going to use the reference implementation of tough or the main implementation of tough in notary now notary takes all the tough ideas, but builds us some actual tools that most of us can use rather than writing.
00:04:01 [W1] in your own code to deal with update management verification and most of us will probably use notary as part of image signing and verification to ensure that when we get an image, we get it from somebody that we trust and it hasn't been modified along the way so that when we pull images when
00:04:16 [W1] Structure we know that we can trust where it came from.
00:04:17 [W1] So that's a notary get tough and notary have this tight relationship where tough is the standards and notaries and implementation of it, but you should look into both those if you're curious about that.
00:04:27 [W1] Next is Harbor Harbor is a private image registry and it has all the features you might expect from a private image registry. You could upload oci compliant images and get things like validation through something like notary or even image inspection or
00:04:42 [W1] And along with a bunch of other features like you might expect from a private registry.
00:04:39 [W1] It also has a really cool feature in that it can be a pull-through cash. So you can run Harbor in your infrastructure hook it up to Public Image Registries and then have your clients point to Harbor and when they need an image if the image exists in a public registry Harbor will pull it through
00:04:54 [W1] Only allowing your clients to pull from Harbor instead of always going over the Internet. So if you need to reduce your overall image pull bandwidth, you might check out Harbor for that. Either way. If you're curious about having a private image registry check out Harbor.
00:05:01 [W1] Next is communities.
00:05:02 [W1] Of course, we can't talk loud knative that talking communities Kuma knative was the first cncf member project to reach the graduated status and it is at its core container orchestration engine.
00:05:12 [W1] So when I say that, I mean, we've got a suite of back-end machines in communities. We call these nodes and we want to run workloads on those notes and we can tell communities.
00:05:21 [W1] Hey, I want you to run a workloads.
00:05:22 [W1] Here's what it should look like and kubenetes will put that workloads somewhere. We can then of course scale-up run multiple copies of our workload and fact communities can handle tons and tons
00:05:31 [W1] Of different workloads with different configuration and because it's an orchestrator, it has things like automatic dealing with things like nodes going down.
00:05:39 [W1] So if a node goes down Community sees that and can redistribute the declared workloads.
00:05:59 [W1] And of course I have to bring in keptn Cube here because they're amazing. And why wouldn't you talk about Captain cube? When you get a chance communities is really kind of the heart of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of
00:06:14 [W1] Going down tufin node goes down pretty sees that and can redistribute the declared workloads.
00:06:43 [W1] Of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of integration points for other systems to build upon kubenetes to provide more value using kubenetes as a core.
00:06:54 [W1] So communities has the ability to add storage or networking layers.
00:06:57 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really communities is the heart of cloud native in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do
00:07:12 [W1] Speaking of communities Helm is a package manager for kubernative.
00:07:15 [W1] So Helm is something we do actually have to run in communities because it's specifically around creating and maintaining applications in communities. So we know that we can run our workloads and communities and we call those pods, but it turns out there's actually a lot of things in communities that we could create
00:07:31 [W1] It's a community has the ability to add storage or networking layers.
00:07:34 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really kubenetes is the heart of cloud knative in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do speaking of
00:07:57 [W1] figuration or services or different routing information to all help Telco knative what our application looks likes to describe our application to kubenetes helm allows us to take all that configuration put it into one thing called a Helm chart and that chart is sort of like a
00:08:12 [W1] What my application look like here's all the things you need to make and how they relate to each other.
00:08:15 [W1] We send that to tell them and helped create it in our communities cluster like we expect the great thing about Helm and Helm chart is it's this kind of recipe this redistributable thing. We can take our chart and not only use it internally, but give that chart to other users or distribute that chart out into
00:08:31 [W1] Other users install our applications into their communities clusters.
00:08:32 [W1] So you'll very often see a lot of the things. I'm going to talk about be installable into your clusters using something like helm.
00:08:40 [W1] Another way that you might manage your applications and kubenetes is using something like Argo Argo is also an application manager, but it takes a different tack than Helm Argo is a gitops based system for kubernative application management.
00:08:50 [W1] meaning that we set up one or more git repositories. And in those repositories, we describe what we want.
00:08:56 [W1] Our application will look like in communities.
00:08:57 [W1] We then hook Argo up between the repositories and our communities cluster or clusters and it takes the described application and makes it true and communities and it's doing gitops. So it's always watching the repository and as a repository.
00:09:09 [W1] Changes it sinks those changes into communities as they happen and it allows us to take all the gitops tools we know and love and use them to manage our communities applications.
00:09:18 [W1] So if you're interested in gitops and Kuma Nettie's check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:09:31 [W1] means they're not mutually exclusive you can use Argo and Helm together. It's absolutely fine.
00:09:36 [W1] One other way we might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operators. Now my 32nd what is an operator talk?
00:09:49 [W1] These applications. So if you're interested in gitops and kubenetes check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:10:01 [W1] That means they're not mutually exclusive you can use Argo and Helm together.
00:10:04 [W1] It's absolutely fine.
00:10:05 [W1] One other way.
00:10:07 [W1] We might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operator.
00:10:17 [W1] Has now my 32nd what is an operator talk?
00:10:19 [W1] So an operator can be thought of as an engine.
00:10:22 [W1] It's a process rerun inside our communities cluster that knows how to create applications for us and we create something communities called a custom resource and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the
00:10:59 [W1] and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the engine we've written called an operator knows how to take that resource and make the application in communities Force based on that resource and then if we make a
00:11:14 [W1] Figuration the operator can operate on that and make that copy of the application.
00:11:18 [W1] Really the focus.
00:11:19 [W1] here is the crd S rather than build a Helm chart or a git repository.
00:11:23 [W1] We actually put a resource into the kuma knative JPI describing our application and we use the operator framework or other tools to create an operator that knows how to take those custom resources and turn them into applications in our cluster.
00:11:35 [W1] So if you're curious about a more advanced way to manage your applications throughout their entire life cycle inside communities, check out the operator framework and just like before the
00:11:43 [W1] Operators and Helm and Argo those are not all mutually exclusive and a lot of them can work together very well.
00:11:50 [W1] Next is Contour Contour is sort of fills another Gap in the communities ecosystem.
00:11:54 [W1] So we talked about communities can run our workloads.
00:11:57 [W1] It can also run these things called Ingress and egress really is just a set of configuration.
00:12:02 [W1] It's a set of configurations like host and path based information says, hey if you're coming in for this host and this path go to this workloads.
00:12:19 [W1] Duration and route to the right place based on that config and that's what it called an is called an Ingress controller in kubenetes and there are a lot of English controllers.
00:12:27 [W1] You can run and Kuma Nettie's many of them are Legacy web servers kind of jammed into this Ingress controller role, but Contour is built from the ground up to be an Ingress controller for kubernative.
00:12:37 [W1] And to try to do the right things for you right from the start check out Contour.
00:12:42 [W1] Next is kubenetes Cube Edge is interesting because it's orchestration built on communities.
00:12:47 [W1] So we already know communities can do container orchestration, but kubenetes is a platform that leverages the kubenetes apis and extension points to allow us to do Edge compute management using the kuma Nettie's apis.
00:12:58 [W1] So if you're curious about doing Edge management and managing compute at the edge and you want to use the community's tools and API check out kubeedge.
00:13:07 [W1] Next is Rook Rook is also orchestration that runs in communities but instead of managing other devices or containers Rook is about managing storage inside kubernative.
00:13:15 [W1] So Rook runs and communities and allows you to deal with block storage or object storage and it can do things like provide persistent volumes for your workloads. So they can have a volume that follows them around in your cluster or do other things like said object storage and other kinds of
00:13:30 [W1] Would build on top of criminality is using Rook. So if you're curious about storage and Kuma Nettie's checkout Rook.
00:13:28 [W1] Next is cryo or cri-o cri-o stands for containerd runtime interface. It is kind of a layer that we described that we defined to help communities run containers + O stands for oci compliant.
00:13:40 [W1] So every single node in our communities cluster runs this thing called a qubit and it's the kubelet job to create and manage containers over their life cycle on the Node, but there is this kind of squishy blue layer.
00:13:51 [W1] I've drawn between the cubelet and actually doing things with containers and that is where cri-o the container runtime interface lives.
00:13:58 [W1] And it is definitely where cri-o lives because cryo is a container runtime interface built specifically for communities to be simple and fast and efficient.
00:14:09 [W1] efficient. So if you're curious about container runtime for communities built for kubenetes check out cryo next to cni-genie.
00:14:27 [W1] It needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:36 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating.
00:14:46 [W1] if you're interested in the low-level operations of networks, check out cni-genie.
00:14:49 [W1] The network sometimes that's on the same node, but very often it's across nodes and something needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:50 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating. So if you're interested in the low-level operations of networks, check out cni-genie,
00:15:05 [W1] Next is grpc.
00:15:07 [W1] So we've got our applications and they need to talk to each other one way.
00:15:10 [W1] They might do that is over something like HTTP.
00:15:13 [W1] It's been around for a long time.
00:15:14 [W1] It's kind of a low level interrupt but HTTP, although it's great has its problems primarily.
00:15:19 [W1] It's got a lot of overhead because it's connection listen stateless.
00:15:23 [W1] There's a lot of overhead and every single HTTP request to describe what's going on grpc exists as an alternative or can run alongside HTTP as another way for applications to communicate with each other over a network and this is stateful and hashicorp.
00:15:37 [W1] Less overhead so it can be a lot lot faster than HTTP grpc also has cool things like bi-directional streaming where applications can stream over a single connection both ways.
00:15:46 [W1] If you leverage things like Proto, you can also get things like type safety using grpc. So if you're looking to do application to application communication over the network and want to go above and beyond what you get from HTTP, check out grpc
00:16:01 [W1] Next is called DNS core DNS is like you might expect a DNS server built for the cloud.
00:16:05 [W1] So we if we are honest with each other DNS is the oldest form of what we call service Discovery, right?
00:16:11 [W1] We ask for something by name DNS gives us back where that lives and even though we've got all of these cool new ways to do service Discovery in the cloud.
00:16:18 [W1] We still tend to tend to use DNS a lot and Cordia necessary about brand new DNS server that is built for the cloud and this picture that I'm showing you seems empty. It seems like there's a lot missing and that's because Cortana is really exist at the coredns.
00:16:31 [W1] seems like there's a lot missing and that's because Cortana is really exist at the core of this big ecosystem of plug-ins for DNS has plug-ins for multiple ways to serve DNS traffic whether that's the traditional UDP or new protocols like HTTP to or grpc it
00:16:46 [W1] Bring in configuration and receive both initial configuration and constant active reconfiguration from multiple sources, including things like Kuma Nettie's at CD which we'll talk about or even public clouds record.
00:16:59 [W1] DNS can privately serve the records that you define in your public Cloud DNS systems. It also has plugins to help you do things like rewrites and tracing and metrics on your DNS and really brings DNS into the modern age.
00:17:11 [W1] In fact, all these features make or DNS the recommended and go to DNS Lucien for doing
00:17:15 [W1] DNS inside of communities and it has been that way for quite a while now. So if you're curious about a modern-day nsmcon mentation checkout coordinates
00:17:24 [W1] Now before I talk about the next two projects, I want to briefly describe.
00:17:27 [W1] What a servicemeshcon.
00:17:53 [W1] Not just not be possible to change the code of a specific application as servicemeshcon.
00:17:59 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application, although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency
00:18:14 [W1] As servicemeshcon as well.
00:18:15 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application. Although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency encryption that kind of
00:18:36 [W1] Thing.
00:18:37 [W1] Well, once we've got all these proxies distributed and running in our ecosystem will want to control plane that can manage these distributed proxies and give us a way to view what's happening with them and control them you combine a proxy in a control plane and you get a servicemeshcon it powerful features like metrics
00:18:52 [W1] You combine a proxy in a control plane? And you get a servicemeshcon powerful features like metrics load balancing encryption and transparency and tracing all from the proxy without ever having to change your service code. And this is very very powerful and there are two projects.
00:19:07 [W1] I want to talk about that are part of a servicemeshcon.
00:19:22 [W1] From the ground up for linkerd E to do this servicemeshcon Clemente these servicemeshcon ideas.
00:19:27 [W1] It also comes with the linkerd E to control plane that allows you to manage all the proxies.
00:19:31 [W1] So it really is a full complete and and servicemeshcon Lucien.
00:19:35 [W1] It's also very easy to get up and going and to use linkerd e, so if you're looking to implement a servicemeshcon you want to get up and going and want a complete solution.
00:19:42 [W1] You can check out linkerd e another alternative for a servicemeshcon voi now Envoy is a bit different in that Envoy just focuses on being the proxy process for the servicemeshcon.
00:19:52 [W1] If you're asking where is the control plane Envoy doesn't provide when it doesn't prescribe one it leaves that open to the implementer and this seems like a downside that Blank Spot may seem initially compared to linkerd e like it's a problem, but it's actually a great power the fact that the envoy haproxy folks
00:20:08 [W1] Hi early on being a servicemeshcon c or just a service proxy means that it can really focus on that and provide the best possible proxy that you could need and actually Envoy is the backing proxy between a lot of other Cloud native projects.
00:20:21 [W1] So if you're curious about just running a service proxy check out Envoy.
00:20:26 [W1] Next is opentracing like you might guess it's all about dealing with traces. So we know that we can use something like a servicemeshcon automatic tracing between app user requests and the applications it bounces around.
00:20:36 [W1] But if we want to know what that user request does in each application visits as it kind of goes from method to method and spends a different amount of time in each application doing different things.
00:20:46 [W1] We need to instrument or application.
00:20:48 [W1] We need to write code to create these tracing these traces and that's what opentracing exist to do it like you might have guessed from the name opentracing is
00:20:56 [W1] Provider agnostic. So the great thing about it is you can instrument your applications.
00:21:00 [W1] You can write coding replications to generate Trace data and opentracing works with a multitude of providers. Meaning that it doesn't care who you use you instrument once and never have to do it again one place. You might export this Trace data from opentracing or elsewhere is to Jaeger.
00:21:14 [W1] Jaeger is a trays aggregation and Trace management platform.
00:21:17 [W1] So we've got our Trace data that we've got from our servicemeshcon something from something like opentracing we need to take that data and send it somewhere so that we can aggregate in-toto.
00:21:26 [W1] The core of tracing in your Cloud native system. So that's Jaeger. If you're curious about somewhere to send your traces and view your traces in the cloud. Check out Jaeger.
00:21:31 [W1] Next is Prometheus.
00:21:33 [W1] Prometheus is all about dealing with metrics in the cloud.
00:21:35 [W1] Now.
00:21:35 [W1] We know that we were on applications all over the place, especially in the cloud and those applications are generating data, right they want to generate data around how many requests they're making how many requests are succeeding and failing that kind of stuff what we used to do when we want. It application metrics was instrument our
00:21:50 [W1] Prometheus is all about dealing with metrics in the cloud. Now.
00:21:40 [W1] We know that we run applications all over the place, especially in the cloud and those applications are generating data, right then we want to generate data around how many requests are making how many requests are succeeding and failing that kind of stuff what we used to do when we wanted application metrics was instrument our
00:22:26 [W1] They have the applications take those metrics and Export them to a specific provider. And if we ever wanted to switch metrics providers we couldn't do it because the applications would have to be retooled.
00:22:35 [W1] Prometheus turns out on its head Kuma T says well part of the Prometheus spec is saying here's a standard way that you're all going to serve up metrics.
00:22:43 [W1] You're going to expose a web page and give me your metrics and Prometheus is going to go to each application individually and pull that metric data down and aggregate it and it knows where your application is live because remember we're in the cloud things are coming and going all the time.
00:22:56 [W1] It knows where your applications are and how to find them by having Cloud integration or Kuma Nettie's Integrations.
00:23:01 [W1] So that as your applications come up and down and move Prometheus always knows where
00:23:05 [W1] Go to get that metric data and it kind of flips the whole idea of metrics on its head.
00:23:10 [W1] Once Prometheus has gone to scrape that metric data and pulled it in.
00:23:13 [W1] It pulls it into its own internal time series data base and can give you features like charting and alerts and Metric searching and other API Integrations that really sort of like Jaeger was with traces.
00:23:23 [W1] Let's Prometheus be the core of metrics in the cloud.
00:23:26 [W1] So if you're interested in an open-source metric system, check out Prometheus now if Thanos exists alongside and with Prometheus to solve some specific problems. It is very easy to get up and going with a single.
00:23:40 [W1] Instance in the cloud no big deal you can get up and going very quickly. But if you're going to run multiple Prometheus instances and if they're going to be distributed across geographic regions, or you want fault tolerance, the Prometheus project isn't really focused on solving that right now, but the Thanos
00:23:55 [W1] Thanos as a wrapper around one or more Prometheus instances that allows you to aggregate data and go to Thanos and run a metrics query and have it go to all your Prometheus instances and run that query for you.
00:24:05 [W1] Then I'll also has the ability to take that data and Export it into multiple cloud storage mechanisms so that you can have long-term Prometheus storage because Prometheus doesn't tend to want to keep data for very long.
00:24:17 [W1] So if you're curious about distributed Prometheus long term metrics from Prometheus check out Thanos in that same vein.
00:24:23 [W1] There's the cortex project now the cortex project also exists to solve the multiple Prometheus problem, but it works a bit differently.
00:24:29 [W1] It's designed to always put all of the data rather than wrapping. It just ingests all the data from all your Prometheus instances and stores it in its own internal architecture that way when you query your metrics, you don't even have to go to your Prometheus instances.
00:24:42 [W1] They just act as a data source and really cortex is at the heart of your metrics at that point. So
00:24:47 [W1] If you're interested in long-term aggregated Prometheus metrics, you can check out cortex.
00:24:51 [W1] Now. I know those two projects than us and cortex seem very similar and that's because they are you'll have to do your own research to find out which of these two projects might be the right one for you.
00:24:59 [W1] Next is fluent D. Now fluent D is all about dealing with streaming text processing and very often log processing in the cloud and elsewhere.
00:25:08 [W1] So at its core affluent D can be set up to taken multiple streams of text read those dreams as they come in process them internally and spit them out into other places.
00:25:17 [W1] Data sources and where you might want to put that data all this makes fluently.
00:25:14 [W1] I really great place to handle logs from communities.
00:25:17 [W1] So we've got communities.
00:25:18 [W1] We've got our workloads running and those workloads are all generating log data, which is Text data and a generating that all the time and coming and going and very often fluent D is the engine behind most of the Prometheus or sorry most of the communities installations that you run where
00:25:33 [W1] With reading those logs as they're generated reformatting them and sending them to Some Cloud integration. So that even though you've got a containers running across many many back and machines.
00:25:42 [W1] You can view all their logs in one place.
00:25:45 [W1] Thanks to the aggregation and Export and manipulation provided by something like fluent D. So whether you're using fluent D and don't know it because it's in communities or you're interested in doing log processing directly with fluentd d you should check it out.
00:25:58 [W1] Next is vitess vitess is all about dealing with relational databases in the cloud.
00:26:02 [W1] So it's easy easy easy to run a relational database in the cloud or increment Eddie's but the problem with these databases that they tend to have to scale vertically and that is really vulnerable and brittle in the cloudbees cuz we don't want things to scale vertically. We want to scale horizontally want to be fault tolerant and distributed
00:26:17 [W1] Want to split that big horizontally or vertically skill database into multiple smaller databases and vitess exist to help us do that vitess is a layer that runs on top of the MySQL or mariadb engine that you already know and trust but allows for powerful features like
00:26:32 [W1] Hurting and you can actually increase replicas and Richard and do all sorts of database manipulation using vitess.
00:26:30 [W1] Well while just using the standard MySQL engine vitess also has a proxy process that you can run that allows you to take in SQL or grpc traffic and distribute it to these kind of more Dynamic sharded replicated changing database instances.
00:26:45 [W1] all this allows vitess to be a really great solution for running relational databases in communities allows you to scale and distribute and be fault tolerant all while still using the kind of database interface that you know and trust
00:26:55 [W1] so that's vitess next is titanium or tikv.
00:26:59 [W1] Tikv is a key value store. So it's all about dealing with key values.
00:27:02 [W1] So it does the things you might expect from a key Value Store does adds it does updates it does deletes but the cool thing about tikv is that it scales horizontally, like vitess escapes horizontally very very well. In fact according to vitess page.
00:27:15 [W1] They say that it scales up to petabyte-scale with a key value data.
00:27:17 [W1] that's huge scale. Another really great thing about tikv is that it supports distributed acid compliant transactions, so you into a
00:27:25 [W1] Tikv insulation you can say I want you to update this key.
00:27:28 [W1] delete these three keys change those two keys and I want you to do all of those operations at once or not at all that ensures that if you're doing multiple key operations in your key Value Store, you don't have to get stuck in a halfway state where one transact one operation worked and the other failed and you
00:27:43 [W1] So if you're interested in really high scale key value store or transactional key Value Store checkout tikv.
00:27:47 [W1] SED is also a key value store.
00:27:51 [W1] That's Cloud native.
00:27:51 [W1] So it does all the same things that I've just described at updates deletes but ci/cd rather than focusing on sheer scale has focused on Simplicity.
00:27:59 [W1] So it's very very easy to get up and going with an STD installation or an STD cluster.
00:28:04 [W1] It's often just a few commands or a single file away from having a fully functional at CD cluster. So it has all the things you might expect.
00:28:12 [W1] Sect leader election fault tolerance distributed load, but it's much much simpler to run than some of the other offerings.
00:28:19 [W1] In fact the kind of combination of features and simplicity have made SED the go to back end for the kubernative API for a long time for a while.
00:28:26 [W1] It was the only back end.
00:28:28 [W1] There are a few others now, but odds are really good that if you're using Kuma Nettie's you're probably using a CD behind the scenes to store all the data you're sending into the kubernative API, but you can absolutely use SED directly for yourself as a key value.
00:28:42 [W1] Store next is dragonfly dragonfly is all about peer-to-peer file transmission.
00:28:47 [W1] So we've got people peers they can send files to each other dragonfly is agnostic about the file content, but it does have some first-class Integrations for images.
00:28:55 [W1] So a dragonfly has knative Integrations to deal with image transmission peer-to-peer and that's not very interesting but what's cool about dragonfly is its distributed peer-to-peer transmission. So you set up dragonfly nodes throughout your system and when anybody wants download a specific image rather
00:29:10 [W1] To a single place to download the whole image. They can download parts of that image or parts of any file from the peers that have chunks of that file rather than always having to go out.
00:29:19 [W1] So if you're curious about a better way to do peer-to-peer file transmission or a better way to do image transmission check out dragonfly.
00:29:27 [W1] Next is cloudevents cloudevents. Like you might guess is all about dealing with event infrastructure in the cloud.
00:29:32 [W1] So we've got our applications and they have the ability. Of course to weave say we decide that we want to do event based infrastructure.
00:29:38 [W1] One thing we might not agree on is the exact format of our events.
00:29:42 [W1] We may want to use different structures different terms and it makes it really hard to say.
00:29:46 [W1] Well we all want to do events but we can't agree on what the event should look like cloudevents exist to be a series of standards and sdks for us to work with event based infrastructure and all agree that we're going to use cloudevents.
00:29:57 [W1] So that we can very easily and efficiently interop with each other because we all use the same back end event structure.
00:30:02 [W1] So if you're curious about event based infrastructure, check out cloudevents in that same vein, let's talk about Nats Nats is at its heart really a message bus. So you have producers and consumers where you can put messages into Nats and get them from other processes and Nats of course can run distributed
00:30:17 [W1] With lots and lots of producers and consumers.
00:30:02 [W1] It supports a lot of different event bus systems.
00:30:05 [W1] So you can do things like pub/sub where you publish a message and multiple subscribers get that message.
00:30:11 [W1] You can do request reply where you were send specifically to someone and get a specific answer back or you can do topic-based or streaming event processing all using Nats Nats also scales dynamically and very efficiently.
00:30:24 [W1] So not only is it fast and flexible and really efficient but it's scales really well, which makes it a great solution for doing event based.
00:30:30 [W1] Lecture in kubernative. So that's Nats. If you're curious about building an event based system.
00:30:34 [W1] Check it out.
00:30:35 [W1] Next is spiffe e or the secure production identity framework for everyone and it's all about like you might guess identity spiffe e is a set of standards and tools for dealing with identity in the cloud now when I say identity, I don't just mean users.
00:30:48 [W1] I mean identity at the node level identity at the workloads level or identity at processes inside the workloads spiffe. He's really about saying well, let's take identity and go as deep as we need to and be more Dynamic and more fluid and more graphql.
00:31:00 [W1] annular if we need to to help us deal with identity in the cloud spiffe e is another case where you probably won't use it directly you probably use the Spire project which takes all the implementations and standards of spiffe e and build some tools the Spire server and the Spire agent where
00:31:15 [W1] I meant something to spiffe e Concepts and get this kind of identity stuff.
00:31:18 [W1] I've been talking about without having to write your own code.
00:31:20 [W1] So if you're interested in identity and you want to write your own code check out spiffe e or if you want to just leverage the spiffe E Concepts check out spire.
00:31:29 [W1] Next is open policy agent.
00:31:30 [W1] It's all about dealing with policy enforcement.
00:31:32 [W1] So we feed open policy agent policy documents saying here's what we do and do not want to allow into a given system.
00:31:38 [W1] And then as we feed objects into that system Opa either accept them or reject them based on the policy.
00:31:43 [W1] It's been given I'm being intentionally vague because open Opa doesn't prescribe a specific thing that it's enforcer of opa has been used to enforce policy for a ton of things one thing. Okay fits really well on to though is Kuma Nettie's Opa can run a top.
00:31:58 [W1] And in front of your communities API so that you can kind of control what you allow into your kubenetes cluster and what you don't based on the policy that you give Opa.
00:32:07 [W1] So if you're interested in policy definition or policy enforcement check out Opa last but not least is Falco Falco is all about container runtime security.
00:32:17 [W1] So we've got our images and we can use things like notary to validate that we're running images we trust but we might still want to watch these images or these containers are these processes the entire time they're running so fast.
00:32:28 [W1] Echo does that Falco is this to run in our infrastructure and watch our processes all the time and it has a set of internal rules that says what it expects them to do and what expect them not to do and if any of our processes does something we don't expect like accessing a database. We didn't expect it to reach
00:32:43 [W1] At and send an alert when it happened so you get always on active security for your workloads.
00:32:41 [W1] So if you're curious about that, check out Falco.
00:32:44 [W1] So that's it.
00:32:46 [W1] I have covered all of the projects in this amount of time. Hopefully you kind of what kind of whet your appetite for a lot of these given you a basic idea of what each of these projects does and how they fit together.
00:32:54 [W1] So you can kind of go forth and learn more about them as you see fit one last thanks to fifty dot IO for these great characters.
00:33:01 [W1] I absolutely love them.
00:33:02 [W1] I use inkscape to create my presentations.
00:33:05 [W1] So Z to animate the presentations and open clipart to get art when I can't draw things myself, and that's it. Again. My name is Carson Anderson.
00:33:14 [W1] Work for we've not that we've you can find out what we do at get we've got cam. You can follow me at Twitter at Carson underscore Ops and on GitHub at carcinoid. Thank you so much for all your time, and I hope to see you again soon.
00:33:30 [W1] All right, so we have time for a bunch of questions.
00:33:34 [W1] Hopefully everybody enjoyed that. I had a blast making it don't mind the extra wide V on the camera here.
00:33:40 [W1] We're all working from home.
00:33:41 [W1] So I'll do what I can to make a little prettier for you guys.
00:33:45 [W1] Let's go ahead and I've been answering a lot of questions as we go.
00:33:49 [W1] I'm going to just kind of buzzed through the last bit that we have on The View here.
00:33:54 [W1] Let's see.
00:33:57 [W1] Be scanning. I did mention that as part of the harbor project.
00:33:59 [W1] They can integrate with some vulnerability scanning tools.
00:34:02 [W1] So my recommendation for dealing with secure images is to use something like that.
00:34:08 [W1] And also I highly highly recommend everybody when they deploy these kind of images pin to a version and pin do a hash if you can that's really my best advice for dealing with secure images.
00:34:21 [W1] Next is cloudevents and a question about does cloudevents work with Kafka and does it have some sort of knative integration as far as I'm aware cloudevents will work with any messaging system.
00:34:31 [W1] It's really a standard around the structure and the format of the messages rather than dealing with exactly how those messages are transmitted between systems.
00:34:39 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:34:45 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or
00:34:51 [W1] Nats no problem
00:34:55 [W1] question about would fluent DB ideal for scraping DOT log files and shipped to Greylock.
00:35:00 [W1] Absolutely.
00:35:01 [W1] That's I mean fluentd really at its core is a text processing system.
00:35:05 [W1] But if you go to their site, they really push and they really show that it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off to external systems things like great log, so I would recommend fluentd for that
00:35:20 [W1] Then dealing with exactly how those messages are transmitted between systems.
00:35:16 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:35:22 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or Nats. No problem.
00:35:32 [W1] Question about would fluent DB ideal for scraping DOT log files and ship to Greylock.
00:35:38 [W1] Absolutely.
00:35:38 [W1] That's I mean fluentd really at its core is a text processing system. But if you go to their site, they really push and they really show that it's it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off
00:36:47 [W1] Is there a Secret store / Vault? I'm not quite sure the context are so kubernative.
00:36:52 [W1] I'll just kind of answer. We had another question about Secrets earlier Lancer kind of my given stock answer for secret. So kubenetes has built-in support for secrets and those have been encrypted for quite a while and a lot of in a lot of the most recent communities versions.
00:37:08 [W1] I also really highly recommend people when they're using things like Argo or other stateful deployments of or gitops for their community's resources Secrets kind of become a problem for a lot of people.
00:37:18 [W1] I really recommend looking into things like the bank vaults operator or there are operators built for most of the cloud secret management solution.
00:37:29 [W1] So if you're running in a public Cloud, there's usually a secret management solution provided for you Amazon has a secret manager gcp as a Secrets manager.
00:37:37 [W1] And most of them have some sort of operator where you can put your secret securely into that system and then set it up so you can create a Kuma knative resource that references those secrets and pulls them down as needed.
00:37:49 [W1] That's a really complicated.
00:37:51 [W1] I actually have an entire other talk about that that people can hit me up for about how we built a secret management system built on the cloud for kubernative. We've but in general I really recommend those kinds of operator based secret Solutions
00:38:06 [W1] Thanks for everybody.
00:38:07 [W1] There's a lot of really great feedback here that just you guys loved it.
00:38:10 [W1] So I'm really really happy to hear that.
00:38:11 [W1] Let me see if I can get down to some more questions.
00:38:16 [W1] Kafka versus Nats Odette, this is a Bugaboo for me.
00:38:19 [W1] I really really am constantly surprised at the usability and scalability of nats.
00:38:27 [W1] If you're looking in this, you know where cncf so spoiler alert is I'm going to kind of lean towards cncf projects here, but I really recommend looking into Nats before Kafka from most people most of the time.
00:38:35 [W1] It's more modern.
00:38:37 [W1] It's faster. It's easier to deploy like it may not scale to the ultra giant scale that Kafka purports to go to but most of us don't need that like Mega scale.
00:38:47 [W1] So I really really recommend people look into a Nats you'd be surprised how easy it is to get up and get going with Nats and and
00:38:54 [W1] get value out of it very quickly.
00:38:58 [W1] Yeah, you can't see the sweatpants. Although you also can't see the dog. So that's unfortunate.
00:39:04 [W1] Let's see.
00:39:04 [W1] Yep going through bunch of my talks. So
00:39:12 [W1] I selected this list.
00:39:14 [W1] Literally my proposal for the presentation was I want to cover all of the cncf graduated incubating projects and and introduce everybody to them so that they could go forth in the conference and learn more and so that was my proposal to the the conference was I'm going to do this
00:39:29 [W1] Between my proposal and when I had to actually make the presentation 12 or projects or added so I made a proposal and I end up sticking with it, but it was literally just wanted everybody to have heard of all these projects these top two tiers of the foundation and I
00:39:44 [W1] two weeks creating the presentation demoing different tools getting to learn all of them ins and outs I could so I could teach you guys, you know, the right things sto 1:7 and linkerd E2 which one is better and why that is a whole
00:39:50 [W1] Sto 1:7 and linkerd E2 which one is better and why that is a whole series of talks and probably a whole track on its own. I would say personal experience linkerd e is exceptionally easy to get up and going quickly.
00:40:00 [W1] And so it's a great way to get into a servicemeshcon cept without a lot of extra infrastructure.
00:40:06 [W1] I won't say better or worse because sto and Envoy have a really really broad feature set and are really successful for a lot of companies. So short version check out linkerd.
00:40:17 [W1] D2 trying to get up quickly and then maybe you'll move on DSD. Oh, maybe you won't depending on what you actually want out of your servicemeshcon.
00:40:47 [W1] Somebody asked where can we get the presentation? If you want the source code for this presentation all of the artwork and and everything. I used to make the talk. You can find that on my GitHub at carcinoid.
00:40:58 [W1] So if you just go to GitHub and search for carcinoid, they'll be a Toc pinned at the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:10 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:41:16 [W1] Repeating his questions.
00:41:17 [W1] So somebody asked where can I get a recording of the talk?
00:41:19 [W1] You'll be able to find that on this conference site after the day is done.
00:41:25 [W1] I think every day at the end of the day they're going to release the recordings.
00:41:28 [W1] My presentation itself is on my GitHub.
00:41:33 [W1] So somebody asked where can we get the presentation if you want the source code for this presentation all of the artwork and and everything I use to make the talk you can find that on my GitHub at carcinoid. So if you just go to GitHub and search for carcinoid
00:41:46 [W1] said they'll be a talk pin to the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:56 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:42:05 [W1] So we also asked about my session on operators. I would I did have a talk last year about operators at keep con. If you want to find out more about that.
00:42:12 [W1] You can go find it on my GitHub as well.
00:42:14 [W1] So I'll try to put these actually now that I think about I'll put these in my speaker bio once this is all done.
00:42:19 [W1] I'll go back and put some of my past years talks and that kind of stuff in my speaker bio and links to those. So also check later today and you should have links to most of those things in my bio.
00:42:31 [W1] Another question again downloading presentation will be on this site at the end of the day.
00:42:36 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:42:41 [W1] What would I use for postgres?
00:42:42 [W1] This question actually came up twice.
00:42:43 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:42:49 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:42:55 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:43:07 [W1] A better way to handle postgres inside of kubernative.
00:43:10 [W1] So the I would definitely refer here anybody to postgres operator, even though it's not quite vitess, but for postgres
00:43:20 [W1] What is the go to Kate's UI?
00:43:22 [W1] That is a really good question.
00:43:24 [W1] I think it depends on your use case.
00:43:29 [W1] Another question I can downloading presentation will be on this site at the end of the day.
00:43:34 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:43:38 [W1] What would I use for postgres? This question actually came up twice.
00:43:41 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:43:47 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:43:53 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:44:04 [W1] A better way to handle postgres inside of kubernative.
00:44:08 [W1] So the I would definitely refer anybody to postgres operator, even though it's not quite vitess, but for postgres
00:44:18 [W1] What is the go to Kate's UI?
00:44:20 [W1] That is a really good question.
00:44:22 [W1] I think it depends on your use case.
00:44:24 [W1] I've said it before I'm a contributor and a minor contributor and a big fan of Argo CD. So I actually really like to use Argo and and have people use Argo as a minimalist kind of Katie why it's not a full do everything you
00:44:39 [W1] Like the direct Cloud offerings for you eyes are also pretty useful.
00:44:44 [W1] Looks like they're making me close up on the session.
00:44:46 [W1] So I think what they want me to do is continue answering questions.
00:44:51 [W1] On slack. So please any of these questions I didn't get to if you have more questions for me.
00:44:55 [W1] I'm on the cloud knative computing slack for Kube Khan. So go ahead and hit me up directly and PM me and I'll be happy to answer any questions that you have Beyond this.
00:45:05 [W1] So I think that's it. I don't know when they're going to cut me off or if they want me to just stop talking but
00:45:14 [W1] Wait to see an answer.
00:45:24 [W1] Presentations I'll just keep talking till they kick me off.
00:45:26 [W1] So the question there's another question here is dragonfly like a torrent protocol sort of it's sort of like it's peer-to-peer transmission. So a lot like torque protocol, but like I said, it's primarily focused on dealing
00:45:41 [W1] The meaning of Creations that has is dealing with downloading images from other nodes.
00:45:44 [W1] So, okay.
00:45:47 [W1] Well, that's it.
00:45:48 [W1] Like I said, please hit me up if you have any more questions and thank you so much for watching.

Transcription for wordly [W0]

00:00:00 [W1] Hello and welcome to a flight over the cloud native landscape.
00:00:03 [W1] My name is Carson Anderson.
00:00:04 [W1] I work for we've not that we've not the one you're thinking, but we'll get there in a second.
00:00:08 [W1] You can find me on Twitter at Carson underscore Ops and on GitHub at carcinoid.
00:00:13 [W1] Both of those platforms if you want to see what I'm doing. In fact every presentation I've ever made all the artwork for this presentation of my others is always open source on GitHub.
00:00:21 [W1] So follow me there now before I go any further, I want to see what I mean when I said not that we've the wave that I work for is not the cloud knative vendor, you might be thinking about my weave is an end-user company and if you're curious about what we do, you can find us on get we've got cam I'm
00:00:36 [W1] Not the cloud knative vendor, you might be thinking about my weave is an end-user company. And if you're curious about what we do, you can find us on get we've.com.
00:00:30 [W1] I'm saying this because I want you to know that I'm going to teach you about a lot of different projects and I want you know, we have no stake.
00:00:35 [W1] I have no personal professional stake in any of these projects. So you're not getting any sport of specific vendor pitch here.
00:00:42 [W1] Now, what I want to do in this presentation is cover the 12 graduated and 21 incubating projects in the cncf. That's 33 projects total and I have less than 35 minutes.
00:00:52 [W1] It's to teach them all to you.
00:00:54 [W1] Now.
00:00:55 [W1] I'm not going to obviously in that amount of time be able to teach you everything about a project or cover all the features and nuances of Any Given project. But I want you to leave this presentation having a basic understanding of what these projects do and how they relate to each other and know which ones you're curious about might want to learn more about or might want to use your self.
00:01:11 [W1] So before we go further, let's say a big thanks to fifty dot IO for these characters.
00:01:16 [W1] I'm going to use them pretty liberally in the presentation because I find them to be a lot more interesting and fun than just having boxes called app one and up two.
00:01:24 [W1] So you're going to see these characters show up quite a bit specifically going to see fit p and when you see fit be it's I'm going to refer to a legacy application written in an unspecified language.
00:01:33 [W1] So could it be any language and we see Goldie it specifically a newer application written in go before we go into the projects themselves. Let's talk about
00:01:40 [W1] What it means to be graduated in the cncf, so if I've got a project and that project wants to be a graduated project. You have to pass a few things first.
00:01:48 [W1] You have to be receiving constant contribution from at least two different organizations into the project. That means that you know that if you're going to use a project that's graduated.
00:01:57 [W1] You're going to use a project that's graduated.
00:01:59 [W1] You're not dependent on one organization to maintain the project.
00:02:02 [W1] You also have to certify that you're passing the best practices for core infrastructure and fully open source software.
00:02:08 [W1] You have to pass a security audit publish some metadata around who governs the project who's in charge of the code and who's using the project once you do that you pass a supermajority vote from the cncf and you've become a graduated project.
00:02:23 [W1] Notice that never in that did I talk about something being quote unquote production-ready graduated status in the cncf.
00:02:31 [W1] You don't have to be graduated to be production-ready. There are plenty of incubating and even sandbox projects that might be production ready for you depending on what you're trying to get.
00:02:39 [W1] So don't be afraid to use the projects that are incubating or sandbox.
00:02:43 [W1] Although it is great to be a graduated project.
00:02:45 [W1] It's not necessarily just a measure of production Readiness, but rather a measure of openness and transparency.
00:02:52 [W1] Now let's go and dig into all these projects.
00:02:54 [W1] Could I have a lot to cover?
00:02:55 [W1] The first one I want to talk about is containerd e it's a Daemon like you might expect from the deep meaning.
00:03:00 [W1] it's a process that runs on your systems to help you manage containers. So you can use containerd e directly in your code to build container images and to manage containers, but most of us won't use containerd e directly most of us will use containerd e as part of something like Docker or
00:03:15 [W1] object so when most of us build a container image today, and we do that with Docker we're doing it with containerd e so, in fact, this project is so useful that is actually built into a lot of the public Cloud communities offerings and the k3s project which is in sandbox state that allows
00:03:25 [W1] all or mostly full-featured communities cluster all-in-one binary and that does that using containerd e so if you're curious about low-level container operations check out containerd e next is TUF TUF stands for the update framework and it's all about dealing with managing
00:03:33 [W1] Series of standards and tools that we can build into our code and use to deal with updates.
00:03:36 [W1] I'm being intentionally vague here because the tough standards don't prescribe to any one specific kind of update.
00:03:42 [W1] You might be they might be packaged updates or image updates. But anything that you might want to get regular updates from and verify the source of so that's tough.
00:03:49 [W1] I won't cover it much because most of us won't use tough.
00:03:51 [W1] We're going to use the reference implementation of tough or the main implementation of tough in notary now notary takes all the tough ideas, but builds us some actual tools that most of us can use rather than writing.
00:04:01 [W1] in your own code to deal with update management verification and most of us will probably use notary as part of image signing and verification to ensure that when we get an image, we get it from somebody that we trust and it hasn't been modified along the way so that when we pull images when
00:04:16 [W1] Structure we know that we can trust where it came from.
00:04:17 [W1] So that's a notary get tough and notary have this tight relationship where tough is the standards and notaries and implementation of it, but you should look into both those if you're curious about that.
00:04:27 [W1] Next is Harbor Harbor is a private image registry and it has all the features you might expect from a private image registry. You could upload oci compliant images and get things like validation through something like notary or even image inspection or
00:04:42 [W1] And along with a bunch of other features like you might expect from a private registry.
00:04:39 [W1] It also has a really cool feature in that it can be a pull-through cash. So you can run Harbor in your infrastructure hook it up to Public Image Registries and then have your clients point to Harbor and when they need an image if the image exists in a public registry Harbor will pull it through
00:04:54 [W1] Only allowing your clients to pull from Harbor instead of always going over the Internet. So if you need to reduce your overall image pull bandwidth, you might check out Harbor for that. Either way. If you're curious about having a private image registry check out Harbor.
00:05:01 [W1] Next is communities.
00:05:02 [W1] Of course, we can't talk loud knative that talking communities Kuma knative was the first cncf member project to reach the graduated status and it is at its core container orchestration engine.
00:05:12 [W1] So when I say that, I mean, we've got a suite of back-end machines in communities. We call these nodes and we want to run workloads on those notes and we can tell communities.
00:05:21 [W1] Hey, I want you to run a workloads.
00:05:22 [W1] Here's what it should look like and kubenetes will put that workloads somewhere. We can then of course scale-up run multiple copies of our workload and fact communities can handle tons and tons
00:05:31 [W1] Of different workloads with different configuration and because it's an orchestrator, it has things like automatic dealing with things like nodes going down.
00:05:39 [W1] So if a node goes down Community sees that and can redistribute the declared workloads.
00:05:59 [W1] And of course I have to bring in keptn Cube here because they're amazing. And why wouldn't you talk about Captain cube? When you get a chance communities is really kind of the heart of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of
00:06:14 [W1] Going down tufin node goes down pretty sees that and can redistribute the declared workloads.
00:06:43 [W1] Of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of integration points for other systems to build upon kubenetes to provide more value using kubenetes as a core.
00:06:54 [W1] So communities has the ability to add storage or networking layers.
00:06:57 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really communities is the heart of cloud native in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do
00:07:12 [W1] Speaking of communities Helm is a package manager for kubernative.
00:07:15 [W1] So Helm is something we do actually have to run in communities because it's specifically around creating and maintaining applications in communities. So we know that we can run our workloads and communities and we call those pods, but it turns out there's actually a lot of things in communities that we could create
00:07:31 [W1] It's a community has the ability to add storage or networking layers.
00:07:34 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really kubenetes is the heart of cloud knative in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do speaking of
00:07:57 [W1] figuration or services or different routing information to all help Telco knative what our application looks likes to describe our application to kubenetes helm allows us to take all that configuration put it into one thing called a Helm chart and that chart is sort of like a
00:08:12 [W1] What my application look like here's all the things you need to make and how they relate to each other.
00:08:15 [W1] We send that to tell them and helped create it in our communities cluster like we expect the great thing about Helm and Helm chart is it's this kind of recipe this redistributable thing. We can take our chart and not only use it internally, but give that chart to other users or distribute that chart out into
00:08:31 [W1] Other users install our applications into their communities clusters.
00:08:32 [W1] So you'll very often see a lot of the things. I'm going to talk about be installable into your clusters using something like helm.
00:08:40 [W1] Another way that you might manage your applications and kubenetes is using something like Argo Argo is also an application manager, but it takes a different tack than Helm Argo is a gitops based system for kubernative application management.
00:08:50 [W1] meaning that we set up one or more git repositories. And in those repositories, we describe what we want.
00:08:56 [W1] Our application will look like in communities.
00:08:57 [W1] We then hook Argo up between the repositories and our communities cluster or clusters and it takes the described application and makes it true and communities and it's doing gitops. So it's always watching the repository and as a repository.
00:09:09 [W1] Changes it sinks those changes into communities as they happen and it allows us to take all the gitops tools we know and love and use them to manage our communities applications.
00:09:18 [W1] So if you're interested in gitops and Kuma Nettie's check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:09:31 [W1] means they're not mutually exclusive you can use Argo and Helm together. It's absolutely fine.
00:09:36 [W1] One other way we might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operators. Now my 32nd what is an operator talk?
00:09:49 [W1] These applications. So if you're interested in gitops and kubenetes check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:10:01 [W1] That means they're not mutually exclusive you can use Argo and Helm together.
00:10:04 [W1] It's absolutely fine.
00:10:05 [W1] One other way.
00:10:07 [W1] We might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operator.
00:10:17 [W1] Has now my 32nd what is an operator talk?
00:10:19 [W1] So an operator can be thought of as an engine.
00:10:22 [W1] It's a process rerun inside our communities cluster that knows how to create applications for us and we create something communities called a custom resource and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the
00:10:59 [W1] and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the engine we've written called an operator knows how to take that resource and make the application in communities Force based on that resource and then if we make a
00:11:14 [W1] Figuration the operator can operate on that and make that copy of the application.
00:11:18 [W1] Really the focus.
00:11:19 [W1] here is the crd S rather than build a Helm chart or a git repository.
00:11:23 [W1] We actually put a resource into the kuma knative JPI describing our application and we use the operator framework or other tools to create an operator that knows how to take those custom resources and turn them into applications in our cluster.
00:11:35 [W1] So if you're curious about a more advanced way to manage your applications throughout their entire life cycle inside communities, check out the operator framework and just like before the
00:11:43 [W1] Operators and Helm and Argo those are not all mutually exclusive and a lot of them can work together very well.
00:11:50 [W1] Next is Contour Contour is sort of fills another Gap in the communities ecosystem.
00:11:54 [W1] So we talked about communities can run our workloads.
00:11:57 [W1] It can also run these things called Ingress and egress really is just a set of configuration.
00:12:02 [W1] It's a set of configurations like host and path based information says, hey if you're coming in for this host and this path go to this workloads.
00:12:19 [W1] Duration and route to the right place based on that config and that's what it called an is called an Ingress controller in kubenetes and there are a lot of English controllers.
00:12:27 [W1] You can run and Kuma Nettie's many of them are Legacy web servers kind of jammed into this Ingress controller role, but Contour is built from the ground up to be an Ingress controller for kubernative.
00:12:37 [W1] And to try to do the right things for you right from the start check out Contour.
00:12:42 [W1] Next is kubenetes Cube Edge is interesting because it's orchestration built on communities.
00:12:47 [W1] So we already know communities can do container orchestration, but kubenetes is a platform that leverages the kubenetes apis and extension points to allow us to do Edge compute management using the kuma Nettie's apis.
00:12:58 [W1] So if you're curious about doing Edge management and managing compute at the edge and you want to use the community's tools and API check out kubeedge.
00:13:07 [W1] Next is Rook Rook is also orchestration that runs in communities but instead of managing other devices or containers Rook is about managing storage inside kubernative.
00:13:15 [W1] So Rook runs and communities and allows you to deal with block storage or object storage and it can do things like provide persistent volumes for your workloads. So they can have a volume that follows them around in your cluster or do other things like said object storage and other kinds of
00:13:30 [W1] Would build on top of criminality is using Rook. So if you're curious about storage and Kuma Nettie's checkout Rook.
00:13:28 [W1] Next is cryo or cri-o cri-o stands for containerd runtime interface. It is kind of a layer that we described that we defined to help communities run containers + O stands for oci compliant.
00:13:40 [W1] So every single node in our communities cluster runs this thing called a qubit and it's the kubelet job to create and manage containers over their life cycle on the Node, but there is this kind of squishy blue layer.
00:13:51 [W1] I've drawn between the cubelet and actually doing things with containers and that is where cri-o the container runtime interface lives.
00:13:58 [W1] And it is definitely where cri-o lives because cryo is a container runtime interface built specifically for communities to be simple and fast and efficient.
00:14:09 [W1] efficient. So if you're curious about container runtime for communities built for kubenetes check out cryo next to cni-genie.
00:14:27 [W1] It needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:36 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating.
00:14:46 [W1] if you're interested in the low-level operations of networks, check out cni-genie.
00:14:49 [W1] The network sometimes that's on the same node, but very often it's across nodes and something needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:50 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating. So if you're interested in the low-level operations of networks, check out cni-genie,
00:15:05 [W1] Next is grpc.
00:15:07 [W1] So we've got our applications and they need to talk to each other one way.
00:15:10 [W1] They might do that is over something like HTTP.
00:15:13 [W1] It's been around for a long time.
00:15:14 [W1] It's kind of a low level interrupt but HTTP, although it's great has its problems primarily.
00:15:19 [W1] It's got a lot of overhead because it's connection listen stateless.
00:15:23 [W1] There's a lot of overhead and every single HTTP request to describe what's going on grpc exists as an alternative or can run alongside HTTP as another way for applications to communicate with each other over a network and this is stateful and hashicorp.
00:15:37 [W1] Less overhead so it can be a lot lot faster than HTTP grpc also has cool things like bi-directional streaming where applications can stream over a single connection both ways.
00:15:46 [W1] If you leverage things like Proto, you can also get things like type safety using grpc. So if you're looking to do application to application communication over the network and want to go above and beyond what you get from HTTP, check out grpc
00:16:01 [W1] Next is called DNS core DNS is like you might expect a DNS server built for the cloud.
00:16:05 [W1] So we if we are honest with each other DNS is the oldest form of what we call service Discovery, right?
00:16:11 [W1] We ask for something by name DNS gives us back where that lives and even though we've got all of these cool new ways to do service Discovery in the cloud.
00:16:18 [W1] We still tend to tend to use DNS a lot and Cordia necessary about brand new DNS server that is built for the cloud and this picture that I'm showing you seems empty. It seems like there's a lot missing and that's because Cortana is really exist at the coredns.
00:16:31 [W1] seems like there's a lot missing and that's because Cortana is really exist at the core of this big ecosystem of plug-ins for DNS has plug-ins for multiple ways to serve DNS traffic whether that's the traditional UDP or new protocols like HTTP to or grpc it
00:16:46 [W1] Bring in configuration and receive both initial configuration and constant active reconfiguration from multiple sources, including things like Kuma Nettie's at CD which we'll talk about or even public clouds record.
00:16:59 [W1] DNS can privately serve the records that you define in your public Cloud DNS systems. It also has plugins to help you do things like rewrites and tracing and metrics on your DNS and really brings DNS into the modern age.
00:17:11 [W1] In fact, all these features make or DNS the recommended and go to DNS Lucien for doing
00:17:15 [W1] DNS inside of communities and it has been that way for quite a while now. So if you're curious about a modern-day nsmcon mentation checkout coordinates
00:17:24 [W1] Now before I talk about the next two projects, I want to briefly describe.
00:17:27 [W1] What a servicemeshcon.
00:17:53 [W1] Not just not be possible to change the code of a specific application as servicemeshcon.
00:17:59 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application, although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency
00:18:14 [W1] As servicemeshcon as well.
00:18:15 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application. Although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency encryption that kind of
00:18:36 [W1] Thing.
00:18:37 [W1] Well, once we've got all these proxies distributed and running in our ecosystem will want to control plane that can manage these distributed proxies and give us a way to view what's happening with them and control them you combine a proxy in a control plane and you get a servicemeshcon it powerful features like metrics
00:18:52 [W1] You combine a proxy in a control plane? And you get a servicemeshcon powerful features like metrics load balancing encryption and transparency and tracing all from the proxy without ever having to change your service code. And this is very very powerful and there are two projects.
00:19:07 [W1] I want to talk about that are part of a servicemeshcon.
00:19:22 [W1] From the ground up for linkerd E to do this servicemeshcon Clemente these servicemeshcon ideas.
00:19:27 [W1] It also comes with the linkerd E to control plane that allows you to manage all the proxies.
00:19:31 [W1] So it really is a full complete and and servicemeshcon Lucien.
00:19:35 [W1] It's also very easy to get up and going and to use linkerd e, so if you're looking to implement a servicemeshcon you want to get up and going and want a complete solution.
00:19:42 [W1] You can check out linkerd e another alternative for a servicemeshcon voi now Envoy is a bit different in that Envoy just focuses on being the proxy process for the servicemeshcon.
00:19:52 [W1] If you're asking where is the control plane Envoy doesn't provide when it doesn't prescribe one it leaves that open to the implementer and this seems like a downside that Blank Spot may seem initially compared to linkerd e like it's a problem, but it's actually a great power the fact that the envoy haproxy folks
00:20:08 [W1] Hi early on being a servicemeshcon c or just a service proxy means that it can really focus on that and provide the best possible proxy that you could need and actually Envoy is the backing proxy between a lot of other Cloud native projects.
00:20:21 [W1] So if you're curious about just running a service proxy check out Envoy.
00:20:26 [W1] Next is opentracing like you might guess it's all about dealing with traces. So we know that we can use something like a servicemeshcon automatic tracing between app user requests and the applications it bounces around.
00:20:36 [W1] But if we want to know what that user request does in each application visits as it kind of goes from method to method and spends a different amount of time in each application doing different things.
00:20:46 [W1] We need to instrument or application.
00:20:48 [W1] We need to write code to create these tracing these traces and that's what opentracing exist to do it like you might have guessed from the name opentracing is
00:20:56 [W1] Provider agnostic. So the great thing about it is you can instrument your applications.
00:21:00 [W1] You can write coding replications to generate Trace data and opentracing works with a multitude of providers. Meaning that it doesn't care who you use you instrument once and never have to do it again one place. You might export this Trace data from opentracing or elsewhere is to Jaeger.
00:21:14 [W1] Jaeger is a trays aggregation and Trace management platform.
00:21:17 [W1] So we've got our Trace data that we've got from our servicemeshcon something from something like opentracing we need to take that data and send it somewhere so that we can aggregate in-toto.
00:21:26 [W1] The core of tracing in your Cloud native system. So that's Jaeger. If you're curious about somewhere to send your traces and view your traces in the cloud. Check out Jaeger.
00:21:31 [W1] Next is Prometheus.
00:21:33 [W1] Prometheus is all about dealing with metrics in the cloud.
00:21:35 [W1] Now.
00:21:35 [W1] We know that we were on applications all over the place, especially in the cloud and those applications are generating data, right they want to generate data around how many requests they're making how many requests are succeeding and failing that kind of stuff what we used to do when we want. It application metrics was instrument our
00:21:50 [W1] Prometheus is all about dealing with metrics in the cloud. Now.
00:21:40 [W1] We know that we run applications all over the place, especially in the cloud and those applications are generating data, right then we want to generate data around how many requests are making how many requests are succeeding and failing that kind of stuff what we used to do when we wanted application metrics was instrument our
00:22:26 [W1] They have the applications take those metrics and Export them to a specific provider. And if we ever wanted to switch metrics providers we couldn't do it because the applications would have to be retooled.
00:22:35 [W1] Prometheus turns out on its head Kuma T says well part of the Prometheus spec is saying here's a standard way that you're all going to serve up metrics.
00:22:43 [W1] You're going to expose a web page and give me your metrics and Prometheus is going to go to each application individually and pull that metric data down and aggregate it and it knows where your application is live because remember we're in the cloud things are coming and going all the time.
00:22:56 [W1] It knows where your applications are and how to find them by having Cloud integration or Kuma Nettie's Integrations.
00:23:01 [W1] So that as your applications come up and down and move Prometheus always knows where
00:23:05 [W1] Go to get that metric data and it kind of flips the whole idea of metrics on its head.
00:23:10 [W1] Once Prometheus has gone to scrape that metric data and pulled it in.
00:23:13 [W1] It pulls it into its own internal time series data base and can give you features like charting and alerts and Metric searching and other API Integrations that really sort of like Jaeger was with traces.
00:23:23 [W1] Let's Prometheus be the core of metrics in the cloud.
00:23:26 [W1] So if you're interested in an open-source metric system, check out Prometheus now if Thanos exists alongside and with Prometheus to solve some specific problems. It is very easy to get up and going with a single.
00:23:40 [W1] Instance in the cloud no big deal you can get up and going very quickly. But if you're going to run multiple Prometheus instances and if they're going to be distributed across geographic regions, or you want fault tolerance, the Prometheus project isn't really focused on solving that right now, but the Thanos
00:23:55 [W1] Thanos as a wrapper around one or more Prometheus instances that allows you to aggregate data and go to Thanos and run a metrics query and have it go to all your Prometheus instances and run that query for you.
00:24:05 [W1] Then I'll also has the ability to take that data and Export it into multiple cloud storage mechanisms so that you can have long-term Prometheus storage because Prometheus doesn't tend to want to keep data for very long.
00:24:17 [W1] So if you're curious about distributed Prometheus long term metrics from Prometheus check out Thanos in that same vein.
00:24:23 [W1] There's the cortex project now the cortex project also exists to solve the multiple Prometheus problem, but it works a bit differently.
00:24:29 [W1] It's designed to always put all of the data rather than wrapping. It just ingests all the data from all your Prometheus instances and stores it in its own internal architecture that way when you query your metrics, you don't even have to go to your Prometheus instances.
00:24:42 [W1] They just act as a data source and really cortex is at the heart of your metrics at that point. So
00:24:47 [W1] If you're interested in long-term aggregated Prometheus metrics, you can check out cortex.
00:24:51 [W1] Now. I know those two projects than us and cortex seem very similar and that's because they are you'll have to do your own research to find out which of these two projects might be the right one for you.
00:24:59 [W1] Next is fluent D. Now fluent D is all about dealing with streaming text processing and very often log processing in the cloud and elsewhere.
00:25:08 [W1] So at its core affluent D can be set up to taken multiple streams of text read those dreams as they come in process them internally and spit them out into other places.
00:25:17 [W1] Data sources and where you might want to put that data all this makes fluently.
00:25:14 [W1] I really great place to handle logs from communities.
00:25:17 [W1] So we've got communities.
00:25:18 [W1] We've got our workloads running and those workloads are all generating log data, which is Text data and a generating that all the time and coming and going and very often fluent D is the engine behind most of the Prometheus or sorry most of the communities installations that you run where
00:25:33 [W1] With reading those logs as they're generated reformatting them and sending them to Some Cloud integration. So that even though you've got a containers running across many many back and machines.
00:25:42 [W1] You can view all their logs in one place.
00:25:45 [W1] Thanks to the aggregation and Export and manipulation provided by something like fluent D. So whether you're using fluent D and don't know it because it's in communities or you're interested in doing log processing directly with fluentd d you should check it out.
00:25:58 [W1] Next is vitess vitess is all about dealing with relational databases in the cloud.
00:26:02 [W1] So it's easy easy easy to run a relational database in the cloud or increment Eddie's but the problem with these databases that they tend to have to scale vertically and that is really vulnerable and brittle in the cloudbees cuz we don't want things to scale vertically. We want to scale horizontally want to be fault tolerant and distributed
00:26:17 [W1] Want to split that big horizontally or vertically skill database into multiple smaller databases and vitess exist to help us do that vitess is a layer that runs on top of the MySQL or mariadb engine that you already know and trust but allows for powerful features like
00:26:32 [W1] Hurting and you can actually increase replicas and Richard and do all sorts of database manipulation using vitess.
00:26:30 [W1] Well while just using the standard MySQL engine vitess also has a proxy process that you can run that allows you to take in SQL or grpc traffic and distribute it to these kind of more Dynamic sharded replicated changing database instances.
00:26:45 [W1] all this allows vitess to be a really great solution for running relational databases in communities allows you to scale and distribute and be fault tolerant all while still using the kind of database interface that you know and trust
00:26:55 [W1] so that's vitess next is titanium or tikv.
00:26:59 [W1] Tikv is a key value store. So it's all about dealing with key values.
00:27:02 [W1] So it does the things you might expect from a key Value Store does adds it does updates it does deletes but the cool thing about tikv is that it scales horizontally, like vitess escapes horizontally very very well. In fact according to vitess page.
00:27:15 [W1] They say that it scales up to petabyte-scale with a key value data.
00:27:17 [W1] that's huge scale. Another really great thing about tikv is that it supports distributed acid compliant transactions, so you into a
00:27:25 [W1] Tikv insulation you can say I want you to update this key.
00:27:28 [W1] delete these three keys change those two keys and I want you to do all of those operations at once or not at all that ensures that if you're doing multiple key operations in your key Value Store, you don't have to get stuck in a halfway state where one transact one operation worked and the other failed and you
00:27:43 [W1] So if you're interested in really high scale key value store or transactional key Value Store checkout tikv.
00:27:47 [W1] SED is also a key value store.
00:27:51 [W1] That's Cloud native.
00:27:51 [W1] So it does all the same things that I've just described at updates deletes but ci/cd rather than focusing on sheer scale has focused on Simplicity.
00:27:59 [W1] So it's very very easy to get up and going with an STD installation or an STD cluster.
00:28:04 [W1] It's often just a few commands or a single file away from having a fully functional at CD cluster. So it has all the things you might expect.
00:28:12 [W1] Sect leader election fault tolerance distributed load, but it's much much simpler to run than some of the other offerings.
00:28:19 [W1] In fact the kind of combination of features and simplicity have made SED the go to back end for the kubernative API for a long time for a while.
00:28:26 [W1] It was the only back end.
00:28:28 [W1] There are a few others now, but odds are really good that if you're using Kuma Nettie's you're probably using a CD behind the scenes to store all the data you're sending into the kubernative API, but you can absolutely use SED directly for yourself as a key value.
00:28:42 [W1] Store next is dragonfly dragonfly is all about peer-to-peer file transmission.
00:28:47 [W1] So we've got people peers they can send files to each other dragonfly is agnostic about the file content, but it does have some first-class Integrations for images.
00:28:55 [W1] So a dragonfly has knative Integrations to deal with image transmission peer-to-peer and that's not very interesting but what's cool about dragonfly is its distributed peer-to-peer transmission. So you set up dragonfly nodes throughout your system and when anybody wants download a specific image rather
00:29:10 [W1] To a single place to download the whole image. They can download parts of that image or parts of any file from the peers that have chunks of that file rather than always having to go out.
00:29:19 [W1] So if you're curious about a better way to do peer-to-peer file transmission or a better way to do image transmission check out dragonfly.
00:29:27 [W1] Next is cloudevents cloudevents. Like you might guess is all about dealing with event infrastructure in the cloud.
00:29:32 [W1] So we've got our applications and they have the ability. Of course to weave say we decide that we want to do event based infrastructure.
00:29:38 [W1] One thing we might not agree on is the exact format of our events.
00:29:42 [W1] We may want to use different structures different terms and it makes it really hard to say.
00:29:46 [W1] Well we all want to do events but we can't agree on what the event should look like cloudevents exist to be a series of standards and sdks for us to work with event based infrastructure and all agree that we're going to use cloudevents.
00:29:57 [W1] So that we can very easily and efficiently interop with each other because we all use the same back end event structure.
00:30:02 [W1] So if you're curious about event based infrastructure, check out cloudevents in that same vein, let's talk about Nats Nats is at its heart really a message bus. So you have producers and consumers where you can put messages into Nats and get them from other processes and Nats of course can run distributed
00:30:17 [W1] With lots and lots of producers and consumers.
00:30:02 [W1] It supports a lot of different event bus systems.
00:30:05 [W1] So you can do things like pub/sub where you publish a message and multiple subscribers get that message.
00:30:11 [W1] You can do request reply where you were send specifically to someone and get a specific answer back or you can do topic-based or streaming event processing all using Nats Nats also scales dynamically and very efficiently.
00:30:24 [W1] So not only is it fast and flexible and really efficient but it's scales really well, which makes it a great solution for doing event based.
00:30:30 [W1] Lecture in kubernative. So that's Nats. If you're curious about building an event based system.
00:30:34 [W1] Check it out.
00:30:35 [W1] Next is spiffe e or the secure production identity framework for everyone and it's all about like you might guess identity spiffe e is a set of standards and tools for dealing with identity in the cloud now when I say identity, I don't just mean users.
00:30:48 [W1] I mean identity at the node level identity at the workloads level or identity at processes inside the workloads spiffe. He's really about saying well, let's take identity and go as deep as we need to and be more Dynamic and more fluid and more graphql.
00:31:00 [W1] annular if we need to to help us deal with identity in the cloud spiffe e is another case where you probably won't use it directly you probably use the Spire project which takes all the implementations and standards of spiffe e and build some tools the Spire server and the Spire agent where
00:31:15 [W1] I meant something to spiffe e Concepts and get this kind of identity stuff.
00:31:18 [W1] I've been talking about without having to write your own code.
00:31:20 [W1] So if you're interested in identity and you want to write your own code check out spiffe e or if you want to just leverage the spiffe E Concepts check out spire.
00:31:29 [W1] Next is open policy agent.
00:31:30 [W1] It's all about dealing with policy enforcement.
00:31:32 [W1] So we feed open policy agent policy documents saying here's what we do and do not want to allow into a given system.
00:31:38 [W1] And then as we feed objects into that system Opa either accept them or reject them based on the policy.
00:31:43 [W1] It's been given I'm being intentionally vague because open Opa doesn't prescribe a specific thing that it's enforcer of opa has been used to enforce policy for a ton of things one thing. Okay fits really well on to though is Kuma Nettie's Opa can run a top.
00:31:58 [W1] And in front of your communities API so that you can kind of control what you allow into your kubenetes cluster and what you don't based on the policy that you give Opa.
00:32:07 [W1] So if you're interested in policy definition or policy enforcement check out Opa last but not least is Falco Falco is all about container runtime security.
00:32:17 [W1] So we've got our images and we can use things like notary to validate that we're running images we trust but we might still want to watch these images or these containers are these processes the entire time they're running so fast.
00:32:28 [W1] Echo does that Falco is this to run in our infrastructure and watch our processes all the time and it has a set of internal rules that says what it expects them to do and what expect them not to do and if any of our processes does something we don't expect like accessing a database. We didn't expect it to reach
00:32:43 [W1] At and send an alert when it happened so you get always on active security for your workloads.
00:32:41 [W1] So if you're curious about that, check out Falco.
00:32:44 [W1] So that's it.
00:32:46 [W1] I have covered all of the projects in this amount of time. Hopefully you kind of what kind of whet your appetite for a lot of these given you a basic idea of what each of these projects does and how they fit together.
00:32:54 [W1] So you can kind of go forth and learn more about them as you see fit one last thanks to fifty dot IO for these great characters.
00:33:01 [W1] I absolutely love them.
00:33:02 [W1] I use inkscape to create my presentations.
00:33:05 [W1] So Z to animate the presentations and open clipart to get art when I can't draw things myself, and that's it. Again. My name is Carson Anderson.
00:33:14 [W1] Work for we've not that we've you can find out what we do at get we've got cam. You can follow me at Twitter at Carson underscore Ops and on GitHub at carcinoid. Thank you so much for all your time, and I hope to see you again soon.
00:33:30 [W1] All right, so we have time for a bunch of questions.
00:33:34 [W1] Hopefully everybody enjoyed that. I had a blast making it don't mind the extra wide V on the camera here.
00:33:40 [W1] We're all working from home.
00:33:41 [W1] So I'll do what I can to make a little prettier for you guys.
00:33:45 [W1] Let's go ahead and I've been answering a lot of questions as we go.
00:33:49 [W1] I'm going to just kind of buzzed through the last bit that we have on The View here.
00:33:54 [W1] Let's see.
00:33:57 [W1] Be scanning. I did mention that as part of the harbor project.
00:33:59 [W1] They can integrate with some vulnerability scanning tools.
00:34:02 [W1] So my recommendation for dealing with secure images is to use something like that.
00:34:08 [W1] And also I highly highly recommend everybody when they deploy these kind of images pin to a version and pin do a hash if you can that's really my best advice for dealing with secure images.
00:34:21 [W1] Next is cloudevents and a question about does cloudevents work with Kafka and does it have some sort of knative integration as far as I'm aware cloudevents will work with any messaging system.
00:34:31 [W1] It's really a standard around the structure and the format of the messages rather than dealing with exactly how those messages are transmitted between systems.
00:34:39 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:34:45 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or
00:34:51 [W1] Nats no problem
00:34:55 [W1] question about would fluent DB ideal for scraping DOT log files and shipped to Greylock.
00:35:00 [W1] Absolutely.
00:35:01 [W1] That's I mean fluentd really at its core is a text processing system.
00:35:05 [W1] But if you go to their site, they really push and they really show that it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off to external systems things like great log, so I would recommend fluentd for that
00:35:20 [W1] Then dealing with exactly how those messages are transmitted between systems.
00:35:16 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:35:22 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or Nats. No problem.
00:35:32 [W1] Question about would fluent DB ideal for scraping DOT log files and ship to Greylock.
00:35:38 [W1] Absolutely.
00:35:38 [W1] That's I mean fluentd really at its core is a text processing system. But if you go to their site, they really push and they really show that it's it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off
00:36:47 [W1] Is there a Secret store / Vault? I'm not quite sure the context are so kubernative.
00:36:52 [W1] I'll just kind of answer. We had another question about Secrets earlier Lancer kind of my given stock answer for secret. So kubenetes has built-in support for secrets and those have been encrypted for quite a while and a lot of in a lot of the most recent communities versions.
00:37:08 [W1] I also really highly recommend people when they're using things like Argo or other stateful deployments of or gitops for their community's resources Secrets kind of become a problem for a lot of people.
00:37:18 [W1] I really recommend looking into things like the bank vaults operator or there are operators built for most of the cloud secret management solution.
00:37:29 [W1] So if you're running in a public Cloud, there's usually a secret management solution provided for you Amazon has a secret manager gcp as a Secrets manager.
00:37:37 [W1] And most of them have some sort of operator where you can put your secret securely into that system and then set it up so you can create a Kuma knative resource that references those secrets and pulls them down as needed.
00:37:49 [W1] That's a really complicated.
00:37:51 [W1] I actually have an entire other talk about that that people can hit me up for about how we built a secret management system built on the cloud for kubernative. We've but in general I really recommend those kinds of operator based secret Solutions
00:38:06 [W1] Thanks for everybody.
00:38:07 [W1] There's a lot of really great feedback here that just you guys loved it.
00:38:10 [W1] So I'm really really happy to hear that.
00:38:11 [W1] Let me see if I can get down to some more questions.
00:38:16 [W1] Kafka versus Nats Odette, this is a Bugaboo for me.
00:38:19 [W1] I really really am constantly surprised at the usability and scalability of nats.
00:38:27 [W1] If you're looking in this, you know where cncf so spoiler alert is I'm going to kind of lean towards cncf projects here, but I really recommend looking into Nats before Kafka from most people most of the time.
00:38:35 [W1] It's more modern.
00:38:37 [W1] It's faster. It's easier to deploy like it may not scale to the ultra giant scale that Kafka purports to go to but most of us don't need that like Mega scale.
00:38:47 [W1] So I really really recommend people look into a Nats you'd be surprised how easy it is to get up and get going with Nats and and
00:38:54 [W1] get value out of it very quickly.
00:38:58 [W1] Yeah, you can't see the sweatpants. Although you also can't see the dog. So that's unfortunate.
00:39:04 [W1] Let's see.
00:39:04 [W1] Yep going through bunch of my talks. So
00:39:12 [W1] I selected this list.
00:39:14 [W1] Literally my proposal for the presentation was I want to cover all of the cncf graduated incubating projects and and introduce everybody to them so that they could go forth in the conference and learn more and so that was my proposal to the the conference was I'm going to do this
00:39:29 [W1] Between my proposal and when I had to actually make the presentation 12 or projects or added so I made a proposal and I end up sticking with it, but it was literally just wanted everybody to have heard of all these projects these top two tiers of the foundation and I
00:39:44 [W1] two weeks creating the presentation demoing different tools getting to learn all of them ins and outs I could so I could teach you guys, you know, the right things sto 1:7 and linkerd E2 which one is better and why that is a whole
00:39:50 [W1] Sto 1:7 and linkerd E2 which one is better and why that is a whole series of talks and probably a whole track on its own. I would say personal experience linkerd e is exceptionally easy to get up and going quickly.
00:40:00 [W1] And so it's a great way to get into a servicemeshcon cept without a lot of extra infrastructure.
00:40:06 [W1] I won't say better or worse because sto and Envoy have a really really broad feature set and are really successful for a lot of companies. So short version check out linkerd.
00:40:17 [W1] D2 trying to get up quickly and then maybe you'll move on DSD. Oh, maybe you won't depending on what you actually want out of your servicemeshcon.
00:40:47 [W1] Somebody asked where can we get the presentation? If you want the source code for this presentation all of the artwork and and everything. I used to make the talk. You can find that on my GitHub at carcinoid.
00:40:58 [W1] So if you just go to GitHub and search for carcinoid, they'll be a Toc pinned at the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:10 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:41:16 [W1] Repeating his questions.
00:41:17 [W1] So somebody asked where can I get a recording of the talk?
00:41:19 [W1] You'll be able to find that on this conference site after the day is done.
00:41:25 [W1] I think every day at the end of the day they're going to release the recordings.
00:41:28 [W1] My presentation itself is on my GitHub.
00:41:33 [W1] So somebody asked where can we get the presentation if you want the source code for this presentation all of the artwork and and everything I use to make the talk you can find that on my GitHub at carcinoid. So if you just go to GitHub and search for carcinoid
00:41:46 [W1] said they'll be a talk pin to the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:56 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:42:05 [W1] So we also asked about my session on operators. I would I did have a talk last year about operators at keep con. If you want to find out more about that.
00:42:12 [W1] You can go find it on my GitHub as well.
00:42:14 [W1] So I'll try to put these actually now that I think about I'll put these in my speaker bio once this is all done.
00:42:19 [W1] I'll go back and put some of my past years talks and that kind of stuff in my speaker bio and links to those. So also check later today and you should have links to most of those things in my bio.
00:42:31 [W1] Another question again downloading presentation will be on this site at the end of the day.
00:42:36 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:42:41 [W1] What would I use for postgres?
00:42:42 [W1] This question actually came up twice.
00:42:43 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:42:49 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:42:55 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:43:07 [W1] A better way to handle postgres inside of kubernative.
00:43:10 [W1] So the I would definitely refer here anybody to postgres operator, even though it's not quite vitess, but for postgres
00:43:20 [W1] What is the go to Kate's UI?
00:43:22 [W1] That is a really good question.
00:43:24 [W1] I think it depends on your use case.
00:43:29 [W1] Another question I can downloading presentation will be on this site at the end of the day.
00:43:34 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:43:38 [W1] What would I use for postgres? This question actually came up twice.
00:43:41 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:43:47 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:43:53 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:44:04 [W1] A better way to handle postgres inside of kubernative.
00:44:08 [W1] So the I would definitely refer anybody to postgres operator, even though it's not quite vitess, but for postgres
00:44:18 [W1] What is the go to Kate's UI?
00:44:20 [W1] That is a really good question.
00:44:22 [W1] I think it depends on your use case.
00:44:24 [W1] I've said it before I'm a contributor and a minor contributor and a big fan of Argo CD. So I actually really like to use Argo and and have people use Argo as a minimalist kind of Katie why it's not a full do everything you
00:44:39 [W1] Like the direct Cloud offerings for you eyes are also pretty useful.
00:44:44 [W1] Looks like they're making me close up on the session.
00:44:46 [W1] So I think what they want me to do is continue answering questions.
00:44:51 [W1] On slack. So please any of these questions I didn't get to if you have more questions for me.
00:44:55 [W1] I'm on the cloud knative computing slack for Kube Khan. So go ahead and hit me up directly and PM me and I'll be happy to answer any questions that you have Beyond this.
00:45:05 [W1] So I think that's it. I don't know when they're going to cut me off or if they want me to just stop talking but
00:45:14 [W1] Wait to see an answer.
00:45:24 [W1] Presentations I'll just keep talking till they kick me off.
00:45:26 [W1] So the question there's another question here is dragonfly like a torrent protocol sort of it's sort of like it's peer-to-peer transmission. So a lot like torque protocol, but like I said, it's primarily focused on dealing
00:45:41 [W1] The meaning of Creations that has is dealing with downloading images from other nodes.
00:45:44 [W1] So, okay.
00:45:47 [W1] Well, that's it.
00:45:48 [W1] Like I said, please hit me up if you have any more questions and thank you so much for watching.

Transcription for wordly [W1]

00:00:00 [W1] Hello and welcome to a flight over the cloud native landscape.
00:00:03 [W1] My name is Carson Anderson.
00:00:04 [W1] I work for we've not that we've not the one you're thinking, but we'll get there in a second.
00:00:08 [W1] You can find me on Twitter at Carson underscore Ops and on GitHub at carcinoid.
00:00:13 [W1] Both of those platforms if you want to see what I'm doing. In fact every presentation I've ever made all the artwork for this presentation of my others is always open source on GitHub.
00:00:21 [W1] So follow me there now before I go any further, I want to see what I mean when I said not that we've the wave that I work for is not the cloud knative vendor, you might be thinking about my weave is an end-user company and if you're curious about what we do, you can find us on get we've got cam I'm
00:00:36 [W1] Not the cloud knative vendor, you might be thinking about my weave is an end-user company. And if you're curious about what we do, you can find us on get we've.com.
00:00:30 [W1] I'm saying this because I want you to know that I'm going to teach you about a lot of different projects and I want you know, we have no stake.
00:00:35 [W1] I have no personal professional stake in any of these projects. So you're not getting any sport of specific vendor pitch here.
00:00:42 [W1] Now, what I want to do in this presentation is cover the 12 graduated and 21 incubating projects in the cncf. That's 33 projects total and I have less than 35 minutes.
00:00:52 [W1] It's to teach them all to you.
00:00:54 [W1] Now.
00:00:55 [W1] I'm not going to obviously in that amount of time be able to teach you everything about a project or cover all the features and nuances of Any Given project. But I want you to leave this presentation having a basic understanding of what these projects do and how they relate to each other and know which ones you're curious about might want to learn more about or might want to use your self.
00:01:11 [W1] So before we go further, let's say a big thanks to fifty dot IO for these characters.
00:01:16 [W1] I'm going to use them pretty liberally in the presentation because I find them to be a lot more interesting and fun than just having boxes called app one and up two.
00:01:24 [W1] So you're going to see these characters show up quite a bit specifically going to see fit p and when you see fit be it's I'm going to refer to a legacy application written in an unspecified language.
00:01:33 [W1] So could it be any language and we see Goldie it specifically a newer application written in go before we go into the projects themselves. Let's talk about
00:01:40 [W1] What it means to be graduated in the cncf, so if I've got a project and that project wants to be a graduated project. You have to pass a few things first.
00:01:48 [W1] You have to be receiving constant contribution from at least two different organizations into the project. That means that you know that if you're going to use a project that's graduated.
00:01:57 [W1] You're going to use a project that's graduated.
00:01:59 [W1] You're not dependent on one organization to maintain the project.
00:02:02 [W1] You also have to certify that you're passing the best practices for core infrastructure and fully open source software.
00:02:08 [W1] You have to pass a security audit publish some metadata around who governs the project who's in charge of the code and who's using the project once you do that you pass a supermajority vote from the cncf and you've become a graduated project.
00:02:23 [W1] Notice that never in that did I talk about something being quote unquote production-ready graduated status in the cncf.
00:02:31 [W1] You don't have to be graduated to be production-ready. There are plenty of incubating and even sandbox projects that might be production ready for you depending on what you're trying to get.
00:02:39 [W1] So don't be afraid to use the projects that are incubating or sandbox.
00:02:43 [W1] Although it is great to be a graduated project.
00:02:45 [W1] It's not necessarily just a measure of production Readiness, but rather a measure of openness and transparency.
00:02:52 [W1] Now let's go and dig into all these projects.
00:02:54 [W1] Could I have a lot to cover?
00:02:55 [W1] The first one I want to talk about is containerd e it's a Daemon like you might expect from the deep meaning.
00:03:00 [W1] it's a process that runs on your systems to help you manage containers. So you can use containerd e directly in your code to build container images and to manage containers, but most of us won't use containerd e directly most of us will use containerd e as part of something like Docker or
00:03:15 [W1] object so when most of us build a container image today, and we do that with Docker we're doing it with containerd e so, in fact, this project is so useful that is actually built into a lot of the public Cloud communities offerings and the k3s project which is in sandbox state that allows
00:03:25 [W1] all or mostly full-featured communities cluster all-in-one binary and that does that using containerd e so if you're curious about low-level container operations check out containerd e next is TUF TUF stands for the update framework and it's all about dealing with managing
00:03:33 [W1] Series of standards and tools that we can build into our code and use to deal with updates.
00:03:36 [W1] I'm being intentionally vague here because the tough standards don't prescribe to any one specific kind of update.
00:03:42 [W1] You might be they might be packaged updates or image updates. But anything that you might want to get regular updates from and verify the source of so that's tough.
00:03:49 [W1] I won't cover it much because most of us won't use tough.
00:03:51 [W1] We're going to use the reference implementation of tough or the main implementation of tough in notary now notary takes all the tough ideas, but builds us some actual tools that most of us can use rather than writing.
00:04:01 [W1] in your own code to deal with update management verification and most of us will probably use notary as part of image signing and verification to ensure that when we get an image, we get it from somebody that we trust and it hasn't been modified along the way so that when we pull images when
00:04:16 [W1] Structure we know that we can trust where it came from.
00:04:17 [W1] So that's a notary get tough and notary have this tight relationship where tough is the standards and notaries and implementation of it, but you should look into both those if you're curious about that.
00:04:27 [W1] Next is Harbor Harbor is a private image registry and it has all the features you might expect from a private image registry. You could upload oci compliant images and get things like validation through something like notary or even image inspection or
00:04:42 [W1] And along with a bunch of other features like you might expect from a private registry.
00:04:39 [W1] It also has a really cool feature in that it can be a pull-through cash. So you can run Harbor in your infrastructure hook it up to Public Image Registries and then have your clients point to Harbor and when they need an image if the image exists in a public registry Harbor will pull it through
00:04:54 [W1] Only allowing your clients to pull from Harbor instead of always going over the Internet. So if you need to reduce your overall image pull bandwidth, you might check out Harbor for that. Either way. If you're curious about having a private image registry check out Harbor.
00:05:01 [W1] Next is communities.
00:05:02 [W1] Of course, we can't talk loud knative that talking communities Kuma knative was the first cncf member project to reach the graduated status and it is at its core container orchestration engine.
00:05:12 [W1] So when I say that, I mean, we've got a suite of back-end machines in communities. We call these nodes and we want to run workloads on those notes and we can tell communities.
00:05:21 [W1] Hey, I want you to run a workloads.
00:05:22 [W1] Here's what it should look like and kubenetes will put that workloads somewhere. We can then of course scale-up run multiple copies of our workload and fact communities can handle tons and tons
00:05:31 [W1] Of different workloads with different configuration and because it's an orchestrator, it has things like automatic dealing with things like nodes going down.
00:05:39 [W1] So if a node goes down Community sees that and can redistribute the declared workloads.
00:05:59 [W1] And of course I have to bring in keptn Cube here because they're amazing. And why wouldn't you talk about Captain cube? When you get a chance communities is really kind of the heart of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of
00:06:14 [W1] Going down tufin node goes down pretty sees that and can redistribute the declared workloads.
00:06:43 [W1] Of a lot of other things that I'm going to talk about and it is that way because it provides a lot of touch points a lot of integration points for other systems to build upon kubenetes to provide more value using kubenetes as a core.
00:06:54 [W1] So communities has the ability to add storage or networking layers.
00:06:57 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really communities is the heart of cloud native in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do
00:07:12 [W1] Speaking of communities Helm is a package manager for kubernative.
00:07:15 [W1] So Helm is something we do actually have to run in communities because it's specifically around creating and maintaining applications in communities. So we know that we can run our workloads and communities and we call those pods, but it turns out there's actually a lot of things in communities that we could create
00:07:31 [W1] It's a community has the ability to add storage or networking layers.
00:07:34 [W1] It can add custom resources which we'll talk about in a second or even extend brand new apis in the communities API ecosystem. So really kubenetes is the heart of cloud knative in a lot of ways now you don't have to run communities be Cloud knative but a lot of us do speaking of
00:07:57 [W1] figuration or services or different routing information to all help Telco knative what our application looks likes to describe our application to kubenetes helm allows us to take all that configuration put it into one thing called a Helm chart and that chart is sort of like a
00:08:12 [W1] What my application look like here's all the things you need to make and how they relate to each other.
00:08:15 [W1] We send that to tell them and helped create it in our communities cluster like we expect the great thing about Helm and Helm chart is it's this kind of recipe this redistributable thing. We can take our chart and not only use it internally, but give that chart to other users or distribute that chart out into
00:08:31 [W1] Other users install our applications into their communities clusters.
00:08:32 [W1] So you'll very often see a lot of the things. I'm going to talk about be installable into your clusters using something like helm.
00:08:40 [W1] Another way that you might manage your applications and kubenetes is using something like Argo Argo is also an application manager, but it takes a different tack than Helm Argo is a gitops based system for kubernative application management.
00:08:50 [W1] meaning that we set up one or more git repositories. And in those repositories, we describe what we want.
00:08:56 [W1] Our application will look like in communities.
00:08:57 [W1] We then hook Argo up between the repositories and our communities cluster or clusters and it takes the described application and makes it true and communities and it's doing gitops. So it's always watching the repository and as a repository.
00:09:09 [W1] Changes it sinks those changes into communities as they happen and it allows us to take all the gitops tools we know and love and use them to manage our communities applications.
00:09:18 [W1] So if you're interested in gitops and Kuma Nettie's check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:09:31 [W1] means they're not mutually exclusive you can use Argo and Helm together. It's absolutely fine.
00:09:36 [W1] One other way we might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operators. Now my 32nd what is an operator talk?
00:09:49 [W1] These applications. So if you're interested in gitops and kubenetes check out Argo, I will also say that Argo you don't have to just use Argo adjust you have use Helm Argo actually knows how to leverage Helm and customize and some other communities deployment mechanisms.
00:10:01 [W1] That means they're not mutually exclusive you can use Argo and Helm together.
00:10:04 [W1] It's absolutely fine.
00:10:05 [W1] One other way.
00:10:07 [W1] We might manage applications or the other applications May create for us to manage them is something called an operator and the operator framework is a set of tools and libraries that helped us build operator.
00:10:17 [W1] Has now my 32nd what is an operator talk?
00:10:19 [W1] So an operator can be thought of as an engine.
00:10:22 [W1] It's a process rerun inside our communities cluster that knows how to create applications for us and we create something communities called a custom resource and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the
00:10:59 [W1] and that custom resource describes just the minimum amount of configuration that would need to exist to describe our application and the engine we've written called an operator knows how to take that resource and make the application in communities Force based on that resource and then if we make a
00:11:14 [W1] Figuration the operator can operate on that and make that copy of the application.
00:11:18 [W1] Really the focus.
00:11:19 [W1] here is the crd S rather than build a Helm chart or a git repository.
00:11:23 [W1] We actually put a resource into the kuma knative JPI describing our application and we use the operator framework or other tools to create an operator that knows how to take those custom resources and turn them into applications in our cluster.
00:11:35 [W1] So if you're curious about a more advanced way to manage your applications throughout their entire life cycle inside communities, check out the operator framework and just like before the
00:11:43 [W1] Operators and Helm and Argo those are not all mutually exclusive and a lot of them can work together very well.
00:11:50 [W1] Next is Contour Contour is sort of fills another Gap in the communities ecosystem.
00:11:54 [W1] So we talked about communities can run our workloads.
00:11:57 [W1] It can also run these things called Ingress and egress really is just a set of configuration.
00:12:02 [W1] It's a set of configurations like host and path based information says, hey if you're coming in for this host and this path go to this workloads.
00:12:19 [W1] Duration and route to the right place based on that config and that's what it called an is called an Ingress controller in kubenetes and there are a lot of English controllers.
00:12:27 [W1] You can run and Kuma Nettie's many of them are Legacy web servers kind of jammed into this Ingress controller role, but Contour is built from the ground up to be an Ingress controller for kubernative.
00:12:37 [W1] And to try to do the right things for you right from the start check out Contour.
00:12:42 [W1] Next is kubenetes Cube Edge is interesting because it's orchestration built on communities.
00:12:47 [W1] So we already know communities can do container orchestration, but kubenetes is a platform that leverages the kubenetes apis and extension points to allow us to do Edge compute management using the kuma Nettie's apis.
00:12:58 [W1] So if you're curious about doing Edge management and managing compute at the edge and you want to use the community's tools and API check out kubeedge.
00:13:07 [W1] Next is Rook Rook is also orchestration that runs in communities but instead of managing other devices or containers Rook is about managing storage inside kubernative.
00:13:15 [W1] So Rook runs and communities and allows you to deal with block storage or object storage and it can do things like provide persistent volumes for your workloads. So they can have a volume that follows them around in your cluster or do other things like said object storage and other kinds of
00:13:30 [W1] Would build on top of criminality is using Rook. So if you're curious about storage and Kuma Nettie's checkout Rook.
00:13:28 [W1] Next is cryo or cri-o cri-o stands for containerd runtime interface. It is kind of a layer that we described that we defined to help communities run containers + O stands for oci compliant.
00:13:40 [W1] So every single node in our communities cluster runs this thing called a qubit and it's the kubelet job to create and manage containers over their life cycle on the Node, but there is this kind of squishy blue layer.
00:13:51 [W1] I've drawn between the cubelet and actually doing things with containers and that is where cri-o the container runtime interface lives.
00:13:58 [W1] And it is definitely where cri-o lives because cryo is a container runtime interface built specifically for communities to be simple and fast and efficient.
00:14:09 [W1] efficient. So if you're curious about container runtime for communities built for kubenetes check out cryo next to cni-genie.
00:14:27 [W1] It needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:36 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating.
00:14:46 [W1] if you're interested in the low-level operations of networks, check out cni-genie.
00:14:49 [W1] The network sometimes that's on the same node, but very often it's across nodes and something needs to exist to kind of Define and Implement standards for how we set up networking between our workloads in a cluster or in the cloud and that's what cncf to do.
00:14:50 [W1] It's a set of standards and tools and some kind of low-level helpers to help you build tools to deal with container to container networking in the cloud or incriminating. So if you're interested in the low-level operations of networks, check out cni-genie,
00:15:05 [W1] Next is grpc.
00:15:07 [W1] So we've got our applications and they need to talk to each other one way.
00:15:10 [W1] They might do that is over something like HTTP.
00:15:13 [W1] It's been around for a long time.
00:15:14 [W1] It's kind of a low level interrupt but HTTP, although it's great has its problems primarily.
00:15:19 [W1] It's got a lot of overhead because it's connection listen stateless.
00:15:23 [W1] There's a lot of overhead and every single HTTP request to describe what's going on grpc exists as an alternative or can run alongside HTTP as another way for applications to communicate with each other over a network and this is stateful and hashicorp.
00:15:37 [W1] Less overhead so it can be a lot lot faster than HTTP grpc also has cool things like bi-directional streaming where applications can stream over a single connection both ways.
00:15:46 [W1] If you leverage things like Proto, you can also get things like type safety using grpc. So if you're looking to do application to application communication over the network and want to go above and beyond what you get from HTTP, check out grpc
00:16:01 [W1] Next is called DNS core DNS is like you might expect a DNS server built for the cloud.
00:16:05 [W1] So we if we are honest with each other DNS is the oldest form of what we call service Discovery, right?
00:16:11 [W1] We ask for something by name DNS gives us back where that lives and even though we've got all of these cool new ways to do service Discovery in the cloud.
00:16:18 [W1] We still tend to tend to use DNS a lot and Cordia necessary about brand new DNS server that is built for the cloud and this picture that I'm showing you seems empty. It seems like there's a lot missing and that's because Cortana is really exist at the coredns.
00:16:31 [W1] seems like there's a lot missing and that's because Cortana is really exist at the core of this big ecosystem of plug-ins for DNS has plug-ins for multiple ways to serve DNS traffic whether that's the traditional UDP or new protocols like HTTP to or grpc it
00:16:46 [W1] Bring in configuration and receive both initial configuration and constant active reconfiguration from multiple sources, including things like Kuma Nettie's at CD which we'll talk about or even public clouds record.
00:16:59 [W1] DNS can privately serve the records that you define in your public Cloud DNS systems. It also has plugins to help you do things like rewrites and tracing and metrics on your DNS and really brings DNS into the modern age.
00:17:11 [W1] In fact, all these features make or DNS the recommended and go to DNS Lucien for doing
00:17:15 [W1] DNS inside of communities and it has been that way for quite a while now. So if you're curious about a modern-day nsmcon mentation checkout coordinates
00:17:24 [W1] Now before I talk about the next two projects, I want to briefly describe.
00:17:27 [W1] What a servicemeshcon.
00:17:53 [W1] Not just not be possible to change the code of a specific application as servicemeshcon.
00:17:59 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application, although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency
00:18:14 [W1] As servicemeshcon as well.
00:18:15 [W1] What if we write a proxy process and this proxy process can be thought of as living around our application. Although it technically lives Just Between the application and the and the network and that proxy is responsible for implementing the code to do all the things. I just talked about transparency encryption that kind of
00:18:36 [W1] Thing.
00:18:37 [W1] Well, once we've got all these proxies distributed and running in our ecosystem will want to control plane that can manage these distributed proxies and give us a way to view what's happening with them and control them you combine a proxy in a control plane and you get a servicemeshcon it powerful features like metrics
00:18:52 [W1] You combine a proxy in a control plane? And you get a servicemeshcon powerful features like metrics load balancing encryption and transparency and tracing all from the proxy without ever having to change your service code. And this is very very powerful and there are two projects.
00:19:07 [W1] I want to talk about that are part of a servicemeshcon.
00:19:22 [W1] From the ground up for linkerd E to do this servicemeshcon Clemente these servicemeshcon ideas.
00:19:27 [W1] It also comes with the linkerd E to control plane that allows you to manage all the proxies.
00:19:31 [W1] So it really is a full complete and and servicemeshcon Lucien.
00:19:35 [W1] It's also very easy to get up and going and to use linkerd e, so if you're looking to implement a servicemeshcon you want to get up and going and want a complete solution.
00:19:42 [W1] You can check out linkerd e another alternative for a servicemeshcon voi now Envoy is a bit different in that Envoy just focuses on being the proxy process for the servicemeshcon.
00:19:52 [W1] If you're asking where is the control plane Envoy doesn't provide when it doesn't prescribe one it leaves that open to the implementer and this seems like a downside that Blank Spot may seem initially compared to linkerd e like it's a problem, but it's actually a great power the fact that the envoy haproxy folks
00:20:08 [W1] Hi early on being a servicemeshcon c or just a service proxy means that it can really focus on that and provide the best possible proxy that you could need and actually Envoy is the backing proxy between a lot of other Cloud native projects.
00:20:21 [W1] So if you're curious about just running a service proxy check out Envoy.
00:20:26 [W1] Next is opentracing like you might guess it's all about dealing with traces. So we know that we can use something like a servicemeshcon automatic tracing between app user requests and the applications it bounces around.
00:20:36 [W1] But if we want to know what that user request does in each application visits as it kind of goes from method to method and spends a different amount of time in each application doing different things.
00:20:46 [W1] We need to instrument or application.
00:20:48 [W1] We need to write code to create these tracing these traces and that's what opentracing exist to do it like you might have guessed from the name opentracing is
00:20:56 [W1] Provider agnostic. So the great thing about it is you can instrument your applications.
00:21:00 [W1] You can write coding replications to generate Trace data and opentracing works with a multitude of providers. Meaning that it doesn't care who you use you instrument once and never have to do it again one place. You might export this Trace data from opentracing or elsewhere is to Jaeger.
00:21:14 [W1] Jaeger is a trays aggregation and Trace management platform.
00:21:17 [W1] So we've got our Trace data that we've got from our servicemeshcon something from something like opentracing we need to take that data and send it somewhere so that we can aggregate in-toto.
00:21:26 [W1] The core of tracing in your Cloud native system. So that's Jaeger. If you're curious about somewhere to send your traces and view your traces in the cloud. Check out Jaeger.
00:21:31 [W1] Next is Prometheus.
00:21:33 [W1] Prometheus is all about dealing with metrics in the cloud.
00:21:35 [W1] Now.
00:21:35 [W1] We know that we were on applications all over the place, especially in the cloud and those applications are generating data, right they want to generate data around how many requests they're making how many requests are succeeding and failing that kind of stuff what we used to do when we want. It application metrics was instrument our
00:21:50 [W1] Prometheus is all about dealing with metrics in the cloud. Now.
00:21:40 [W1] We know that we run applications all over the place, especially in the cloud and those applications are generating data, right then we want to generate data around how many requests are making how many requests are succeeding and failing that kind of stuff what we used to do when we wanted application metrics was instrument our
00:22:26 [W1] They have the applications take those metrics and Export them to a specific provider. And if we ever wanted to switch metrics providers we couldn't do it because the applications would have to be retooled.
00:22:35 [W1] Prometheus turns out on its head Kuma T says well part of the Prometheus spec is saying here's a standard way that you're all going to serve up metrics.
00:22:43 [W1] You're going to expose a web page and give me your metrics and Prometheus is going to go to each application individually and pull that metric data down and aggregate it and it knows where your application is live because remember we're in the cloud things are coming and going all the time.
00:22:56 [W1] It knows where your applications are and how to find them by having Cloud integration or Kuma Nettie's Integrations.
00:23:01 [W1] So that as your applications come up and down and move Prometheus always knows where
00:23:05 [W1] Go to get that metric data and it kind of flips the whole idea of metrics on its head.
00:23:10 [W1] Once Prometheus has gone to scrape that metric data and pulled it in.
00:23:13 [W1] It pulls it into its own internal time series data base and can give you features like charting and alerts and Metric searching and other API Integrations that really sort of like Jaeger was with traces.
00:23:23 [W1] Let's Prometheus be the core of metrics in the cloud.
00:23:26 [W1] So if you're interested in an open-source metric system, check out Prometheus now if Thanos exists alongside and with Prometheus to solve some specific problems. It is very easy to get up and going with a single.
00:23:40 [W1] Instance in the cloud no big deal you can get up and going very quickly. But if you're going to run multiple Prometheus instances and if they're going to be distributed across geographic regions, or you want fault tolerance, the Prometheus project isn't really focused on solving that right now, but the Thanos
00:23:55 [W1] Thanos as a wrapper around one or more Prometheus instances that allows you to aggregate data and go to Thanos and run a metrics query and have it go to all your Prometheus instances and run that query for you.
00:24:05 [W1] Then I'll also has the ability to take that data and Export it into multiple cloud storage mechanisms so that you can have long-term Prometheus storage because Prometheus doesn't tend to want to keep data for very long.
00:24:17 [W1] So if you're curious about distributed Prometheus long term metrics from Prometheus check out Thanos in that same vein.
00:24:23 [W1] There's the cortex project now the cortex project also exists to solve the multiple Prometheus problem, but it works a bit differently.
00:24:29 [W1] It's designed to always put all of the data rather than wrapping. It just ingests all the data from all your Prometheus instances and stores it in its own internal architecture that way when you query your metrics, you don't even have to go to your Prometheus instances.
00:24:42 [W1] They just act as a data source and really cortex is at the heart of your metrics at that point. So
00:24:47 [W1] If you're interested in long-term aggregated Prometheus metrics, you can check out cortex.
00:24:51 [W1] Now. I know those two projects than us and cortex seem very similar and that's because they are you'll have to do your own research to find out which of these two projects might be the right one for you.
00:24:59 [W1] Next is fluent D. Now fluent D is all about dealing with streaming text processing and very often log processing in the cloud and elsewhere.
00:25:08 [W1] So at its core affluent D can be set up to taken multiple streams of text read those dreams as they come in process them internally and spit them out into other places.
00:25:17 [W1] Data sources and where you might want to put that data all this makes fluently.
00:25:14 [W1] I really great place to handle logs from communities.
00:25:17 [W1] So we've got communities.
00:25:18 [W1] We've got our workloads running and those workloads are all generating log data, which is Text data and a generating that all the time and coming and going and very often fluent D is the engine behind most of the Prometheus or sorry most of the communities installations that you run where
00:25:33 [W1] With reading those logs as they're generated reformatting them and sending them to Some Cloud integration. So that even though you've got a containers running across many many back and machines.
00:25:42 [W1] You can view all their logs in one place.
00:25:45 [W1] Thanks to the aggregation and Export and manipulation provided by something like fluent D. So whether you're using fluent D and don't know it because it's in communities or you're interested in doing log processing directly with fluentd d you should check it out.
00:25:58 [W1] Next is vitess vitess is all about dealing with relational databases in the cloud.
00:26:02 [W1] So it's easy easy easy to run a relational database in the cloud or increment Eddie's but the problem with these databases that they tend to have to scale vertically and that is really vulnerable and brittle in the cloudbees cuz we don't want things to scale vertically. We want to scale horizontally want to be fault tolerant and distributed
00:26:17 [W1] Want to split that big horizontally or vertically skill database into multiple smaller databases and vitess exist to help us do that vitess is a layer that runs on top of the MySQL or mariadb engine that you already know and trust but allows for powerful features like
00:26:32 [W1] Hurting and you can actually increase replicas and Richard and do all sorts of database manipulation using vitess.
00:26:30 [W1] Well while just using the standard MySQL engine vitess also has a proxy process that you can run that allows you to take in SQL or grpc traffic and distribute it to these kind of more Dynamic sharded replicated changing database instances.
00:26:45 [W1] all this allows vitess to be a really great solution for running relational databases in communities allows you to scale and distribute and be fault tolerant all while still using the kind of database interface that you know and trust
00:26:55 [W1] so that's vitess next is titanium or tikv.
00:26:59 [W1] Tikv is a key value store. So it's all about dealing with key values.
00:27:02 [W1] So it does the things you might expect from a key Value Store does adds it does updates it does deletes but the cool thing about tikv is that it scales horizontally, like vitess escapes horizontally very very well. In fact according to vitess page.
00:27:15 [W1] They say that it scales up to petabyte-scale with a key value data.
00:27:17 [W1] that's huge scale. Another really great thing about tikv is that it supports distributed acid compliant transactions, so you into a
00:27:25 [W1] Tikv insulation you can say I want you to update this key.
00:27:28 [W1] delete these three keys change those two keys and I want you to do all of those operations at once or not at all that ensures that if you're doing multiple key operations in your key Value Store, you don't have to get stuck in a halfway state where one transact one operation worked and the other failed and you
00:27:43 [W1] So if you're interested in really high scale key value store or transactional key Value Store checkout tikv.
00:27:47 [W1] SED is also a key value store.
00:27:51 [W1] That's Cloud native.
00:27:51 [W1] So it does all the same things that I've just described at updates deletes but ci/cd rather than focusing on sheer scale has focused on Simplicity.
00:27:59 [W1] So it's very very easy to get up and going with an STD installation or an STD cluster.
00:28:04 [W1] It's often just a few commands or a single file away from having a fully functional at CD cluster. So it has all the things you might expect.
00:28:12 [W1] Sect leader election fault tolerance distributed load, but it's much much simpler to run than some of the other offerings.
00:28:19 [W1] In fact the kind of combination of features and simplicity have made SED the go to back end for the kubernative API for a long time for a while.
00:28:26 [W1] It was the only back end.
00:28:28 [W1] There are a few others now, but odds are really good that if you're using Kuma Nettie's you're probably using a CD behind the scenes to store all the data you're sending into the kubernative API, but you can absolutely use SED directly for yourself as a key value.
00:28:42 [W1] Store next is dragonfly dragonfly is all about peer-to-peer file transmission.
00:28:47 [W1] So we've got people peers they can send files to each other dragonfly is agnostic about the file content, but it does have some first-class Integrations for images.
00:28:55 [W1] So a dragonfly has knative Integrations to deal with image transmission peer-to-peer and that's not very interesting but what's cool about dragonfly is its distributed peer-to-peer transmission. So you set up dragonfly nodes throughout your system and when anybody wants download a specific image rather
00:29:10 [W1] To a single place to download the whole image. They can download parts of that image or parts of any file from the peers that have chunks of that file rather than always having to go out.
00:29:19 [W1] So if you're curious about a better way to do peer-to-peer file transmission or a better way to do image transmission check out dragonfly.
00:29:27 [W1] Next is cloudevents cloudevents. Like you might guess is all about dealing with event infrastructure in the cloud.
00:29:32 [W1] So we've got our applications and they have the ability. Of course to weave say we decide that we want to do event based infrastructure.
00:29:38 [W1] One thing we might not agree on is the exact format of our events.
00:29:42 [W1] We may want to use different structures different terms and it makes it really hard to say.
00:29:46 [W1] Well we all want to do events but we can't agree on what the event should look like cloudevents exist to be a series of standards and sdks for us to work with event based infrastructure and all agree that we're going to use cloudevents.
00:29:57 [W1] So that we can very easily and efficiently interop with each other because we all use the same back end event structure.
00:30:02 [W1] So if you're curious about event based infrastructure, check out cloudevents in that same vein, let's talk about Nats Nats is at its heart really a message bus. So you have producers and consumers where you can put messages into Nats and get them from other processes and Nats of course can run distributed
00:30:17 [W1] With lots and lots of producers and consumers.
00:30:02 [W1] It supports a lot of different event bus systems.
00:30:05 [W1] So you can do things like pub/sub where you publish a message and multiple subscribers get that message.
00:30:11 [W1] You can do request reply where you were send specifically to someone and get a specific answer back or you can do topic-based or streaming event processing all using Nats Nats also scales dynamically and very efficiently.
00:30:24 [W1] So not only is it fast and flexible and really efficient but it's scales really well, which makes it a great solution for doing event based.
00:30:30 [W1] Lecture in kubernative. So that's Nats. If you're curious about building an event based system.
00:30:34 [W1] Check it out.
00:30:35 [W1] Next is spiffe e or the secure production identity framework for everyone and it's all about like you might guess identity spiffe e is a set of standards and tools for dealing with identity in the cloud now when I say identity, I don't just mean users.
00:30:48 [W1] I mean identity at the node level identity at the workloads level or identity at processes inside the workloads spiffe. He's really about saying well, let's take identity and go as deep as we need to and be more Dynamic and more fluid and more graphql.
00:31:00 [W1] annular if we need to to help us deal with identity in the cloud spiffe e is another case where you probably won't use it directly you probably use the Spire project which takes all the implementations and standards of spiffe e and build some tools the Spire server and the Spire agent where
00:31:15 [W1] I meant something to spiffe e Concepts and get this kind of identity stuff.
00:31:18 [W1] I've been talking about without having to write your own code.
00:31:20 [W1] So if you're interested in identity and you want to write your own code check out spiffe e or if you want to just leverage the spiffe E Concepts check out spire.
00:31:29 [W1] Next is open policy agent.
00:31:30 [W1] It's all about dealing with policy enforcement.
00:31:32 [W1] So we feed open policy agent policy documents saying here's what we do and do not want to allow into a given system.
00:31:38 [W1] And then as we feed objects into that system Opa either accept them or reject them based on the policy.
00:31:43 [W1] It's been given I'm being intentionally vague because open Opa doesn't prescribe a specific thing that it's enforcer of opa has been used to enforce policy for a ton of things one thing. Okay fits really well on to though is Kuma Nettie's Opa can run a top.
00:31:58 [W1] And in front of your communities API so that you can kind of control what you allow into your kubenetes cluster and what you don't based on the policy that you give Opa.
00:32:07 [W1] So if you're interested in policy definition or policy enforcement check out Opa last but not least is Falco Falco is all about container runtime security.
00:32:17 [W1] So we've got our images and we can use things like notary to validate that we're running images we trust but we might still want to watch these images or these containers are these processes the entire time they're running so fast.
00:32:28 [W1] Echo does that Falco is this to run in our infrastructure and watch our processes all the time and it has a set of internal rules that says what it expects them to do and what expect them not to do and if any of our processes does something we don't expect like accessing a database. We didn't expect it to reach
00:32:43 [W1] At and send an alert when it happened so you get always on active security for your workloads.
00:32:41 [W1] So if you're curious about that, check out Falco.
00:32:44 [W1] So that's it.
00:32:46 [W1] I have covered all of the projects in this amount of time. Hopefully you kind of what kind of whet your appetite for a lot of these given you a basic idea of what each of these projects does and how they fit together.
00:32:54 [W1] So you can kind of go forth and learn more about them as you see fit one last thanks to fifty dot IO for these great characters.
00:33:01 [W1] I absolutely love them.
00:33:02 [W1] I use inkscape to create my presentations.
00:33:05 [W1] So Z to animate the presentations and open clipart to get art when I can't draw things myself, and that's it. Again. My name is Carson Anderson.
00:33:14 [W1] Work for we've not that we've you can find out what we do at get we've got cam. You can follow me at Twitter at Carson underscore Ops and on GitHub at carcinoid. Thank you so much for all your time, and I hope to see you again soon.
00:33:30 [W1] All right, so we have time for a bunch of questions.
00:33:34 [W1] Hopefully everybody enjoyed that. I had a blast making it don't mind the extra wide V on the camera here.
00:33:40 [W1] We're all working from home.
00:33:41 [W1] So I'll do what I can to make a little prettier for you guys.
00:33:45 [W1] Let's go ahead and I've been answering a lot of questions as we go.
00:33:49 [W1] I'm going to just kind of buzzed through the last bit that we have on The View here.
00:33:54 [W1] Let's see.
00:33:57 [W1] Be scanning. I did mention that as part of the harbor project.
00:33:59 [W1] They can integrate with some vulnerability scanning tools.
00:34:02 [W1] So my recommendation for dealing with secure images is to use something like that.
00:34:08 [W1] And also I highly highly recommend everybody when they deploy these kind of images pin to a version and pin do a hash if you can that's really my best advice for dealing with secure images.
00:34:21 [W1] Next is cloudevents and a question about does cloudevents work with Kafka and does it have some sort of knative integration as far as I'm aware cloudevents will work with any messaging system.
00:34:31 [W1] It's really a standard around the structure and the format of the messages rather than dealing with exactly how those messages are transmitted between systems.
00:34:39 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:34:45 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or
00:34:51 [W1] Nats no problem
00:34:55 [W1] question about would fluent DB ideal for scraping DOT log files and shipped to Greylock.
00:35:00 [W1] Absolutely.
00:35:01 [W1] That's I mean fluentd really at its core is a text processing system.
00:35:05 [W1] But if you go to their site, they really push and they really show that it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off to external systems things like great log, so I would recommend fluentd for that
00:35:20 [W1] Then dealing with exactly how those messages are transmitted between systems.
00:35:16 [W1] So cloudevents is sort of but instead of instead of having to build your own event structure.
00:35:22 [W1] You would find a cloudevents get some tools and get some interoperability and it would work with something like Kafka or Nats. No problem.
00:35:32 [W1] Question about would fluent DB ideal for scraping DOT log files and ship to Greylock.
00:35:38 [W1] Absolutely.
00:35:38 [W1] That's I mean fluentd really at its core is a text processing system. But if you go to their site, they really push and they really show that it's it's primarily built for logs and fluently is ideal for scraping lots and lots of log files and shipping them off
00:36:47 [W1] Is there a Secret store / Vault? I'm not quite sure the context are so kubernative.
00:36:52 [W1] I'll just kind of answer. We had another question about Secrets earlier Lancer kind of my given stock answer for secret. So kubenetes has built-in support for secrets and those have been encrypted for quite a while and a lot of in a lot of the most recent communities versions.
00:37:08 [W1] I also really highly recommend people when they're using things like Argo or other stateful deployments of or gitops for their community's resources Secrets kind of become a problem for a lot of people.
00:37:18 [W1] I really recommend looking into things like the bank vaults operator or there are operators built for most of the cloud secret management solution.
00:37:29 [W1] So if you're running in a public Cloud, there's usually a secret management solution provided for you Amazon has a secret manager gcp as a Secrets manager.
00:37:37 [W1] And most of them have some sort of operator where you can put your secret securely into that system and then set it up so you can create a Kuma knative resource that references those secrets and pulls them down as needed.
00:37:49 [W1] That's a really complicated.
00:37:51 [W1] I actually have an entire other talk about that that people can hit me up for about how we built a secret management system built on the cloud for kubernative. We've but in general I really recommend those kinds of operator based secret Solutions
00:38:06 [W1] Thanks for everybody.
00:38:07 [W1] There's a lot of really great feedback here that just you guys loved it.
00:38:10 [W1] So I'm really really happy to hear that.
00:38:11 [W1] Let me see if I can get down to some more questions.
00:38:16 [W1] Kafka versus Nats Odette, this is a Bugaboo for me.
00:38:19 [W1] I really really am constantly surprised at the usability and scalability of nats.
00:38:27 [W1] If you're looking in this, you know where cncf so spoiler alert is I'm going to kind of lean towards cncf projects here, but I really recommend looking into Nats before Kafka from most people most of the time.
00:38:35 [W1] It's more modern.
00:38:37 [W1] It's faster. It's easier to deploy like it may not scale to the ultra giant scale that Kafka purports to go to but most of us don't need that like Mega scale.
00:38:47 [W1] So I really really recommend people look into a Nats you'd be surprised how easy it is to get up and get going with Nats and and
00:38:54 [W1] get value out of it very quickly.
00:38:58 [W1] Yeah, you can't see the sweatpants. Although you also can't see the dog. So that's unfortunate.
00:39:04 [W1] Let's see.
00:39:04 [W1] Yep going through bunch of my talks. So
00:39:12 [W1] I selected this list.
00:39:14 [W1] Literally my proposal for the presentation was I want to cover all of the cncf graduated incubating projects and and introduce everybody to them so that they could go forth in the conference and learn more and so that was my proposal to the the conference was I'm going to do this
00:39:29 [W1] Between my proposal and when I had to actually make the presentation 12 or projects or added so I made a proposal and I end up sticking with it, but it was literally just wanted everybody to have heard of all these projects these top two tiers of the foundation and I
00:39:44 [W1] two weeks creating the presentation demoing different tools getting to learn all of them ins and outs I could so I could teach you guys, you know, the right things sto 1:7 and linkerd E2 which one is better and why that is a whole
00:39:50 [W1] Sto 1:7 and linkerd E2 which one is better and why that is a whole series of talks and probably a whole track on its own. I would say personal experience linkerd e is exceptionally easy to get up and going quickly.
00:40:00 [W1] And so it's a great way to get into a servicemeshcon cept without a lot of extra infrastructure.
00:40:06 [W1] I won't say better or worse because sto and Envoy have a really really broad feature set and are really successful for a lot of companies. So short version check out linkerd.
00:40:17 [W1] D2 trying to get up quickly and then maybe you'll move on DSD. Oh, maybe you won't depending on what you actually want out of your servicemeshcon.
00:40:47 [W1] Somebody asked where can we get the presentation? If you want the source code for this presentation all of the artwork and and everything. I used to make the talk. You can find that on my GitHub at carcinoid.
00:40:58 [W1] So if you just go to GitHub and search for carcinoid, they'll be a Toc pinned at the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:10 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:41:16 [W1] Repeating his questions.
00:41:17 [W1] So somebody asked where can I get a recording of the talk?
00:41:19 [W1] You'll be able to find that on this conference site after the day is done.
00:41:25 [W1] I think every day at the end of the day they're going to release the recordings.
00:41:28 [W1] My presentation itself is on my GitHub.
00:41:33 [W1] So somebody asked where can we get the presentation if you want the source code for this presentation all of the artwork and and everything I use to make the talk you can find that on my GitHub at carcinoid. So if you just go to GitHub and search for carcinoid
00:41:46 [W1] said they'll be a talk pin to the top that is for this presentation. So if you want to find the source code, that's where you'll find it.
00:41:56 [W1] And somebody asked for my Twitter handle was so that is Carson underscore Ops if you're looking to find me on Twitter.
00:42:05 [W1] So we also asked about my session on operators. I would I did have a talk last year about operators at keep con. If you want to find out more about that.
00:42:12 [W1] You can go find it on my GitHub as well.
00:42:14 [W1] So I'll try to put these actually now that I think about I'll put these in my speaker bio once this is all done.
00:42:19 [W1] I'll go back and put some of my past years talks and that kind of stuff in my speaker bio and links to those. So also check later today and you should have links to most of those things in my bio.
00:42:31 [W1] Another question again downloading presentation will be on this site at the end of the day.
00:42:36 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:42:41 [W1] What would I use for postgres?
00:42:42 [W1] This question actually came up twice.
00:42:43 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:42:49 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:42:55 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:43:07 [W1] A better way to handle postgres inside of kubernative.
00:43:10 [W1] So the I would definitely refer here anybody to postgres operator, even though it's not quite vitess, but for postgres
00:43:20 [W1] What is the go to Kate's UI?
00:43:22 [W1] That is a really good question.
00:43:24 [W1] I think it depends on your use case.
00:43:29 [W1] Another question I can downloading presentation will be on this site at the end of the day.
00:43:34 [W1] Question, you mentioned vitess for horizontal scaling of MySQL compatible engines.
00:43:38 [W1] What would I use for postgres? This question actually came up twice.
00:43:41 [W1] I'm not aware of a vitess style solution for postgres and kubenetes yet.
00:43:47 [W1] So vitess is really focused on integrating with the my SQL engine and providing a front for the MySQL engine.
00:43:53 [W1] I think right now there is a at least one postgres operator out there. So you won't get the same level of features as you would from something like vitess, but you would get at least
00:44:04 [W1] A better way to handle postgres inside of kubernative.
00:44:08 [W1] So the I would definitely refer anybody to postgres operator, even though it's not quite vitess, but for postgres
00:44:18 [W1] What is the go to Kate's UI?
00:44:20 [W1] That is a really good question.
00:44:22 [W1] I think it depends on your use case.
00:44:24 [W1] I've said it before I'm a contributor and a minor contributor and a big fan of Argo CD. So I actually really like to use Argo and and have people use Argo as a minimalist kind of Katie why it's not a full do everything you
00:44:39 [W1] Like the direct Cloud offerings for you eyes are also pretty useful.
00:44:44 [W1] Looks like they're making me close up on the session.
00:44:46 [W1] So I think what they want me to do is continue answering questions.
00:44:51 [W1] On slack. So please any of these questions I didn't get to if you have more questions for me.
00:44:55 [W1] I'm on the cloud knative computing slack for Kube Khan. So go ahead and hit me up directly and PM me and I'll be happy to answer any questions that you have Beyond this.
00:45:05 [W1] So I think that's it. I don't know when they're going to cut me off or if they want me to just stop talking but
00:45:14 [W1] Wait to see an answer.
00:45:24 [W1] Presentations I'll just keep talking till they kick me off.
00:45:26 [W1] So the question there's another question here is dragonfly like a torrent protocol sort of it's sort of like it's peer-to-peer transmission. So a lot like torque protocol, but like I said, it's primarily focused on dealing
00:45:41 [W1] The meaning of Creations that has is dealing with downloading images from other nodes.
00:45:44 [W1] So, okay.
00:45:47 [W1] Well, that's it.
00:45:48 [W1] Like I said, please hit me up if you have any more questions and thank you so much for watching.
