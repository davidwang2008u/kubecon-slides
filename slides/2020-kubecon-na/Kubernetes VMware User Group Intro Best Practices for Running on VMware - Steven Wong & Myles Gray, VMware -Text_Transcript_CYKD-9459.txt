Kubernetes VMware User Group Intro: Best Practices for Running on VMware: CYKD-9459 - events@cncf.io - Thursday, November 19, 2020 4:52 PM - 36 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, this is Sharon is presented by the kubernetes VMware User Group.
00:00:04 [W] This group is about running all forms of kubernative.
00:00:07 [W] Zon. The VMware infrastructure platforms from a user's perspective.
00:00:10 [W] I'm Steve Wang co-chair of the user group.
00:00:25 [W] Hi, this is Sharon is presented by the kubernetes VMware User Group.
00:00:29 [W] This group is about running all forms of kubernative.
00:00:32 [W] Zon. The VMware infrastructure platforms from a user's perspective.
00:00:36 [W] I'm Steve Wang co-chair of the user group.
00:00:40 [W] I work on kubernative and a few other open source projects as an employee of VMware today.
00:00:45 [W] I'm joined by Miles the other co-chair miles.
00:00:49 [W] I'll let you introduce yourself and take over.
00:00:51 [W] Okay, thanks for that Steve. So.
00:00:54 [W] When we get into it here, we're going to have a look through the agenda.
00:00:58 [W] So first thing that we're going to do is give an overview of the vsphere cloud provider and the related stored blood moons.
00:01:05 [W] That is the vsphere cloud provider out of tree and the CSI driver that we launched about a year and a half ago now, so we're going to cover any recent changes in features in that as well as the current
00:01:20 [W] Of the vsphere cloud provider and the related stored plugins.
00:01:24 [W] that is the vsphere client provider out of tree and the CSI driver that we launched about a year and a half ago now, so we're going to cover any recent changes in features in that as well as the current
00:01:49 [W] CSI driver and how we're planning on doing a migration from the old entry vcp to the new CSI driver that we have then I'm going to hand it over to Steve and Steve is going to talk about the new kubernative features that are built into our desktop
00:02:04 [W] VMware and then he's going to close up with how you can get involved with the VMware user group meet other users of VMware technologies that are trying to run containers and kubernative to share experiences and advice with each other.
00:02:17 [W] So let's start off with the vsphere client provider and what it does. So the vsphere cloud provider is what makes kubernative Cloud native.
00:02:30 [W] It's the glue layer if you want to think about it that
00:02:34 [W] Way between kubernative and the infrastructure that means kubernative doesn't have to be opinionated or doesn't have to have built in logic about how to interact with the underlying platform.
00:02:43 [W] It just knows how to do that via these cloud provider plugins.
00:02:46 [W] So there were two different models to begin with there was the entry Cloud providers and what that means is the core code for adding support for a new Cloud was added to the kubernative code base itself. It wasn't really a plug-in. It was
00:03:01 [W] Provide our plugins.
00:03:01 [W] So there were two different models to begin with there was the entry Cloud providers and what that means is the core code for adding support for a new Cloud was added to the kubernative code base itself. It wasn't really a plug-in.
00:03:14 [W] It was put a plug-in from the API perspective, but it was directly in the core code.
00:03:19 [W] So that meant if you install kubernative hu had that cloud provider plug-in just sort of installed by default because it was just part of the core code that led to a lot of
00:03:28 [W] Is particularly around code bloat in the core kubernetes repo secure Devon security vulnerabilities.
00:03:36 [W] that weren't getting patched if there was a bug for example in one of those cloud provider plugins, you would have to upgrade your entire kubernative distribution in order to patch that you couldn't just patch the plug-in because it's part of the core code.
00:03:47 [W] So there was a lot of drawbacks to actually embedding them in kubernative.
00:03:50 [W] So Community came together and decided we're going to move all this stuff out of tree into discrete plugins that you have to explicitly install so
00:03:58 [W] built the vsphere client provider, which is the out of tree version of that and it supports stuff like how telling kubernative how to interact with the vsphere infrastructure underneath how to tag particular VMS how to
00:04:13 [W] D. So Community came together and decided we're going to move all this stuff out of tree into discrete plugins that you have to explicitly install.
00:04:21 [W] So we built the vsphere client provider, which is the out of tree version of that and it supports stuff like how telling kubernative how to interact with the vsphere infrastructure underneath how to tag particular vm's how
00:04:49 [W] What AZ a given host is in vsphere how that correlates to kubernative.
00:04:55 [W] He's and it also provides a lot of the infrastructure that we use in the CSI driver around logic of how the plug-in can talk to the Centre itself.
00:05:04 [W] There are a bunch of Primitives that are supported by the kubernative cloud provider spec that we don't support out of the box with the vsphere cloud provider. So things like load balancers roots and that kind of thing because we have a separate cni-genie. Does that as well?
00:05:19 [W] As well.
00:05:20 [W] So if you want more details go and have a look at the cloud provider vsphere Sig sight there on the slide.
00:05:27 [W] So I'm going to talk to you now about storage on vsphere as it pertains to kubernative.
00:05:33 [W] He's so we're going to look at the VC P, which is the old entry way of doing things.
00:05:36 [W] We talked about the CPI just now and by the way CPI is what we're calling the cloud provider interface or the vsphere cloudbees vital and the CSI is the cloud storage container storage interface
00:05:52 [W] Eddie's so we're going to look at the VC P, which is the old entry way of doing things.
00:05:55 [W] We talked about the CPI just now and by the way CPI is what we're calling the cloud provider interface or the vsphere cloudbees Vidor and the CSI is the cloud storage container storage interface
00:06:21 [W] Is what we call our new out of tree version of the storage Drive Earth.
00:06:25 [W] So the vcp to give you sort of an architectural look at things was what I was just talking about the thing that was built into kubernative natively. So you can see that here. It's just a core part of kubernative.
00:06:37 [W] you install kubernative you get the vcp and it enabled Dynamic provisioning of volumes and kubernative which is pretty much a must have whenever you're trying to deploy State flaps on top of Kate's you need something that does dynamic provisioning of storage and it did
00:06:52 [W] Dynamic provisioning of volumes and kubernative which is pretty much a must have whenever you're trying to deploy State flaps on top of kids. You need something that does dynamic provisioning of storage and it did data service granularity at an SP be and policy level.
00:07:06 [W] So if you created a storage class within kubenetes that one-to-one maps to a storage policy in vcenter so storage polysaccharides of vsphere concept, which is essentially identical actually to a storage class.
00:07:19 [W] So it's very convenient. We just mapped one to the other.
00:07:21 [W] Oh, but we talked about the drawbacks before if there was a bug in the VC P or if there was a problem with it, you had to upgrade your entire kubernative distribution.
00:07:30 [W] So say you're on 1.11, you know, because the vcp is quite old now if you are on 1.11 there was a bug in the vcp say we fixed it in 1.12 and push that into the Upstream repo.
00:07:42 [W] You would have to move your entire kubernative distribution from 1.11 to 1.12 home major step. So rather than just being able to update.
00:07:52 [W] The vcp itself you had to update everything. So there were some drawbacks there.
00:07:56 [W] So with vsphere 6.7 you three, we introduced two things.
00:08:03 [W] So we introduced the CNS control plane inside of these fear and we introduced a CSI driver as well.
00:08:09 [W] So the CSI driver took the logic that was in the VC p and made it out of tree.
00:08:13 [W] That means that you can rev the CSI driver without having to upgrade your kubernative. So again, if there's a new feature or there's a bug or
00:08:22 [W] Fixed or whatever in a new version of CSI driver. You just update CSI.
00:08:27 [W] You don't have to update your k3s and you can see that here.
00:08:29 [W] That runs inside your cage cluster and it uses this new CMS control plane that we built into vsphere.
00:08:35 [W] So the old way of doing things with the PCP.
00:08:39 [W] It was kind of hacky. If you look at how it actually provision volumes. Basically what it did was if you requested a hundred big volume, for example, it would create a VM with no resources attached to it, but it would also attach a
00:08:54 [W] actually, if you look at how it actually provision volumes, basically what it did was if you requested a hundred big volume, for example, it would create a VM with no resources attached to it, but it would also attach a
00:09:23 [W] UK of a hundred gigs in size and the reason it did that was because there was no primitive to just deploy a disk. So it would deploy a VM with a disk attached on my the disk throw away the throw away the VM and then remount the
00:09:38 [W] Unmount the desk throw away the VIP throw away the VM and then remake the disk into wherever it was needed.
00:09:44 [W] Obviously, that's a lot of overhead that really didn't need to happen.
00:09:47 [W] So we fixed all of that stuff with CNS and CSI.
00:09:50 [W] So whenever we actually made this a product in 6.7 you three the new CNS control plane uses a back-end storage class or a back-end storage provisioning tape in vcenter called first-class tisks.
00:10:05 [W] You're going to have a Google for first-class discs. There's a good boy.
00:10:07 [W] Lock By Conrad Kuma Cogan on that and that's essentially like a global catalog for disk. So then V Center so it allowed this individual creation of disks without having to have a VM that sort of managed or
00:10:23 [W] So we introduced that and that God already got rid of a lot of our technical debt that we had in the BCP.
00:10:29 [W] It's essentially a grind up rewrite of the vcp from a storage perspective and it did just the same stuff.
00:10:36 [W] It allowed you to do Dynamic provisioning of kubernative volumes again, very important. Whenever you're deploying staple workloads on top of cakes, but it also added some really nice UI elements into V Center as well for cloud native storage.
00:10:49 [W] So there's a whole new UI in vsphere 4 coordinator.
00:10:52 [W] storage so any of your kubernative clusters and any of your applications that are deployed on top of each sphere on top of kubernative because they are provision via CNN s we expose all that information straight into the UI so you can see exactly, you know an
00:11:08 [W] You can see that there is a mongodb primary node mounting this particular volume and all that information is directly accessible to your vsphere administrators.
00:11:15 [W] So they get all of that rich context from the application layer dynatrace infrastructure layer to more effectively help you troubleshoot things.
00:11:23 [W] Like for example, we use the same PVC names as you have in kubernative to identify the volumes and vsphere just anything that we could do to lower the boundary between the two teams so that they speak the same sort of
00:11:34 [W] language
00:11:36 [W] then you know that was 673.
00:11:40 [W] We've had two big releases since then we had 7.0 and 7.0 you one and we added a whole bunch of stuff to see nsmcon slide for that.
00:11:47 [W] The first one was we offer a file-based persistent volumes. Now if you're using V Sanders your back-end storage, so if you need rewrite many volume types for kubernative, you can do that now if your visa and storage so it will automatically provision NFS shares might them into the appropriate
00:12:02 [W] We added support for all types of these fear storage block.
00:12:05 [W] So if you only need right ones you can I use that with feasts and V volts vmfs and NFS storage.
00:12:12 [W] So any stores type supported by vsphere and additionally we added a few other bits and pieces. So we have offline volume resize support their support for snapshotting discs via Valero with RV ADP plug-in so we can do
00:12:27 [W] Mike snapshots at the infrastructure layer of volumes that are being backed up by Valero and purposes in volume encryption as well.
00:12:24 [W] So you can assign a storage policy that has encryption tied to it and it will encrypt every disc by default for you.
00:12:30 [W] We also added support for exporting metrics from esxi and Vis and into Prometheus.
00:12:37 [W] So if you're using Prometheus for monitoring, you can pull a metrics or scrape the metrics from the esxi hosts directly into Prometheus do auto scaling or whatever it is.
00:12:45 [W] Is you want to do on it there or maybe just build fancy Griffin a dashboards.
00:12:51 [W] So I think we all sort of know how this works, but we're going to go through it here.
00:12:56 [W] This is the kind of the important bit whenever it comes to CNS and CSI as to how this makes volume provisioning work automatically.
00:13:06 [W] So everything in Orange here is vsphere everything in blue is kubernative.
00:13:10 [W] He's so the first thing you or your administrator creates a storage policy within vcenter that defines things like raid level quality of service all that kind of stuff and
00:13:21 [W] And kubernative.
00:13:22 [W] He's you create a storage class that points to that spbm policy.
00:13:26 [W] It's just a name mapping really really simple. And then you've got CSI that actually ties the two things together. So you come along you deploy your Cassandra app, it requests a volume. So it asks for that from a particular storage class storage class looks at the provider and says this
00:13:41 [W] I hand it to our CSI driver.
00:13:39 [W] It will then look up the vcenter connection information.
00:13:42 [W] So it connects to the correct vcenter CNS instance CNS looks at the request as okay.
00:13:48 [W] They've asked for a hundred big volume with a raid 1 policy applied to it hands that off to the system creates two copies of it right one and then passes that back up into the appropriate worker node gives it a file system and mindset back into the Container so
00:14:03 [W] Creates two copies of it right one and then passes that back up into the appropriate worker node gives it a file system and makes it back into the Container so fully automated end to end once you have created your spbm policy and storage class, which you only need
00:14:02 [W] an end to end once you have created your spbm policy and storage class which you only need to do once
00:14:10 [W] Now the thing that most of you are probably wondering about is if I have this old vcp driver, how do I migrated to the new CSI driver?
00:14:21 [W] That's very good question.
00:14:22 [W] So this is something that we've been actively working on in the back end.
00:14:24 [W] It's not fully baked yet. But we do have Beta Support for this as of Kate's 119 and vsphere 7.0 you one. So if you're on an infrastructure level of vsphere 7 OU 7.0 you won or above, you know, whenever you're watching this.
00:14:40 [W] And you have kubernative 1.19. You can enable migration from the vcp to CSI.
00:14:47 [W] There is some documentation up on the GitHub site.
00:14:50 [W] So if you want more info on how this actually works or how to do it go have a look there. But essentially what it does is whenever you enable this at the cubelet level on each of your kubernative nodes
00:15:05 [W] A web hook for any calls that would go to the vcp and redirects them to the CSI driver.
00:15:11 [W] So the CSI driver will then take those calls and just translate them into CNS calls.
00:15:16 [W] So it'll do that for any newly created volumes in addition.
00:15:21 [W] You can add a flag or an annotation to your PVCs and that will say migrate these volumes, you know or this particular volume in the background then because like I said, we added this new first class this concept FCD
00:15:35 [W] Except we need to do a disk migration at the vsphere lower layer.
00:15:38 [W] Whenever you add that annotation, it will then go and on schedule the pod that's attached to that move the vmdk from just being a standard vmdk to being a first-class disk, and then it will mount it back
00:15:53 [W] Spent it back up again.
00:15:49 [W] So it does the back end data migration for you as well.
00:15:54 [W] Like I say this will also require a new CSI driver as well because the currency aside driver that that exists out there doesn't have this baked in fully supported. But if you want to try it in beta it's in a patch
00:16:09 [W] So the projected cutoff date for the entry cloud provider is supposed to be kubernative 1.21, you know, it's been said to be 1.16.
00:16:12 [W] It was said to be 1.18 the goalposts keep moving, but at this point, you know, we have a built migration path that is in beta.
00:16:22 [W] So I would be fairly confident that 1.21 is a good prediction for the vcp driver to be
00:16:31 [W] From the core kubernetes code base. So I would imagine that that's sort of our cutoff date there. The I'd of tree driver. The CSI driver is already recommended for all new installations.
00:16:41 [W] There is no reason that you should be using the vcp other than it's the only thing that your distribution supports, but you should be able to install custom CSI drivers into whatever distribution of kubernative you're using.
00:16:56 [W] And use the CSI driver from the get-go will just make your life a lot easier older vsphere versions the BCP supported from 6.5 onward because of this new core code change that we had to do at the
00:17:11 [W] That support will be removed and you'll have to be on 6.7 you 3 or above to use this ESI driver?
00:17:09 [W] So that's sort of that covered. I am going to have a little look now at any of the recent changes what they were there wasn't a whole lot since the last time we did this at Q Khan Europe.
00:17:24 [W] So there's only just to bug fixes listed here.
00:17:24 [W] We have one where we fix the bug when discovering the VM diapey address by subnet or the network name. So that's been fixed and fixed a bug where the node may be prematurely deleted if
00:17:39 [W] Slow to respond to report its host name.
00:17:38 [W] So that's been fixed as well aside from that not a whole lot because we are still developing some of the big major functionality and going to push that up later in the year.
00:17:48 [W] So at this point I'm going to hand it back to Steve.
00:17:54 [W] Thanks miles.
00:17:55 [W] I'm going to move on from the vsphere hypervisor to using desktop hypervisors to host kubernative clusters. Why run kubernative on your laptop?
00:18:06 [W] Well, yes for production workloads hosting in a public cloud or an on-premises data center is recommended but a local Dev cluster is useful in some circumstances when you want to learning environment and you already have
00:18:21 [W] And kubernative on your laptop.
00:18:20 [W] Well, yes for production workloads hosting in a public cloud or an on-premises data center is recommended but a local Dev cluster is useful in some circumstances when you want to learning environment and you
00:18:47 [W] It may be cheaper than paying for cloud hosting when your internet connection is limited.
00:18:53 [W] It might be your only option when you want rapid turns as in early stages of development work flows a local Dev cluster can be useful, but you need to be careful about how this fits with your ci/cd.
00:19:08 [W] Lee
00:19:11 [W] These are the options for kubernative Xin a desktop hypervisor.
00:19:15 [W] first kind second minikube and third running a light weight distribution could be k3s.
00:19:22 [W] Micro Kate's Upstream Cube at mm or something else inside a VM a desktop hypervisor, whether it's workstation Fusion virtual box or something else can support all of these variants and
00:19:38 [W] inside a VM I desktop hypervisor, whether it's workstation Fusion virtual box or something else can support all of these variants and if you're running Linux or a doctor platform, maybe you
00:19:52 [W] X or a Docker platform, maybe you can avoid using a general-purpose hypervisor all together.
00:20:00 [W] Kind stands for kubernative Xin Docker and like the name implies.
00:20:05 [W] It runs the various kubernative subsystems inside docker.
00:20:09 [W] It is lighter weight than the other options at the expense of being less like the Clusters that you would deploy in a cloud kinds lighter weight tends to lead to a life cycle of very short-lived clusters.
00:20:25 [W] To tear them down and then rebuild them as you need them and these might be more reliable for a local Dev cluster use case.
00:20:33 [W] You aren't tempted to postpone patches as much as you are with some of the other variants and maybe you can afford to test against multiple versions of kubernative today.
00:20:44 [W] I'm going to demo kind in a workstation hypervisor, but see the cube Con Europe recording which is linked.
00:20:53 [W] Here for instructions on running minikube in a desktop hypervisor as an alternative.
00:21:01 [W] I'm intentionally the do doing this demo on Windows for two reasons first.
00:21:07 [W] It's a little less straightforward.
00:21:09 [W] And if you have a Windows laptop, I think you might be a little bit more in need of an assist from what you might learn from a demo the same thing will work on Linux.
00:21:24 [W] Our Mac although a few of the install steps will vary a bit.
00:21:29 [W] They're close enough that I'm pretty confident. You'd figure it out.
00:21:35 [W] about a month ago as I'm recording this VMware released a new version of the workstation infusion hypervisors for Windows Linux and Mac the key new feature that was added for a kubernative audience is built-in support
00:21:50 [W] Containers and kubernative clusters with a new CLI called V cuddle.
00:21:51 [W] Now, you've always been able to run kubernative and Linux VMS and minikube has been supported for years, but now you can directly deploy a kind cluster using the CLI, and I'm about to show you how this works on Windows.
00:22:07 [W] I've pre-staged some of the necessary steps because you've seen installs before but obviously you start by downloading the installer binary for the desktop hypervisor and installing it the cube cuddle CLI is not
00:22:22 [W] The hypervisor so you need to install LED independently and this is just a standard kubernative component. That's well documented in the kubernative stocks. If you're on Windows and a kubernative newbie, I'd recommend
00:22:18 [W] In chocolaty as an easy way to do it with a one-line cut and paste to an admin command line and you can see the that command line right here.
00:22:23 [W] The other thing I've pre-staged is the installation of a tekton CLI.
00:22:29 [W] CLI. And the reason I've done this is because I'm going to host a tekton ci/cd platform on the kubernative cluster that I'm about to deploy so
00:22:41 [W] I'm going to cut to a demo screen if you want to try this at home later in the published DEC.
00:22:47 [W] I'm including slides the detail all of these steps with the exact entry that you need to type in on your command line.
00:22:56 [W] So download the deck and you can cut and paste commands from it. Later.
00:23:02 [W] So the new container and kind feature is based on technology similar to what went into VCR 7 for hosting containers within a rapidly stage of all VM kind is a binary produced by the kubernetes project for deploying kubernative
00:23:17 [W] Send to a doctor runtime with workstation 16. What you've got is a version of kind that thinks it's talking to a standard container runtime interface like the one used by doctor, but it is hooked up to the desktop.
00:23:30 [W] So the desktop hypervisors start with a CLI be cuddle that is intentionally similar to the docker CLI.
00:23:26 [W] You're already familiar with let you should let me show you this by typing in V cuddle with no parameters, you get usage guidance and what you see is the familiar command options of buildpacks sh--
00:23:41 [W] Which I'm sure you're familiar with from Docker.
00:23:41 [W] Let me prove it by doing a Docker or a v cuddle pole of the classic hello world container.
00:23:52 [W] I know what you're saying.
00:23:53 [W] Wow. Steve messed up the demo already but trust me I did this on purpose the error tells you what to do to fix the issue. The container engine needs to be started. This is intentional so that you when you don't need the container
00:24:07 [W] Runtime it can be completely shut down something you might appreciate when you need the most out of your laptops memory CPU or battery.
00:24:18 [W] So now I'll type in that V cuttle system start that the error message suggested.
00:24:33 [W] And after a little while we're going to see that a container runtime is available. We can type in V cuddle system.
00:24:44 [W] info
00:24:48 [W] and this shows what is available to The Container runtime if it's needed this is configurable and you can see the output explains what the current settings are and where the config file and the log lives now
00:25:03 [W] That a container runtime is available. We can type in the cuddle system.
00:25:06 [W] info
00:25:10 [W] and this shows what is available to The Container runtime if it's needed this is configurable and you can see the output explains what the current settings are and where the config file and the log lives now
00:25:38 [W] Runtime is available.
00:25:40 [W] Let's try that Docker.
00:25:41 [W] Hello world exercise again.
00:25:43 [W] First.
00:25:44 [W] I'm going to pull the image.
00:25:53 [W] And then next I'll run it.
00:26:08 [W] What we want is full kubernative not just docker.
00:26:13 [W] Let's deploy a kind cluster. So the instruction for that is simply V cuddle kind.
00:26:21 [W] And you'll see a new window popped up and this window is set up to use kind.
00:26:29 [W] When it's going to make it a little bigger here and after maximizing this window, I'm going to do a kind create cluster.
00:26:42 [W] This is going to take a few minutes.
00:26:43 [W] Probably more than one minute in less than 5 the time depends on the speed of your system as well as the speed of your network connection in the interest of time on this recorded demo.
00:26:56 [W] demo. I'm going to cut the elapsed time down but expected to take between one and five minutes.
00:27:06 [W] But let's prove it by trying a few standard kind Docker and kubernative commands first. We'll try be cuddle images the equivalent of Docker images.
00:27:19 [W] You can see that we have that helloworld image we pulled as well as a new one in the list associated with kind next.
00:27:27 [W] we'll try Cube cuddle version and will see the version of the kubernative cluster as well as the cube cuddle client yellowest in the last decimal point.
00:27:40 [W] They're a little off but they're both version 1.18.
00:27:42 [W] So that's fine. Next.
00:27:46 [W] Let's do Cube cuddle.
00:27:50 [W] To look at what's lurking in the system namespace?
00:28:00 [W] and we see a lot of kubernative components that are self hosted in the cluster we can also do a
00:28:10 [W] Cube cuddle cluster info to take a look at what we've got.
00:28:20 [W] And the kind binary will also show us what it thinks. It's deployed as far as clusters.
00:28:30 [W] Use cute cuddle get nodes.
00:28:25 [W] We can use be cuddle PS to look at learning Docker containers.
00:28:31 [W] And finally we'll do a use the CLI for the hypervisor to see what has happened with regard to running VMS.
00:28:41 [W] We've got one VM running.
00:28:42 [W] So at this point we have a standard kubernative cluster with standard functionality nice, but for that developer workflow I talked about it's a foundation but still not a complete tufin.
00:28:54 [W] Look it.
00:28:55 [W] What about ci/cd? Well, tekton is a popular open source framework for driving ci/cd by hosting steps and pipelines in a portable way that takes advantage of kubernative the
00:29:10 [W] That you can build test and deploy anywhere and your ci/cd pipeline can't tell the difference between when it's run on my laptop or when it's running a public cloud.
00:29:12 [W] Tell the difference between when it's run on my laptop or when it's running a public Cloud the result should be the same anywhere with no snowflake build steps going on. We can use our kind cluster to host the
00:29:16 [W] Use our kind cluster to host the additional tools. We need for a tekton framework.
00:29:23 [W] This isn't going to be a talk on tekton.
00:29:26 [W] So I'm just going to get started but let me at least show you how you can use a desktop hypervisor as part of your ci/cd process.
00:29:36 [W] We'll start by installing tekton into our kubernative cluster with Cube cuddle.
00:29:41 [W] I'm going to download and apply the ammo from the tekton open source project.
00:29:50 [W] so there we have q cuddle applying the a mole downloaded from the tekton project and you can see a bunch of stuff is going on take a look Q cuddle get podzol namespaces and I'm going to predict we're going to see the
00:30:06 [W] so there we have q cuddle applying the yeah mole downloaded from the tekton project and you can see a bunch of stuff is going on take a look Q cuddle get podzol namespaces and I'm going to predict we're going to see the
00:30:30 [W] Couple of tekton PODS they're not in the ready state yet, but they've been deployed.
00:30:38 [W] So what wait for those to start up?
00:30:54 [W] So now we've got tekton running tekton run ci/cd steps as something it calls tasks.
00:31:02 [W] It's a framework for running a ci/cd process on your choice of kubernative rather than run a full build test deploy cycle in the interest of time.
00:31:15 [W] I'm going to run a tekton hello world example that pretends that emitting hello world is a step that is
00:31:24 [W] As part of your build process this comes from a blog post from Brian McLane that I'm going to link in the deck.
00:31:31 [W] So let me deploy from the yamo - oh mother I ins GitHub and we've deployed a tekton step that emulates a
00:31:47 [W] I'm going to use the tekton CLI describe command to examine that build step.
00:31:56 [W] And you can see that it's started and status of running.
00:32:00 [W] So I'm going to try again to look at the state of that tekton step.
00:32:09 [W] And you can see now that it had a duration of two minutes and now has a status of succeeded and see what's in the logs.
00:32:21 [W] And you can see that log show that hello world was echoed.
00:32:27 [W] So this content is brought to you by a kubernative user group. And if you're using kubernative Zon vsphere, I invite you to formally joined the group. We have meetings each month where we present tutorials and best practices.
00:32:42 [W] And you can see that log show that hello world was echoed.
00:32:29 [W] So this content is brought to you by a kubernative user group.
00:32:33 [W] And if you're using kubenetes on vsphere I invite you to formally join the group.
00:32:40 [W] We have meetings each month where we present tutorials and best practices.
00:32:44 [W] The agenda is user group. And so members are encouraged to nominate presentation subjects and discussion topics including discussions of feature requests. We have to user tech leads who
00:32:58 [W] Here today Bryson Shepherd and Djoser see who helped us get this group started and we're looking to grow this group into a diverse set of worldwide users.
00:33:08 [W] The group is also running a slack Channel, which is a great place to ask questions. The kubernative user Channel might be a better place for generic kubernative questions, but if you have a vsphere specific
00:33:23 [W] All might be a better place for generic kubernative questions. But if you have a vsphere specific topic to discuss the VMware users channel is a great place to find other users and also to reach
00:33:38 [W] imitation contributors
00:33:41 [W] So the next user group meeting will be December 3rd in the North American time zone.
00:33:47 [W] You can go to the kubernative community calendar to get a conversion to your local time time zone and add it to your calendar use the link in this slide.
00:33:58 [W] You become a group member by joining the mailing list using the link shown here. And then finally there's a link to the group's slack Channel.
00:34:08 [W] Miles and I are going to hang around for QA.
00:34:11 [W] Here's a link that will get you this deck and these are our GitHub IDs and you can also get in touch with us and the broader user Community inside from inside the group's slack Channel.
00:34:25 [W] Thank you, and I hope to see you in a future meeting at this point.
00:34:28 [W] I'm going to turn it back to the cncf conference moderators.
00:35:47 [W] Hey, there, we are.
00:35:48 [W] Hey everyone.
00:35:50 [W] We just want to say thanks again for attending our session.
00:35:53 [W] We're here to answer questions and like Steve said will be on slack as well on the maintainer channels to answer your questions there too.
00:36:04 [W] think we got to everything through the Q&A section. Yeah that channel is to - cute Khan - maintainer of the cncf slack and once again, the links are in the deck a lot of those.
00:36:17 [W] Six slides have active links to go find the slack channels and whatever please join the group.
00:36:23 [W] All right.
00:36:23 [W] Thanks everyone for attending. See you around Slack.
