Persistent Memory in Kubernetes: APBC-4018 - events@cncf.io - Thursday, November 19, 2020 3:47 PM - 33 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Welcome to this presentation about persistent memory in kubenetes.
00:00:04 [W] My name is Patrick audio.
00:00:06 [W] I'm a software developer at Intel working on given it is and storage related Technologies.
00:00:12 [W] Storage Drive of it makes persistent memory available in kubenetes.
00:00:17 [W] Usually I give presentation in front of an actual audience.
00:00:20 [W] So now doing it in front of a computer.
00:00:22 [W] It's a bit lonely.
00:00:23 [W] Therefore I brought my faithful companion the rubber duck.
00:00:27 [W] She's very good at debugging and who knows perhaps she will even have questions about this presentation.
00:00:32 [W] So let's get started before we get to p.m. CSI. Let's talk about persistent memory in general.
00:00:42 [W] As the name implies persistent memory is storing persistently in contrast to D Ram where part data is lost when your power a power down the machine.
00:00:53 [W] It's also addressable like memory you can do reads and writes with your CPU and you immediately get the actual data in contrast to SSD is where you first need to load an entire sector into
00:01:08 [W] Anything with the data?
00:00:56 [W] In times of performance and capacity it also sits in the middle between actual dram and SSD is it has read performance.
00:01:07 [W] That's almost as good as dear. Am right performance is a little bit lower and it has capacity that is closer to actual storage touch higher and higher capacity than dram.
00:01:22 [W] It is an industry Consortium that defines how to use and how to attach persistent memory to a system.
00:01:30 [W] But it's also an actual product from Intel introduced in 2019.
00:01:35 [W] There has been a product called Intel obtain persistent memory.
00:01:39 [W] its dim that you put into the same slot as a dram and then can configure in different ways as we will see in the following slides 2020 has been a refresh.
00:01:52 [W] hands variance will have higher bandwidth key points to remember for this presentation is that you can build machines individual boxes in your server rack that have much higher capacity in terms of memory than
00:02:07 [W] Teams build with d Ram.
00:02:00 [W] It's possible to build machines with up to 6th six terabytes of p.m. In the to stock and system that makes it together with the by addressable and memory like characteristics and ideal solution
00:02:15 [W] Asians like in-memory databases or caching systems like memcache D or radius where traditionally you would have used the ram, but we're limited by the amount of DRM that you can put into an individual machine.
00:02:16 [W] So, how do you use Pima - system different ways of doing it different ways that are set up by the administrator.
00:02:24 [W] The first mode is called memory mode.
00:02:28 [W] That's the one that's completely transparent to the application and to be operating even to the operating system.
00:02:35 [W] It's configured in buyers in this mode.
00:02:38 [W] The total capacity of a system is determined by p.m.
00:02:42 [W] m. And dram is just acting as a cache. It's not
00:02:46 [W] Tressa put separately. You do lose some capacity that way but you can use it with Legacy systems that don't know about p.m.
00:02:56 [W] p.m. At all because the software itself doesn't need to know about it. It's all handled in Hardware.
00:03:03 [W] For me as a software developer. The more interesting mode is the so-called app direct mode here p.m.
00:03:11 [W] And DRM are exposed to the operating system separately. And then we operating system and applications get to decide what they do with that p.m. The operating system typically will make it
00:03:26 [W] So don't know about p.m.
00:03:26 [W] At all because the software itself doesn't need to know about it all handled in Hardware.
00:03:33 [W] For me as a software developer.
00:03:34 [W] The more interesting mode is the so-called Arab direct mode here p.m.
00:03:41 [W] And DRM are exposed to the operating system separately. And then we operating system and applications get to decide what they do with that p.m. The operating system typically will might make
00:04:18 [W] As a fire system, that's how persistency is handled via the usual file IO operations.
00:04:26 [W] But then applications cannot take a large file and memory map that into their address space and then that address space is by the dress ability we get for full advantage of p.m.
00:04:39 [W] m. And with special CPU instructions, we can ensure that individual cache lines are flushed persistently and available even after sudden power loss.
00:04:50 [W] So applications been can use vat characteristic to implement things like a warm cash or ask to to speed up restarting the application after rebooting the machine.
00:05:04 [W] One application for does that is memcache T traditionally? It has been used for in-memory database in memory caches.
00:05:13 [W] And that leads to a problem you need if you need to update memcachedb restarted, it will start up with an empty cache and performance will be worse while it still needs to populate that cash with data in p.m.
00:05:28 [W] Almost immediately has access to the same data that it had before restarting it.
00:05:32 [W] Finally, you can also do traditional file I/O over the file system that is mounted on an app Direct namespace on Linux X FS + X4 have specifically been enhanced to support
00:05:48 [W] They will do some operations such that data is stored directly in p.m.
00:05:41 [W] Completely bypassing the Linux kernel page cache so you get very high performance iot.
00:06:06 [W] So some resources that you may want to know about for Intel.
00:06:11 [W] There is a tool that actually configures the individual dims that's called IPM cattle then at one higher level when the independent. We do have ND cutter that
00:06:26 [W] With the things that are defined by the p.m. Standards so-called regions and namespaces.
00:06:31 [W] Regions can be assigned or created on a single dim or combine multiple teams in interleave mode and then regions get split up into individual name spaces for different purposes and different
00:06:46 [W] And these namespaces been get mounted by the Linux kernel and exposed as a deaf device where you mount your file system.
00:06:40 [W] For developers who want to write app direct enabled applications. Where is PMI? Oh a website that collects information and offers different tools that simplify the task of a software developer.
00:06:56 [W] Because persistency in particular depends on doing some things correctly like flushing data when needed can it be a bit hard to program. If you do it manually the persistent memory development kit helps with that by implementing higher level
00:07:11 [W] Persistency and particular depends on doing some things correctly like flushing data when needed can it be a bit hard to program. If you do it manually the persistent memory development kit helps with that by implementing higher level data structures.
00:07:32 [W] That van work efficiently on p.m.
00:07:35 [W] And for applications like memcached even need to store data intelligently verse development kind and mlr replacement that can give you data or memory charms either in Pima more DRM depending on what you want.
00:07:52 [W] Oh rubber duck is getting a bit impatient.
00:07:54 [W] She wants to know how all of that works in kubenetes. So let's get to that part.
00:08:00 [W] For memory mode, you don't really need to do anything on the software side.
00:08:04 [W] You just run kubenetes and you do have access to p.m.
00:08:08 [W] And DRM as a cache 4 p.m.
00:08:10 [W] But for app direct mode, you need some software that actually manages your storage your p.m. That's where p.m. CSI comes in.
00:08:21 [W] It is a container storage interface called CSI driver that manages your local storage.
00:08:30 [W] We have two different ways of creating volume Stone a mcclee one is working at the level of Endicott. Oh using a library that's coming from the same source code here
00:08:45 [W] Is a separate name space the advantage is that you in Fury can use the p.m.
00:08:54 [W] For something else in parallel, but this mode suffers from fragmentation issues because if you create
00:09:02 [W] Multiple namespaces you have one name space left in the middle of your region.
00:09:06 [W] Then the remaining two space in the front and after it can only be used separately. You can't create one large namespace that combines both parts.
00:09:16 [W] With lvm.
00:09:18 [W] that isn't a problem you we first allocate certain amount of memory and create a volume group. And then that warning group can be split up arbitrarily into logical volumes.
00:09:34 [W] Set aside p.m.
00:09:24 [W] Permanently for use by PMC as I which may be.
00:09:28 [W] Okay, so it's actually the recommended mode at the moment.
00:09:34 [W] then
00:09:36 [W] once the volume has been created PMC is I also make sure that once in the Pod wants to use it.
00:09:42 [W] There is a file system on it and it will support X4 + XF s mounted with a suitable options that are you able p.m.
00:09:51 [W] m. Support.
00:09:52 [W] Or we can also provide a volume as a raw block device if an application wants to do that part itself.
00:10:07 [W] First slide here gives an overview of a different pmc's I releases the exact timeline isn't very important.
00:10:14 [W] I'd rather use the opportunity to talk about the features that we've added over time.
00:10:20 [W] So since very beginning since the first public release in August 2019, we have had Docker images available on Docker Hub and together with jungle files in our repository.
00:10:34 [W] It was possible in is possible to deploy p.m. CSI.
00:10:38 [W] For and xfs mounted with a suitable options that are you able p.m.
00:10:36 [W] Support.
00:10:37 [W] Or we can also provide a volume as a raw block device if an application wants to do that part itself.
00:10:51 [W] This slide here gives an overview of the different pmc's I releases the exact timeline isn't very important. I'd rather use the opportunity to talk about the features that we've added over time.
00:11:05 [W] So since the very beginning since the first public release in August 2002 19 we have had Docker images available on Docker Hub and together with jungle files in our repository.
00:11:19 [W] It was possible in is possible to deploy p.m. CSI.
00:11:24 [W] And end of 2019 we added support for roadblock volumes one that I already mentioned perhaps more importantly. We also added support for FML inline volumes.
00:11:36 [W] So assume for a second that your application doesn't need persistency.
00:11:40 [W] You are just creating Parts using some higher level controller like a deployment in kubernative.
00:11:48 [W] And now you want to add p.m.
00:11:50 [W] To that now by specifying in line in the pot spec that you want storage.
00:11:56 [W] The communities controllers will automatically create and destroy that storage for you and violet report runs.
00:12:05 [W] It does have p.m.
00:12:06 [W] That's ideal for for local nonsense persistent scratch space. It's much easier to manage. No less less work that you need to do for persistent.
00:12:20 [W] Later on, we figure that jungle files perhaps on that ideal when it comes to updating a deployment operators are much more elegant way of doing the same thing.
00:12:30 [W] So we implemented that it is on operator hub.
00:12:35 [W] And once you have the pmm CSI operator installed in a cluster you can use that to create the actual p.m.
00:12:43 [W] m. CSI deployment the operator then we'll also take care of updating pmc's I went we do new releases.
00:12:51 [W] One problem that we ran into with local storage in particular is that Vicki? Burnett is scheduler doesn't really know where your storage is available.
00:14:05 [W] Our second video application doesn't need persistency.
00:14:08 [W] You are just creating Parts using some higher level controller like it deployment in kubernative.
00:14:16 [W] And now you want to add p.m.
00:14:18 [W] To that now by specifying in line in the pot spec that you want storage. The communities controllers will automatically create and destroy that storage for you. And while the pot runs it does
00:15:18 [W] You'll extensions when a pot needs to be scheduled kubenetes is asking good pmc's I wear which notes are suitable and pmc's a I will query its own database and figure out where pot may run and then
00:15:33 [W] Tries to stop the pot. It's not perfect.
00:15:37 [W] I'm working on something that hopefully will work more reliably, but for now, it's good enough.
00:15:44 [W] We also added support for Kata containers. For those of you who don't know cutter containers is a secure environment where applications run inside the VM that posed a challenge for mounting the p.m.
00:15:59 [W] File system.
00:16:00 [W] We've solved that such that you can all get full knative performance because we p.m.
00:16:06 [W] Volume really gets memory-mapped inside for the virtual machine. So with promises with same as if you were running without cutting containers,
00:16:15 [W] Finally just released this month.
00:16:18 [W] We decided to call the core features of Premium CSI production-ready. We've made sure that upgrades and downgrades work seamlessly. We are testing as always on all of these two poor currently supported kubenetes versions.
00:16:33 [W] And we've added also worsen skew testing as we will see on the next slide premium see is actually has different components and doing an upgrade.
00:16:43 [W] Some of them might be an old released. Some of it might be a new one and that all has been casted now and it works and for the administrators Among Us.
00:16:55 [W] So, how does PMC is I Really Work the biggest challenge that we encountered conceptual issue that we had was that kubenetes doesn't really know how to deal with local storage or not.
00:17:07 [W] We'll all of it that I make provisioning of volumes assumes that you have a central component that interacts with a control plane that works with API server when you see when you create a persistent volume claim
00:17:23 [W] The kubenetes CSI site called the so called the external provisioner will see that request and it wants to talk to some CSI componentconfig roll part and ask that part to create a volume.
00:17:37 [W] So we do have that in p.m.
00:17:40 [W] CSI.
00:17:40 [W] We have a component that runs alongside external provisioner once in your cluster.
00:17:46 [W] And that part of BMC is I knows about all of the different note instances of p.m.
00:17:55 [W] CSI because voters register when they start with the control control pad and then when volume needs to be created for control part of reaches out to the individual nodes.
00:18:06 [W] It's actually using CSI calls for that but it's almost like a custom protocol here where we then find the mode create.
00:18:16 [W] About topology that the volume now exists and we to Paula Jean information ensures that kubenetes becomes aware of the fact that a volume is only available on a certain node, and then we'll make sure that pot
00:18:24 [W] Using that volume line runs runs on the Note where we have created the volume.
00:18:07 [W] If all of that looks a little bit complicated when you get variety brushing, it is a system that is a little bit fragile. We found race conditions. For example if we still need to address
00:18:19 [W] So my long-term can include plan is to move all of that complexity into kubenetes.
00:18:29 [W] So in kubenetes itself, I managed to get two new features into 119.
00:18:37 [W] They are in Alpha.
00:18:38 [W] So not immediately available.
00:18:40 [W] Not not not always enabled but they are available one is taking care of storage capacity tracking.
00:18:49 [W] It's an API that allows a saw a CSI driver to publish information about its.
00:18:59 [W] here
00:19:01 [W] and then the kubernative scheduler can query that information directly from by talking to be a pi server.
00:19:09 [W] There's no need anymore to implement a driver specific schedule extension.
00:19:16 [W] That's much nicer because both schedule extensions are kind of hard to set up. Unfortunately.
00:19:23 [W] Capacity tracking that will be built into keeping itself. It will work the same way everywhere.
00:19:26 [W] The other feature was motivated by some limitations of the existing support for FML volumes for current approach is that kublr Don anode asks a local
00:19:42 [W] Approach is that Cupid on a node asks a local CSI driver to create a volume but only after pot has already been scheduled to a DOT between node and if it then turns out that
00:19:46 [W] Out that the node doesn't have enough storage which can happen even with storage capacity to dragging it.
00:19:52 [W] It pots getting you might have used outdated information.
00:19:54 [W] The driver Greeley can't do much and keep an edit itself also will not recover from from out of memory situations for portworx basically get stuck.
00:20:07 [W] that limitation doesn't exist when using Dynamic volume provisioning because in that case volumes get created before pots scheduling and if volume creation on a certain node fails, we can try on another note
00:20:23 [W] Idea behind generic ephemeral volumes. They rely on the normal volume provisioning mechanism.
00:20:30 [W] There's a new controller that creates volume claims automatically for a pod and then and completely unmodified cri-o driver can be used to create the volume. That's the other big Advantage for CSI
00:20:45 [W] Pot and then and completely unmodified See Is Right driver can be used to create the volume. That's the other big Advantage for CSI driver developers.
00:20:48 [W] We don't need to do anything special for this feature to work.
00:20:50 [W] It's all in Cuba Nettie's and because we are using for normal volume provisioning mechanism.
00:20:57 [W] We also get support for all of the other features that come with that like restoring a snapshot restoring a clone that you took earlier.
00:21:04 [W] So the volume doesn't even need to be empty anymore.
00:21:09 [W] Last looked over at least we Central component of PMC is I've at that is what I want to eliminate by deploying the external provisioner on each node, alongside for local
00:21:24 [W] Eliminate by deploying the external provisioner on each node, alongside for local pmc's iPod.
00:21:33 [W] That's currently under investigation.
00:21:34 [W] There is some preliminary code.
00:21:36 [W] The biggest open is how to do volume provisioning. Then when the volume may be created on different nodes. We have to coordinate between the different external provision are instances.
00:21:49 [W] But there are some ideas and perhaps battle that'll be something that actually works in practice.
00:21:55 [W] We'll see.
00:21:58 [W] And with that I'd like to conclude my presentation.
00:22:00 [W] There are things you can do right away.
00:22:04 [W] You don't even need to buy p.m.
00:22:07 [W] Of course. If you could can please go ahead Intel certainly won't mind.
00:22:11 [W] But if you are just want to try it out.
00:22:13 [W] It is possible to bring up a qmo cluster where RPM is Emile emulated you find instructions and subscripts for the ad in the pmc's irepository. And for PMC is irepository.
00:22:27 [W] Also has deployment examples don't deployment files for memcache D. So you can even bring up a real application on that virtual cluster.
00:22:36 [W] with my kubenetes 6 storage shed on we would also love to get feedback for the new communities Alpha features because without feedback we will not be able to make them stable or
00:22:52 [W] Move into boards, bwz that we live depends on feedback from actual users. So you can reach me personally via my Intel email address or you can hop onto the kubernative slack and usually find me
00:23:07 [W] Until email address or you can hop onto the kubernative slack and usually find me and all of the other kubenetes CSI developers on the CSI Channel or the Sig storage and I'll and with that
00:23:08 [W] verse six storage challenges and with that
00:23:12 [W] Rubber duck and I would say I would like to say thank you for listening, and I hope you have questions after the dog or later on after watching this video.
00:23:21 [W] Thank you.
00:23:37 [W] Hello, so I'm I'm actually watching the questions box here in my interface. And if you have further questions, I'd certainly would love to answer them.
00:23:52 [W] We had a few questions why the talk was running perhaps I can address those first.
00:23:57 [W] The question was whether that is for memory databases.
00:24:01 [W] Yes. Absolutely sap. Hana was one example that was asked about and that is enabled for p.m.
00:24:14 [W] You may want to ask sap about sap Hana for keeping it is I'm not sure how well that is supported for moment, but it is being worked on as far as I know.
00:24:30 [W] And other applications memcachedb is my prime example because it also has a very well written blog post that explains the benefits that you get from building a system with p.m.
00:24:44 [W] Performance numbers are in that blog post.
00:24:46 [W] For example.
00:25:02 [W] So another question was what's the difference compared to an in-memory database that regularly flushes to data the key difference here, is that
00:25:18 [W] What's the difference compared to an in-memory database that regularly flushes to data the key difference here is that you you you don't have
00:25:22 [W] You don't have to different levels of storage you'd actually store your data immediately in the long-term storage and and the backend storage and this whole complex setup that memcached you supports with taking
00:25:37 [W] And flushing it to disk that is not needed anymore.
00:25:40 [W] You can just build a system without an additional local disk, and you still have persistency.
00:25:48 [W] You could of course still use the PM volume in I almost and then unmodified applications that support flushing to disk will also work.
00:26:04 [W] That that's fine. If you don't have an app direct enabled application.
00:26:09 [W] But for example memcache D performs better if you just directly enable the the Dex styra can direct access Mode 4 p.m. And run it that way.
00:26:33 [W] There is a question about data replication.
00:26:36 [W] That's really the same as any other local SSD.
00:26:42 [W] It must be supported by the application p.m.
00:26:47 [W] Itself doesn't deal with that at all.
00:26:50 [W] It just is local storage in the application must be built such verdict can deal with a failing node.
00:26:57 [W] So that that probably should be covered at the application Level.
00:27:02 [W] another question
00:27:07 [W] There's a question from about about object storage container object interface controller right now.
00:27:17 [W] There's no collaboration.
00:27:20 [W] It would be interesting to see what such a provider would need to do right now p.m. CSI is really just the lowest level in your software stack.
00:27:34 [W] It stops at giving an application of file system amounted file system with Dex demanding and what applications do in terms of higher level semantics like Object Store that really is up to the for software stack that runs on
00:27:49 [W] and how well it works with Pima than depends on the enabling that people up store in the higher levels of a stack put into their databases for measure cashing services or the object store Intel has
00:27:56 [W] Some Partners or well for example worked with memcache Jian that enabling is working with a zippy but at some point we just have to point people towards the PMI all website and hope that people will be able to use that
00:28:01 [W] V PM M IO website and hope that people will be able to use that information and and adapt their applications.
00:28:08 [W] And if you want to try that out, I really can encourage you to do that under Guillermo because you can do full software development without the actual Hardware.
00:28:21 [W] Another question on the on the jet about on beyond the question box about performance when using p.m.
00:28:30 [W] As a local storage disk.
00:28:32 [W] it is faster than normal ssds.
00:28:38 [W] It depends a bit whether you set up the file system for app direct mode or for normal Silo in PMC is I we are currently focusing on the app direct mode and invert mode
00:28:54 [W] normal ssds
00:28:58 [W] it depends a bit whether you set up the file system for app direct mode or for normal Silo in PMC is I we are currently focusing on the app direct mode and invert mode
00:29:27 [W] - has this most distant deactivated that affects performance for some operations.
00:29:33 [W] We are currently thinking whether we should just have a flag another parameter in our storage class that says I want that p.m.
00:29:41 [W] William for normal file I/O and then use it like it really normal volume that then uses the page cache for for things that were
00:29:56 [W] Persist it immediately and that may be a better choice in some cases.
00:30:02 [W] If the if that is something for people who want to do with p.m.
00:30:05 [W] Then we'll just support that also in PMC. The science just hasn't hasn't been a priority yet.
00:30:13 [W] Now the billion-dollar question are there Cloud providers offering p.m.
00:30:18 [W] There is or was an alpha project program at Google to get it into the Google Cloud.
00:30:27 [W] We do have some documentation in the pmc's irepository about that under examples.
00:30:34 [W] It's unfortunately Alpha. So you need an invite you need to work with Google to get access to those machines if
00:30:43 [W] You have a need for this and you know that Pima would be useful to you.
00:30:47 [W] I can only encourage you to talk to your Cloud providers and ask them to buy the necessary hardware and integrate that into the cloud offering.
00:31:00 [W] So another question is about the using persistent memory via remote dma.
00:31:09 [W] that is possible, but it's technically possible.
00:31:15 [W] Self we've been investigating a little bit but it's way too early to say whether that is useful or what it would take to implement something like that.
00:31:27 [W] No PMC is I is a local storage drive and that's what it's currently designed for.
00:31:37 [W] These are all good questions.
00:31:38 [W] Just let keep them coming can PBS allocated review?
00:31:45 [W] So yeah, if you implement if you mount it as Dex mode, then we power failure is is it is resistant against sudden power failures both at
00:32:00 [W] So V of X FS x 4 + XF s file systems they are doing the necessary flushing when writing their metadata and applications that flush also.
00:32:13 [W] protected against power loss
00:32:10 [W] if they don't flush then you may still have some lost data because data gets written into the CPU cache line first and then it's not stored persistently yet until that cache line gets flushed.
00:32:25 [W] but for data itself, it has been flushed with proper CPU instruction that is guaranteed to survive the power loss and that the Kremlin arity of that guarantee is actually at the byte level you don't and you
00:32:39 [W] He of that guarantee is actually at the byte level you don't and you can't go you can't.
00:32:39 [W] Well that's actually partly a problem for traditional applications.
00:32:42 [W] You can't assume that entire blocks are always written completely that is a small difference of all depending on your application might be a bit might be a problem.
00:32:54 [W] It is a difference conceptually compared to traditional ssds where you either write a full block or don't write the block at all with
00:33:02 [W] with the new deck semantics and with p.m. As a backing store for the file system. You can actually have a partially written block and that is something that we applications need to be aware when supporting or when they want to recover from a
00:33:24 [W] Okay, only one Limitless one minute left.
00:33:28 [W] Thanks for attending and I'll be on slack if you have further questions.
