Tutorial: From Notebook to Kubeflow Pipelines with HP Tuning: A Data Science Journey: CUSJ-1722 - events@cncf.io - Monday, August 17, 2020 9:00 AM - 286 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:04:40 [W] Hello everyone and Welcome to our session from notebook to kubeflow pipelines for hyperbole under two new in this session will fully we will be showing a complete data science workflow for optimizing your models and we will be using tutor notebooks.
00:04:55 [W] They got a band kubeflow Pipelines.
00:04:56 [W] This Is Us Timeless categories software engineer director
00:05:01 [W] Hi everyone.
00:05:04 [W] I'm Stephen of your Lancer software engineer at a record as well. At this point. We would like to give some credit to Sarah models from Google who had helped us dance with this tutorial.
00:05:15 [W] What's in it for you in this session?
00:05:19 [W] What'd you learn by the end of the session? You will have converted a jupyter notebook to develop a plan with gay.
00:05:28 [W] This makes your workflow a lot easier and faster.
00:05:35 [W] You will have performed by pain bible-based hyper current returning with caching which makes your pipelines run faster.
00:05:41 [W] Everything will be simply since we will be using intuitive you guys for everything.
00:05:47 [W] will also accelerate your machine learning life cycle with jail as an orchestration to me and finally you will gain complete with buildpacks.
00:05:49 [W] Of your training you can find the slide deck following this link here where you're also able to win some cool prizes.
00:05:57 [W] Although this is a recorded session who have live Q&A on Twitter and Linkedin using these hashtags.
00:06:03 [W] The machine learning platform we're going to use is to flow for those who are not familiar with it.
00:06:14 [W] Kubeflow is an open source project dedicated to making the deployment of machine learning workloads on kubernative simple portable and scalable and how can you use it you can deploy and manage complex machine Learning Systems of scale.
00:06:26 [W] can experiment rapidly. You can perform hyperparameters tuning you can Chloe hybrid and multi-cloud workloads and you can have the so-called.
00:06:34 [W] feature of ci/cd continuous integration and deployment
00:06:38 [W] this is a 10,000 feet overview of machine learning as mm as animal engineer would look at it.
00:06:45 [W] Machine learning requires some tools and Frameworks for data scientists.
00:06:49 [W] Kubeflow encapsulate applications and services to run those tools on top of kubernative clusters.
00:06:55 [W] And kubernative runs on Prem or on any cloud.
00:07:00 [W] And this is a machine learning workflow as a data scientist experiences that the science begins with identifying the problem and collecting and analyzing data.
00:07:11 [W] Then the data scientist has to choose a machine learning algorithm and code their Motors.
00:07:16 [W] Subsequently they have to experiment with data and model training.
00:07:23 [W] Then the optimizer model with hyperparameters unit.
00:07:30 [W] And of course in the very end, they need to serve the model with the best result kubeflow has various components to achieve their all the aforementioned in this session.
00:07:38 [W] also jupyter notebooks kale Gotham and kubeflow Pipelines
00:07:41 [W] How can someone use the flow?
00:07:43 [W] You have a graphical user interface GUI to manage notebooks volumes snapshots pipelines Etc. This is an image of the central dashboard.
00:07:53 [W] Also, you have to command line interface has a of cattle and cubicle.
00:08:00 [W] And of course kubeflow has its own apis and sdks and here are some examples departments. Okay got the BPI in metadata is the day.
00:08:09 [W] There's a perception that machine learning is all about more than gold and coupling complex math. But in reality machine learning application are complex distributed systems require lots of devils and various web components for monitoring serving
00:08:25 [W] Sources it's a kubeflow container has always components so that you can run them kubernative. Thus you have a uniform way around your workloads everywhere.
00:08:36 [W] And at this point I'll hand over to Stephanie.
00:08:39 [W] Thank you Alias.
00:08:46 [W] So now you've seen how kubeflow can accelerate and simplify the whole orchestration process of running machine learning workflows at scale in this tutorial.
00:09:01 [W] We're going to specifically focus on the data science experience.
00:09:10 [W] So how kubeflow and the tools we are going to present we enable to dramatically simplify and accelerate the data science workflow starting from
00:09:13 [W] Development environment that is most often Jupiter Jupiter.
00:09:19 [W] notebooks then how kale as I work to do with enabled you to create pipelines cncf seamlessly and run them in kubeflow pipelines and then while and then we're doing all of these Rock will enable you to
00:09:34 [W] Will close the circle go all the way back from browning to developing.
00:09:41 [W] essentially.
00:09:47 [W] I was completely refused ability enabling you to explore your pipelines back in time.
00:09:48 [W] So we were talking about pipelines as the as the core workflow tool in kubeflow.
00:10:08 [W] This is because I've the data science is inherently a pipeline process.
00:10:13 [W] Usually when you do that the science your job your work consists of several independent steps that come
00:10:19 [W] One after the other very these may be data ingestion and then transformation validation afterwards. You do model trading either locally, or at scale and you end up serving and monitoring
00:10:34 [W] Perhaps you can be combined together in Pipelines.
00:10:39 [W] Now this Workshop will focus on how to simplify as much as possible the building process of a pipeline directly from your local environment via a graphical based approach
00:10:54 [W] how to completely automate data versioning to enable to enable reproducibility and collaboration, of course the tools that will make this possible our kale and directors Rock
00:11:10 [W] Now I've been talking about converting a notebook to a pipeline.
00:11:22 [W] So this is this makes a lot of sense because if you think about it when you work in a jupyter notebook, you are already defining a sort of work flow. That is
00:11:32 [W] Hold of yourselves in the notebook can be thought of as isolated steps can be even combined together and attached to one another with dependencies.
00:11:48 [W] Crosses makes it very easy to parallelize and isolate your code at scale.
00:12:02 [W] So you can you could even easily run hyperparameters running over a notebook and it steps and then also it becomes very easy when running pipelines
00:12:10 [W] Planes to apply data versioning and reproducibility to all of these independent steps and you'll see how you see later how this can be achieved and the various parts of your notebook to become independent
00:12:26 [W] Might have different requirements.
00:12:35 [W] So data processing step might be cpu-bound. So it will need a certain kind of Hardware resources while another part of your code that might be doing
00:12:44 [W] I would benefit from running in a node with a GPU and all of this comes visible with kale and Brock.
00:12:53 [W] Let's take a look at how these tools combined together with kubeflow dramatically improve your development workflow and life cycle.
00:13:09 [W] So traditionally you would write some machine learning code may be in your notebook.
00:13:19 [W] But then once you're ready to build a pipeline out of it and scale it up you would need to create a new Docker image where you back Edge all of your dependencies and your code then
00:13:25 [W] You need to teach your original code or machine learning code and actually write the pipeline with some specific SDK.
00:13:35 [W] For example, the kubeflow pipelines DSL combine this pipeline upload it and run it whenever you need to go back because maybe you made a mistake or you need to improve your own code.
00:13:47 [W] need to go through all of these Cycles the cycle again you to write again your code repackage it into a Docker image again and
00:13:54 [W] again, but now with Kate and rock all of these processes process becomes much simpler because you just need to write the code in the notebook tag The Notebook with the UI driven way. You'll see later how
00:14:09 [W] The click of a button just run it whenever you need to amend your code.
00:14:16 [W] It's seamless just write it in the notebook and click the button again.
00:14:25 [W] You can already think how this workflow dramatically improves your iteration time and how much faster allows you to be productive.
00:14:34 [W] Okay, so besides converting notebooks to single bike lands look also at how to run hyper parameter tuning optimizations in a data science word.
00:14:49 [W] It is usually the case that you are running machine learning algorithms that are dependent on some parameters.
00:15:04 [W] So what you end up doing after writing your first iteration of an algorithm is starting to tinkering with the
00:15:05 [W] There's and analyzing by yourself the produced Matrix. This is of course a very time-consuming process and also error prone but then there are ways to automate all of these
00:15:20 [W] Automatic tools that basically use your algorithm and search through a given search space the parameters that you give in automatically with some specific
00:15:38 [W] A given search space the problem with respect to give in automatically with some specific mathematical optimization techniques and optimize over the metrics that you want to optimize
00:15:46 [W] And optimize over the metrics that you want to optimize in the case of kubeflow.
00:15:50 [W] Kata pissed your fishtail hyper parameter kublr its support several machine learning Frameworks, including including tensorflow MX MX net and others. And today what we are going to see
00:16:02 [W] How you can go from notebook to kale to khatib. So scaling up your notebook as Pipeline with Hyper parameter tuning with just the click of a button
00:16:18 [W] Notebook as Pipeline with Hyper parameter tuning with just the click of a button and with this I will hand it over back to Elias who will start walking you through the actual tutorial.
00:16:26 [W] Thank you. Stephanie to follow the tutorial.
00:16:35 [W] Please go to this link find the corresponding code lab that contains the step-by-step instructions.
00:16:38 [W] This is the agenda of the tutorial.
00:16:42 [W] We will install many F. Then we get some already cooked machine learning code explore it converted flow Pipeline and then perform a corrupted zoom in on a specific mode.
00:16:53 [W] The first step is to install many F. But let's see what many KF is
00:16:56 [W] Yes is the easiest way to deploy and experiment with kubeflow in just a few minutes.
00:17:05 [W] You can have it on GC p on your laptop or any on bream infrastructure.
00:17:06 [W] It is an all in one single note include should use.
00:17:09 [W] And mean KF minikube flow is actually minikube with kubeflow and our software Rogue a data management platform.
00:17:18 [W] This image is taken from the tf-x paper kubeflow brings all these parts together in a container of the way and gives you a nice way to perform hyperparameters sugar moreover.
00:17:31 [W] You should provide you with an intuitive UI manage all your components as we have already mentioned to Florence to the of kubernative which takes care of duplicates of or castration. But then you also need to manage your storage you can use
00:17:44 [W] For your cloud provider subjects tour, but many F comes with rock or data management platform.
00:17:52 [W] We have made a lot of contributions to kubeflow because of our expertise in storage and data management who have extended kubeflow making it data where you contributed code for the trip look and use kubernative e c--'s no matter where it was deployed on your laptop on Prem
00:18:08 [W] Rook takes care of data versioning and since your data across all different infrastructures
00:18:15 [W] Let's see.
00:18:20 [W] Why data values versioning is important machine learning.
00:18:23 [W] Let's say we have a pipeline where the first step is data validation when they step is done.
00:18:28 [W] We take a snapshot of it and we do the same for the next step data pre-processing then another step runs added phase.
00:18:34 [W] are we going to debug we can clone the latest snapshot explore the data and code?
00:18:40 [W] And then fix the error and continue the pipeline.
00:18:43 [W] Listen, I'll set up for many years.
00:18:48 [W] This is the code lab you found before.
00:18:54 [W] Here's introduction related to the workload. We are going to follow.
00:18:59 [W] Here are various information for those who are not familiar with Google Cloud. Finally.
00:19:16 [W] Let's head to the marketplace and solving here.
00:19:16 [W] When KS is a Marketplace solution.
00:19:20 [W] You searched for it?
00:19:32 [W] click on launch
00:19:38 [W] Tuesday the project were going to deploy working.
00:19:44 [W] And here are various configuration. We can choose for our mini F. Then choose the zoom can choose its discs.
00:19:57 [W] you can select to be used if they're available in the zone. And once we're ready we can click on deploy.
00:20:01 [W] So now the instance is going to come up and after that it will take about 15 minutes Foreman KF to be up and running and for you to be ready to experiment.
00:20:13 [W] At this point I'll hand over back to Stephanie.
00:20:18 [W] Thank you aliased. So in no time you should have a ready minikube. Then you will be able to start with with the tutorial yourself now.
00:20:35 [W] I will walk you through the rest of the code lab. So we'll see how you can go from notebook to cut it to kubeflow Pipelines.
00:20:44 [W] So I will head over to my minikube.
00:20:51 [W] Yes, as you can see here. I have the kubeflow sense of dashboard. If you already used to kubeflow if you've already used it, you might notice that this the the sidebar
00:21:04 [W] He's a little bit different.
00:21:10 [W] This is because with the working on the kubeflow central dashboard to unify in a consistent way all of the kubeflow components together in a single place and you will see how I will be using.
00:21:21 [W] These throughout this tutorial now, of course, this is work that we are going to contribute Upstream very soon.
00:21:30 [W] So you will be able to see it in the key table discussions.
00:21:36 [W] now I'm heading over to notebooks.
00:21:37 [W] And I will create a new notebook just for this demo and then it Q Khan, I'm selecting Jupiter image that includes kale.
00:21:50 [W] Then I'm also adding a data volume.
00:21:56 [W] All right, press launch. And that's it.
00:22:03 [W] This is very simple in kubeflow to spin up new Jupiter servers where you can work on your own code and start new Pipelines.
00:22:12 [W] Okay, The Notebook server has been provisioned so you can just connect and in no time. We have provisioned in a self-serve way a full tutor lab environment.
00:22:43 [W] Now I want to clone some code where I can work on.
00:22:49 [W] I can do that from the record.
00:22:53 [W] Love as you can see here.
00:22:55 [W] There are overfill structures that you can follow Natalie your stage now, I'll just scroll here to the code that allows me to clone the K Repository.
00:23:06 [W] So I'll head over back to my notebook.
00:23:10 [W] So we are cloning your fish. Okay repository that is open Stones as you can see there is an examples folder we're going to open.
00:23:24 [W] dog treat classification example
00:23:28 [W] that we prepared just for this tutorial.
00:23:34 [W] There are two notebooks. We will start with the dark brick one and then switch over to the drug dog breed captain.
00:23:41 [W] Now I want to go over into the details of what is in the main team in this notebook.
00:23:52 [W] Just keep in mind that this code is based on a Udacity project where we are basically processing some dog images and building a deep learning classifier
00:24:05 [W] recognize dog breeds
00:24:08 [W] The first thing I want to do is to verify that my notebooks has the dependency needs to run the code.
00:24:23 [W] So just from the first selfie everything important and we notice that we are missing some dependencies.
00:24:25 [W] So I have a helper here that surrounds the install to install the requirements notice how I am installing this Library here now on the Fly this will be important at a later stage when we will convert these to a
00:24:38 [W] Priests all the requirements notice how I am installing this Library here now on the Fly this will be important at a later stage when we will convert these to the pipeline.
00:24:40 [W] You can think of this process as I could be tinkering with the code solving bugs at the you features and then installing the planet dependencies on the fly as I got you covered.
00:24:56 [W] Okay, so now we should have all the dependencies we need.
00:25:02 [W] Harvest off my colonel
00:25:06 [W] Okay, so every import succeed.
00:25:14 [W] So let's say I'm done with the code down with the code.
00:25:21 [W] So I have implemented all my data preparation. And then I'm detecting my dogs and brought in some detectors brought the CNN and deep learning code to recognize them again.
00:25:35 [W] Let's say I'm done with the code down with the code.
00:25:36 [W] So I have implemented all my data preparation. And then I'm detecting my dogs and brought in some detectors brought the CNN and deep learning code to recognize them again.
00:25:38 [W] I won't go into details, but you see there's some involved code in these this notebook.
00:25:44 [W] So I test it locally.
00:25:49 [W] I have all my dependencies and I want to scale it up and convert it to a pipeline.
00:25:49 [W] How do I do that? So I can go over here on the left? Click on the K panel enable it?
00:25:56 [W] And now you see a whole bunch of things happening.
00:25:59 [W] So kale is a is a way for you to tag notebook cells in a UI driven way and assign these cells to pipeline steps.
00:26:15 [W] Let's let's take an example here.
00:26:16 [W] Your you see this cell is tagged as step load data.
00:26:22 [W] The pressing on the top right here uck provides a very simple way for you to Target these cells with the specific name.
00:26:38 [W] And what are you doing here is basically assigning these sell for pipeline step multiple cells, as you can see can be merged together in a single one and then it is that simple to create new steps.
00:26:48 [W] and dependencies
00:26:51 [W] once you do this, of course, we've already dug this notebook for Simplicity.
00:27:00 [W] You can just select them in kubeflow experiment if the notebook a name.
00:27:03 [W] And then click the compile and run button.
00:27:07 [W] So what is happening now?
00:27:12 [W] Is that kale has validated a notebook now Brock are Reckless Rock data platform is taking a snapshot of the current notebook all of its volume volumes so that your environment
00:27:24 [W] East completely dry produced in the pipeline steps.
00:27:29 [W] This is why even though I just installed on the fly my library's the code will run seamlessly the pipeline steps without you having to build the new Docker images.
00:27:44 [W] So kale then compiled The Notebook created a pipe and uploaded it to you for five times and it starts with a new run. Let's go looking for pipelines using this link and see the pipeline running.
00:27:58 [W] Okay, so you can see here. I have my two volumes that are basically new volumes that start from snapshot that were taken by The Rock when I click the button and like python is starting.
00:28:20 [W] Let's give you the little bit of time to farm while I go back to the slides and talk to you and walk you through how kale is actually doing all of this.
00:28:32 [W] So kale is basically a python package and a group the lab extension.
00:28:38 [W] It allows you to convert to jupyter notebook to keep the pipe lines in a completely UI driven way without the need of writing any kind of external SDK.
00:28:52 [W] How does he do that?
00:28:55 [W] Well in the moment you click the compile and button running compile button.
00:29:09 [W] Ko6 the notebook and parsec analyzing all of your notations. You've added the other kill you. I these turns into an internal graphic presentation that is parsed by KO so that
00:29:15 [W] Look at all the data dependencies between the notebook cells.
00:29:29 [W] So cable the text is dependencies. And then it takes care of marshalling the data between the pipeline steps for you. So that the code execution is completely seamless just like it happens in The Notebook.
00:29:36 [W] And all of this is possible because scale also takes care of generating a whole bunch of code that you would have that you don't need to write any more because Kayla is doing it for you.
00:29:50 [W] No, that scale is an open source project.
00:29:57 [W] that is hosted in the kubeflow kata-kata program ization, and we would be very happy to receive feedback and questions and also contributions.
00:30:10 [W] Now let's go over back over to the pipeline that it is still running.
00:30:19 [W] It should be done very quickly. So notice how we already have two steps. You might remember that we dug some cells with the load data
00:30:35 [W] Elotl notation and then we talked some other cells that detect token notation specifying a dependency.
00:30:50 [W] So Kate was able to separate this code into multiple steps and then even Branch out the training of the several different deep learning algorithms because the dependencies
00:30:59 [W] We're all referring to a common step.
00:31:06 [W] So you now have code that would have run cereal in the notebook running concurrently in the kubeflow by plan.
00:31:10 [W] All of the logs are preserved.
00:31:14 [W] And also let me go to a pipeline was already concluded or rather.
00:31:26 [W] I take this one since it has already completed.
00:31:30 [W] as you can see
00:31:34 [W] I have a whole bunch of logs here that's are taking that are basically reproducing the exact visualization that you would see in Jupiter.
00:31:49 [W] For example here the CNN resonant 50 in the notebook produces a visualization at the end of its computation to show one of the detective images one of the
00:32:01 [W] active
00:32:02 [W] so as you can see kale is able to completely reproduce even the notebook visualizations.
00:32:15 [W] So whether you are plotting with MacBook or interactive with a visualization with lockley, all of these is preserved and reproduce in kubeflow Pipelines.
00:32:21 [W] Okay.
00:32:24 [W] So now that we've run a single pipeline we would want to scale it up and run it as hyper parameter tuning job.
00:32:35 [W] Let's go to the coffee table.
00:32:38 [W] notebook
00:32:40 [W] In this specific notebook.
00:32:44 [W] We are skipping two of the deep learning algorithms and we are running just over there is net 50.
00:32:52 [W] What we want to do is to parameterize the notebook with some input parameters and run the generated Pipeline with Scotty to optimize over certain metric so that we can optimize automatically.
00:33:10 [W] The training algorithm.
00:33:18 [W] So we need two important things to do this input parameters and an output metric.
00:33:24 [W] So with kale it is very simple to create a pipeline parameters from The Notebook. All you need to do is to Define notebook cell with some viable
00:33:36 [W] lens and Target
00:33:38 [W] with the KU I with the pipeline parameter tag.
00:33:43 [W] And then I'm scrolling down all the way to the bottom of the notebook. What you want to do is to have your code produce a metric produce a metric that then will be exported
00:33:59 [W] like bicycling and analyzed by Kathy
00:34:03 [W] So what you need to do is to print whatever variable your algorithm is producing and that you want to be used as an optimization and Metric printed and then tag the cell.
00:34:18 [W] With the pipeline magic stock.
00:34:21 [W] This is all you need. Once you have this you can go over the kale sidebar enable the Ducati taco and click on the setup cutting job button.
00:34:36 [W] Now as you can see, okay provides you with this very simple to use UI that provides the input parameters.
00:34:49 [W] These are exactly the names of the variables.
00:34:52 [W] I defined in the bookshelf.
00:35:03 [W] We have already filled the values we want to use but you could do you could insert here whatever values you want from ranges least integer float string values. What have you
00:35:07 [W] Then I can select a search optimization algorithm.
00:35:12 [W] And my subjective again, this is exactly the variable name.
00:35:22 [W] We are printing in the pipeline magalix cell.
00:35:22 [W] That's all you need to do clicking close and again the compiler and run button.
00:35:30 [W] So now a similar thing happens as before keep value this notebook Rock takes a new double snap shot at this specific point in time, then kale converts The Notebook uploads a new pipeline.
00:35:46 [W] And now here's what's happening kale is actually starting a new cutting job.
00:35:55 [W] And you got experiment where each category run is actually a kubeflow pipeline spiffe. This was not possible before in his scale that is that it is acting as a bridge between
00:36:10 [W] Thank you for pipelines.
00:36:13 [W] Let's go see these experiments.
00:36:16 [W] So here I'm opening the copy experiment page.
00:36:27 [W] Let's give you a few seconds to load up.
00:36:28 [W] As you can see, I have a new cat experiment and here and you kubeflow experiment.
00:36:44 [W] They have the same names.
00:36:47 [W] They have one single Run for the moment again with the same name.
00:36:51 [W] name. So you have a one to one correspondence here. You see which are the input parameters that copy decided to give as input to this pipeline. So I'm heading back over to pipelines. I see.
00:37:04 [W] Five times is generated.
00:37:06 [W] It is starting to run.
00:37:09 [W] I can see them here in the config tab will tolerate input parameters.
00:37:19 [W] So this pipeline is running just like before but with specific parameter parameters that were given by K T. Now, it would take a lot of time for us to monitor tens of
00:37:36 [W] I have lines to find his experiment.
00:37:43 [W] So let me head over to an experiment that we already run some time ago.
00:37:45 [W] Okay. So this was an experiment created in the same way. We did now from The Notebook clicking a button on kale as you can see. I have several pipelines here that produced a specific metric
00:38:06 [W] So this is the matter that we were printing from The Notebook here was able to interpret that and output. This metric is an artifact.
00:38:15 [W] Let's go over also put the cat experiment.
00:38:19 [W] This is the corresponding khatib experiment that was running as you can see. We have many rounds and how their performance were ride over time. We have all of the runs even here with the corresponding Matrix
00:38:39 [W] able to kilometers
00:38:41 [W] In the meanwhile.
00:38:50 [W] Our experiment is still running and you can see here. You can have a live view directly from The Notebook about how many
00:39:02 [W] rounds are are running in this moment. And as soon as one at least one would finish you would see here a live view over.
00:39:17 [W] What is the current best result as well all from your notebook?
00:39:22 [W] Okay, so that was it for the Hands-On tutorial.
00:39:29 [W] hope you will have time to go through your code lab the around with the code. Stop start your own pipe.
00:39:33 [W] Rise and have fun.
00:39:35 [W] So let me go back to the slides to summarize what we saw together today.
00:39:41 [W] So what have you learned today today?
00:39:52 [W] You have to learn how you can run a pipeline based hyper parameter to the workflow starting directly from your jupyter notebook in a super simple UI driven way.
00:40:06 [W] Tell his acting as dwarf flow tool that brings together notebooks give a pipelines and cognitive experiments.
00:40:13 [W] This is a great simplification of how you can run machine learning workflows in a UI driven way.
00:40:21 [W] also all of these exploit caching and I will show you in a second how that I actually forgot to mention before so we'll go back to
00:40:36 [W] But I actually forgot to mention before so we'll go back to the to the kidney for pipelines you I said you can see that all of this work flow.
00:40:44 [W] Thanks to rock is exploiting our cash in feature that is built on top of kubeflow.
00:40:49 [W] Also, this allows you to collaborate faster and more easily.
00:40:54 [W] So let's head over back to the kubeflow dashboard.
00:41:03 [W] This is actually something I wanted to show you but God.
00:41:05 [W] So as you can see here.
00:41:10 [W] I am looking at the experiments that we had already run in the past.
00:41:18 [W] So I have several rounds.
00:41:19 [W] I took one of them at random and there is this little icon here.
00:41:27 [W] So this means that this pipeline was cashed. Actually these steps were cashed.
00:41:30 [W] So why is this happening? The CNN resonant? 50 step is the only one in the pipeline that is dependent on some input parameters.
00:41:47 [W] It is the only part of the note on the original notebook code that reads that acts upon those parameters since Caleb knows these with also knows that the
00:41:58 [W] Won't change anything that will never change across all of your runs.
00:42:06 [W] This means that running log data and the tekton has several times would be a waste of resources.
00:42:14 [W] So what we've done is we've extended the current caching implementation of kubeflow that works over artifacts.
00:42:23 [W] To work also over snapshots and PVCs in this way the running time and the performance of all of these pipelines is greatly improved.
00:42:35 [W] Okay.
00:42:39 [W] So back again to us our summary running pipelines from notebook to kubeflow in a completely UI driven way with cashing in order to accelerate your workflow instamatic lie.
00:42:56 [W] I would like to conclude by saying that in a Rik talk. We are very passionate open source contributors.
00:43:10 [W] We've contributed to many components in kubeflow from Jupiter manager UI to pipelines to building are opinionated and super portable kubeflow distribution.
00:43:21 [W] Minikube F.
00:43:24 [W] We've done lots of work to the authentication and authorization.
00:43:28 [W] Architecture of kubeflow and also many contributions to the Linux kernel to support our data platform.
00:43:37 [W] Kubeflow is a great Community made up of many many big companies.
00:43:46 [W] And this is a community.
00:43:50 [W] We are always striving to find new contribution contributors and new passionate users.
00:43:56 [W] So please get involved. You can find here lots of pointers from the GitHub organization to our slack Twitter handle or write us an email to the kubeflow
00:44:08 [W] Scott's Google group will be happy to support you and onboard you.
00:44:14 [W] So thank you for staying with us.
00:44:22 [W] We really hope you have fun playing around with our tutorial make sure to head over our website and their end this quick link where you will find the slides and and you could win some
00:44:33 [W] Make sure to head over our website and their end this quick link where you will find the slides and and you could we some cool prices.
00:44:36 [W] Thank you again and see you soon.
00:44:38 [W] Hello and welcome. Again, we are now live to and ready to answer questions that have come up or may come up later on.
00:44:54 [W] Let's introduce ourselves again.
00:44:56 [W] I am yes categories and software engineer for right though and here are seven.
00:45:01 [W] Right.
00:45:02 [W] Hi everyone.
00:45:03 [W] So let's start with some questions.
00:45:10 [W] Does take work with another cabinet or just python Kiernan? That's a question from Dave son Ray energy cost.
00:45:23 [W] Okay, so
00:45:29 [W] okay actually works only with python Cardinals because there is some degree of introspection in the users code in order to magically create independent
00:45:45 [W] So let's start with some questions.
00:45:46 [W] Does Jade work with another Channel or just python Colonel? That's a question from Dave son Rainier T cost?
00:45:48 [W] Okay, so
00:45:48 [W] okay actually works only with python Cardinals because there is some degree of introspection in the users code in order to magically create independent
00:45:49 [W] That then come pipelines. So Gail actually needs to parse the users code and as of now it supports python.
00:45:56 [W] Nice.
00:46:01 [W] Thanks.
00:46:02 [W] We have another one from again.
00:46:14 [W] May I use this pipelines for other machine learning?
00:46:17 [W] I guess other machine learning other machine learning servers like I'm at flow.
00:46:22 [W] so I guess
00:46:32 [W] I guess you're meaning to use scale with other platforms that are similar to keep the pipeline's the answer is no currently kale supports converting notebooks just
00:46:51 [W] pause
00:46:52 [W] then
00:47:02 [W] we have a question from every mode and experience with the GPU acceleration in kubeflow.
00:47:10 [W] Okay, so the flu supports gpus as long as your kubernative cluster has attached Hassan gpus attached to these nodes. You can attach your gpus
00:47:27 [W] servers using the notebooks UI when creating the notebook server and you can also have used to be using your pipeline steps actually who have implemented
00:47:43 [W] Fidel future if you have a feature in the lab extension where when when defining the cell the step and adding the cell you can select your gpus to the mouth.
00:47:59 [W] We have another question from Maurice.
00:48:15 [W] When will your contributions the Upstream available in kubeflow.
00:48:16 [W] You have a Target release number.
00:48:17 [W] So we have monkey have made some changes to different components and we plan to put them on Upstream.
00:48:34 [W] We don't have a specific plan, but we should for publishing them soon.
00:48:44 [W] As far as taking features are concerned then everything is on the public Repository.
00:48:49 [W] We post them using the arson you you're welcome to contribute if you think any feature to necessity.
00:48:59 [W] Actually find its way in minikube flow of your can test it on gcp very easily like you saw in the tutorial and then eventually all of these new contributions with will also be available up straight.
00:49:29 [W] Many people have asked about the links.
00:49:43 [W] They should be published or should have published them as questions with high priority.
00:49:47 [W] Define both of the link for the general link with all our sessions and presentations and more.
00:50:00 [W] And the gold love as well.
00:50:02 [W] Heh, I see a question. If all of these can be running raspberries clusters what you say?
00:50:16 [W] Yes.
00:50:17 [W] I believe that silencing it would be very fun to do that.
00:50:25 [W] But it definitely needs work have kubeflow randomly raspberries.
00:50:30 [W] Yeah.
00:50:34 [W] I see a question. If all of these can be running raspberries clusters what you say?
00:50:44 [W] Yes.
00:50:44 [W] I believe that silencing it would be very fun to do that.
00:50:45 [W] But it definitely needs work have kubeflow randomly raspberries.
00:50:46 [W] Yeah.
00:50:46 [W] I have a question. I'm not sure about the answer may be Stefano.
00:50:50 [W] you can help is it possible to use kubeflow and Dale to version the models in rated of the datasets used?
00:50:56 [W] Oh interesting question actually.
00:51:05 [W] I wouldn't say that K or by itself helps you in version.
00:51:13 [W] rather Rock would be the major helper in this if you noticed in the in this tutorial with run several pipelines and at every step of
00:51:27 [W] The say that K of by itself helps you in version rather Rock would be the major helper in this if you noticed in the
00:51:28 [W] Ruff was taking snapshots and you can see snapshots as versions as version point in time of their work.
00:51:38 [W] so we are not.
00:51:39 [W] Let's say generating a specific models like exporting a machine learning model and taking version of it the rather taking snapshots of a of your own of your entire workspace comprising data
00:51:55 [W] Libraries source code everything and having virgins at each point in time.
00:52:03 [W] There is also another question by more it if it is possible to use our Jupiter.
00:52:24 [W] love Dockery images with a vanilla kubeflow distribution.
00:52:28 [W] There's a lot of course we can use our three matches.
00:52:37 [W] However note that.
00:52:39 [W] all the tremendous come with
00:52:43 [W] the rock come with rope module installed the blind which you will not be able to use so still you will be able to see the kale UI and use all the great features apart from the rock integration.
00:52:59 [W] Look at food.
00:53:01 [W] The question another question this question might be a little bit above from the topic.
00:53:37 [W] So typically when a model is created using the flow, how is it being consumed in production?
00:53:39 [W] Is it like API calls sending puts and then get out?
00:53:42 [W] So, okay. So how's the general kubernative like environment?
00:54:01 [W] Yes when you produce.
00:54:04 [W] They might be something coming soon Stefan.
00:54:18 [W] Okay. So how's you General kubernative like environment?
00:54:31 [W] Yes when you produce.
00:54:31 [W] There might be something fairly soon Stephanie.
00:54:32 [W] So let me try to answer the question, but step on the nose more. That's why you're I'd prefer for him to answer.
00:54:53 [W] So to flow has some components that aim to serve models. However
00:55:03 [W] I'm not quite sure how you can generate a model.
00:55:12 [W] Pick it up from somewhere else.
00:55:17 [W] That's something we plan on working on.
00:55:20 [W] And as far as far as you've defined it, then you should be able to use it.
00:55:33 [W] Actually, we have Rock taking snapshots of your produced data. So
00:55:36 [W] If you can access with your sins, then you should be able to access your models.
00:55:41 [W] Stefano are you back?
00:55:44 [W] Jack and tonight.
00:56:03 [W] Yeah, sorry for these guys.
00:56:08 [W] I completely missed the the answer.
00:56:09 [W] I was talking alone. Apparently they could you provide your answer.
00:56:22 [W] Maybe I was I don't have a great Insight on this part.
00:56:24 [W] Okay about model serving.
00:56:27 [W] Yeah, so kubeflow provides its own component called cave serving that is used to put models in production.
00:56:34 [W] does this work when you when you build a model in if you have a notebook you train in-toto?
00:56:39 [W] new finalize it once you have your final model you want to production allies it you export the model and then, you know kubernative like environment you want to containerize it and expose its functionality as a
00:56:55 [W] Build a model in if you have a notebook you drain it new finalize it once you have your final model you want to reproduction allies it you export the model and then, you know kubernative like environment you want to
00:56:57 [W] It's a rest service so that you can query and then it Returns the result.
00:57:03 [W] So kubeflow provides chaosmesh serving to do just that to simplify the whole process of mounting auto-scaling containerized production models. And actually Kaylee in the very near future will support exactly
00:57:16 [W] Just like we convert a notebook to a pipeline will be able also to automatically produce these cave serving resources on the Fly automatically with the click of the
00:57:31 [W] Is there any other question?
00:57:49 [W] Don't forget you can also chat with either cncf slack or live on Twitter using hashtags and mentioning us.
00:58:55 [W] I guess we'll be here for another 50 minutes.
00:59:14 [W] Not sure when the live Q&A will end.
00:59:19 [W] But we'll stick around for a few minutes.
00:59:24 [W] There is another question from Miguel.
01:00:18 [W] Can I create staging states to test before getting the production environment?
01:00:20 [W] There is another question from Miguel.
01:00:33 [W] Can I create staging states to test before getting the production environment?
01:00:33 [W] So using rock, it was a part of our presentation actually using Rock You Can enable recruit using your environment in various places.
01:00:36 [W] Reproducing your environment in various places so you can have the damn key experiment somewhere locally. For example in the media as we showed you can then move to
01:00:47 [W] move to more
01:00:50 [W] no more subs and training with bigger data set and once we are okay with the results, once you are happy with it, and you want to serve it you can move it to a production environment.
01:01:03 [W] A question from Nami. So it does it's also work with other ideas.
01:01:57 [W] Sil
01:02:01 [W] A question from Nami so I can look now, right that is also work with other ideas.
01:02:20 [W] So
01:02:20 [W] So currently this works on jupyter notebooks actually on Jupiter lab since you can spin up jupyter notebooks in a web-based environment and on kubeflow not completely self-serving way you click a button and
01:02:22 [W] Our server is ended up just for you.
01:02:23 [W] It would be possible to use Kale also on other web-based IDs for example, Visual Studio code and and we might integrate there with that in the future.
01:02:39 [W] Median follows up with another question show. Can we go back to previous versions of the motels?
01:03:00 [W] Yes, as long as you're using our software rock, you should be able to get two other persons.
01:03:05 [W] since you've taken Snapchat to it
01:03:09 [W] and another question from rotten.
01:03:25 [W] So how do you see Sage maker in this architecture?
01:03:27 [W] I've seen this asked a couple of times.
01:03:32 [W] So we've actually never worked with sage maker.
01:03:37 [W] maker. So to be honest, I don't know how to answer to the best of my knowledge says maker is a set of services and machine learning Services provided by AWS and there exists.
01:03:49 [W] I would say that once you build a pipeline with K. You can easily extend the generative DSL to also make use of these Brady to use components but
01:04:08 [W] That's of all I can say from my side.
01:04:13 [W] I've been 3 congrats on the quiz score.
01:05:05 [W] We did last year in May. I collaborate with your project.
01:05:42 [W] All right.
01:05:52 [W] Well, of course kale is a completely open source project and we really welcome and external contribution if you like what we do, please contact us you can
01:05:59 [W] Fusion if you like what we do, please contact us you can open issues on the GitHub. You can reach out on slack on the kubeflow kale channel
01:06:06 [W] Slack on the kubeflow KO channel channel, or you can even Pingas privately.
01:06:10 [W] Any other questions, come on?
01:07:00 [W] Okay, so we have another question from Josh to go over again the caching process we showed earlier.
01:08:28 [W] So tufin pipelines have implemented the very nice passing mechanism for for the problem steps.
01:08:44 [W] What we have done is that we have extended so that we use so that we exploit what Rook allows us to do that is reproduced
01:08:55 [W] cashing process we showed earlier
01:08:56 [W] So you feel pipelines for implementing the very nice passing mechanism for for the peplum steps.
01:08:57 [W] What we have done is that we have extended so that we use so that we exploit what Rook allows us to do that is reproduced environments
01:08:59 [W] Once we once did steps run and Rook is available.
01:09:04 [W] They automatically take snapshots of their own environment. So
01:09:09 [W] what we do is that we can steep some steps in order to make the the pipeline execution faster as we said earlier. Everything
01:09:25 [W] Took steps in order to make the the pipeline's execution faster as we said earlier. Everything is versioned Everything can be reduced so
01:09:28 [W] Everything can be reduced.
01:09:32 [W] So this makes us be able to work even faster. The pipelines can run faster.
01:09:35 [W] What else what else?
01:09:38 [W] Yeah, let's say that as soon as you start.
01:09:44 [W] Branching out of workloads, especially with Kathy right? You can start running hundreds of pipelines concurrently, maybe just to do one of the components of the pipeline
01:10:00 [W] Cash using local Superfast snapshots with rock dramatically increases the performance of the job making iterations much faster
01:10:15 [W] The performance of the job making iterations much faster workloads less computationally expensive.
01:10:19 [W] It's a
01:10:25 [W] I see some people are starting to complete the quiz on Twitter.
01:10:49 [W] I remind you that you can win some cool prizes. So keep up with the answers.
01:10:57 [W] A person from new to me.
01:12:33 [W] many does a scientist should have the company to start investing money and time in support?
01:12:38 [W] Well, if you are just starting out, you don't know the platform and you want to experiment actually if you are using Mini S.
01:12:51 [W] It's very easy.
01:12:55 [W] Just one person. You have a easy solution.
01:12:57 [W] Integrated that can be spin up with the click of a button from gcp.
01:13:08 [W] So if you're starting out to explore components explore how kubeflow pipelines and kale and rockwork just head over to gcp and one person will be surely enough.
01:13:15 [W] And some kale and rockwork just head over to gcp and one person will be surely enough.
01:13:17 [W] And then we have Michaela who is asking how do you deal with CPU and memory limits for the various odds, that kubeflow is using is it dealt with all automatically?
01:13:30 [W] so this part is actually another mini responsibility the admin knows what the glass that can support and users may have some
01:13:48 [W] Set them but they shouldn't have a last word in it.
01:13:54 [W] So when when the finding pipelines, for example, you may be able to specify your question limits, but there are some mechanisms in between defining
01:14:07 [W] Actually, you have been deployed that the admin May.
01:14:16 [W] May have a husband.
01:14:22 [W] We have set up.
01:14:24 [W] To reduce some load or make things work us as they think it should be in there cluster.
01:14:35 [W] hundred tons of since we fight some production
01:15:41 [W] Well, yes, we find we've had customers that we've had we if we have customers.
01:15:56 [W] I work in the very large files you production that we can it's not just a matter of the over to leave that you're using its this case. It's the users code makes
01:16:12 [W] It's not like Kalin kubeflow become a hindrance in using very large data sets.
01:16:24 [W] See another question from any tree is an engineer.
01:16:39 [W] Do you prefer jupyter notebook development or Python scripts for the design projects?
01:16:41 [W] What I would say, but I think that's a better person with reference.
01:16:52 [W] I'd say that I prefer by - three paths depending on what I'm doing for the designs. I think tutor notebooks are easy to use Easy to iterate
01:17:10 [W] Don't remove them. So everything comes comes in easier when Stadium
01:17:17 [W] Okay, so if you want to shoot the last minute questions will have a couple of more minutes.
01:18:34 [W] So if you follow all the links, you will be able to enter the filling the quiz you will be able to take part in a in a contest to win
01:19:54 [W] Among them.
01:19:56 [W] It's a scooter.
01:20:00 [W] Of course your results in the quiz won't matter much.
01:20:05 [W] soften the the participants.
01:20:08 [W] This folder is motorized and has a pretty fly branding.
01:20:26 [W] We're probably good with a questions.
01:22:04 [W] I think it's about time to wrap until then.
01:22:13 [W] Thank you for being in our system.
01:22:15 [W] Thank you for watching and you made some great questions.
