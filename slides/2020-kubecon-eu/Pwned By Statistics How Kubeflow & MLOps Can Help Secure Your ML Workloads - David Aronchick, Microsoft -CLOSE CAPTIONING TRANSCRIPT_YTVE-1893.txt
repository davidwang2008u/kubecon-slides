Pwned By Statistics: How Kubeflow & MLOps Can Help Secure Your ML Workloads: YTVE-1893 - events@cncf.io - Wednesday, August 19, 2020 11:37 AM - 1141 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:03:31 [W] Hi everyone.
00:03:34 [W] Thank you so much for the opportunity to present owned by statistics a way of looking at how to make your mlperf pipelines more secure using kubeflow and mlrs this is
00:03:47 [W] how to make your mlperf pipelines more secure using kubeflow and mlrs this is unbelievably weird I've spoken at keep con many times and this is by far the most surreal experience
00:03:54 [W] I've spoken at keep con many times and this is by far the most surreal experience that that you're on the other side watching this on a video, but I'm really excited to talk about this today. So
00:04:03 [W] At Microsoft we've talked about machine learning for a long time and we're really excited about the opportunity to you know, help the entire industry move forward when it comes to machine learning.
00:04:18 [W] You can see some of the big research breakthroughs that we've done in the past few years.
00:04:21 [W] Of course, we all give this back to open source, but you can see this is really where machine learning has you can see where machine learning has really
00:04:33 [W] The way that people are doing things where you are able to really transform the way that machine learning and you approach different problems here.
00:04:48 [W] You can see already that that you're seeing better than human performance in a bunch of different areas and we've given this whole back to the community.
00:04:56 [W] Not only that, of course, we use it throughout all our products just about every product that Microsoft uses machine learning and some way from commercial to Enterprise.
00:05:05 [W] And so on and we're really excited when we talk about these things that you know, now you can build an entire slide deck just using you know verbal commands.
00:05:14 [W] That's those are the kind of things that machine learning unlocks.
00:05:20 [W] Here you can see already that that you're seeing better than human performance in a bunch of different areas and we've given this all back to the community.
00:05:25 [W] Not only that. Of course, we use it throughout all our products just about every product at Microsoft uses machine learning in some way from commercial to Enterprise and so on and we're really excited when we talk about these things that
00:05:26 [W] Looking at machine learning you see things like this. And and one of the biggest reasons for it is what you see here where you know, there's so much focus on the next big model A GP T 3 comes out and everyone talks about how this
00:05:40 [W] Breaking, but when you go and look at what it actually takes to build something like that.
00:05:49 [W] It's a very complicated pipeline like this.
00:05:53 [W] Now you're saying well, I'm a data scientist.
00:05:53 [W] I don't really care about that.
00:05:56 [W] All I do want to care about is building a model. But the fact is you do care about and the reason you care about it is things like this you can go and build the best model in the world and and use it and it's better than human performance across the board.
00:06:09 [W] Rolling that out to production. That's where you get into big issues.
00:06:15 [W] And so that's where the practice of something like MLS comes up where you have a you you bring people into the overall process of machine learning on one side and you allow those data
00:06:30 [W] you bring people into the overall process of machine learning on one side and you allow those data scientists to do a whole bunch of activity there at the beginning iterating very very quickly around their overall Loop,
00:06:37 [W] activity there at the beginning iterating very very quickly around their overall Loop, but then when it comes to actually rolling it out there able to bring it into your standard gitops pipeline where you're doing development and operations and things like that
00:06:47 [W] And operations and things like that.
00:06:47 [W] Now you're saying wasn't this supposed to be a talk about security and the ways that machine learning models are better there.
00:06:56 [W] It has absolutely was because mlperf is the Baseline for security.
00:07:00 [W] Now you're saying well, it's just math.
00:07:04 [W] How bad could it be?
00:07:08 [W] We're going to talk about three different types of attacks today most of which if you're using machine learning or already, you know on your mind and those three tacks are how you know, your attacker gets your mlperf you in some way.
00:07:20 [W] Again, your attacker takes your models and then third your attacker finds out about hidden data.
00:07:27 [W] So let's talk about these first your attacker gets your MLM online tool.
00:07:43 [W] I you we're going to go through this with a very quick thing and let's say you've built a machine learning pipe line. That looks something like this first you start with
00:07:47 [W] Second you talk about engineering and splitting your data.
00:07:54 [W] That's where you take your data and you split it into your training and your test sets and so on then you are going to bring that together for your overall training experience.
00:08:06 [W] And then finally you serve now. This is a very very simple platform.
00:08:10 [W] I'm going to build a very complicated model here called a circle detector.
00:08:16 [W] so the first thing I'm going to go do is collect a whole bunch of circles and put that into my training and test data and so on and then
00:08:21 [W] Then I take a picture of a circle and I present it to my model and I say well did it work and the answer is yes, of course. It did.
00:08:26 [W] Look it's a circle obviously, but then I take it and I present another picture.
00:08:34 [W] And in this case I present a square like well, obviously that's going to fail but it still says it's a circle you're like well what is going on there?
00:08:44 [W] This one's blue and round and this one over here is square of got right angles. What's going on? Well, it turns out that the problem is that
00:08:48 [W] Inside that square you have all the pixels that represents a circle.
00:08:57 [W] And so your machine learning model says this is good enough.
00:09:00 [W] I know what I'm looking for and it works and the problem here is that I didn't do enough presentation of both good examples of machine learning models and bad examples, the two are obviously different and I need to explain to the machine learning
00:09:12 [W] So they might say well, you know, surely Advanced models are better.
00:09:20 [W] This is a phenomenal paper.
00:09:21 [W] And by the way all the papers that you see here are going to be listed at the end.
00:09:26 [W] So don't feel like you have to write them all down at once. But in this case they do this analysis here you have wolves vs.
00:09:32 [W] Huskies and the model came out really, well only one mistake, but then when you actually begin to look at it and use some of the great tools out there, I believe this was lime to explore
00:09:43 [W] or exactly what's going wrong.
00:09:50 [W] It turns out that the things that the pixels that made the difference for Huskies.
00:09:53 [W] You can see there it it looks pretty close to the actual dog itself.
00:09:58 [W] But when you look at the things that were predicted as wolves the pixels that look like wolves are actually totally excluded.
00:10:04 [W] they weren't important at all.
00:10:11 [W] So what happened the here is it looks like it's actually just a snow detector. Most of those pictures ended up being on snow and so if there was no detected
00:10:16 [W] Picture the machine learning model detected that and said it was a wolf.
00:10:18 [W] Now you're like well, okay, how bad could that be?
00:10:20 [W] here? You have where things start to get adversarial again another paper on the left hand side.
00:10:25 [W] you take a stop sign and you apply stickers on it and it tells the machine learning model that it's a speed limit sign.
00:10:34 [W] Again, who cares so you're able to fool a model?
00:10:50 [W] Well when you start to depend on models very deeply.
00:10:52 [W] that's where things get really bad.
00:10:55 [W] So here for example Amazon's face recognition system went through and detected against members of Congress and found that 28 of them matched mug shots, and and even if you wanted to opt out of
00:11:07 [W] I can't and again at airports and things like that.
00:11:13 [W] They're starting to implement these as the first line of defense. So are you terrified yet?
00:11:24 [W] What you need to do in your machine learning model to defend against those adversaries is use something like kubeflow and Emma waafs to build a rich Pipeline and test at every possible place you can you add more
00:11:31 [W] There or look for detecting bad data, you build better evaluation metrics.
00:11:40 [W] It's not just enough to say that this was detected as a circle.
00:11:46 [W] It's also important to say this was detected as not a circle is square was detected as not a circle and fold in additional models.
00:11:49 [W] You should always be attacking your own models first.
00:11:50 [W] I cannot stress enough you having a red team as part of your MLS team or your MLT, Ms.
00:11:55 [W] Critical and building a lot of alerting and monitoring and then finally taking all that data and
00:12:02 [W] dang it back and moving it into production are assuming moving it into your overall training set and is incredibly important continually training on those kind of things.
00:12:13 [W] So, how do you do that?
00:12:15 [W] You build an ml off Sidelines and building. The pipeline is pretty straightforward you use a lot of the ci/cd systems that you already have today something like you have actions Jenkins and so on there are many many of them you add modular components as it makes sense based on the criteria
00:12:28 [W] There's a lot of different tools out there for falling on these things and we have some pre-built one's for you at mlperf github.com and third just measure measure measure and continually update.
00:12:41 [W] I cannot stress enough static models don't last and we're going to show how this gets much worse in in just a little bit.
00:12:48 [W] So a standard pipeline that you might build look something like this you start over here with your IDE and this case.
00:12:57 [W] maybe you're starting with Visual Studio code you check that into your Source repository system and this case GitHub and then that'll kick off your ci/cd system and inside your ci/cd system.
00:13:10 [W] You're going to set up things where there are certain elements that execute on the ci/cd system itself in this case something like processing your data that might be something trivial like just copying between buckets.
00:13:19 [W] But then you might have larger jobs that executes externally and so in this case, maybe you're going to execute against spark to do your feature engineering when that's done. It calls back into the ci/cd system and loodse that it's done.
00:13:32 [W] you do the same thing in this case.
00:13:33 [W] We're going to train using kubeflow and kubeflow pipelines you train you finalize those those jobs and you call back in then you package and that just might be putting it into a container, you know, attaching flask whatever it might be including the production libraries and then you roll it out for
00:13:47 [W] In this case on something like it as machine learning. So all of that together shows you how to build a complex system where each one of those components is quite modular and you're able to swap in whatever you like.
00:14:03 [W] Now, one of the most important things is storing the metadata about each of these steps every input execution and output that's involved. You want to do that in an immutable store and they're obviously hundreds of those out there that you can use
00:14:16 [W] Him then because you built this in this very modular way and let's say I want to swap in an explainer.
00:14:22 [W] It's very trivial.
00:14:29 [W] You're just say, oh, you know between training and packaging I want to drop in this explainer explainers our service or explain to library.
00:14:29 [W] Whatever makes sense you call it out and then it calls back into the packaging step once it's passed.
00:14:35 [W] So again quite sophisticated but nothing so complicated that you wouldn't be able to pick it up and and and piece it together yourself.
00:14:42 [W] So that's getting your model to lie to you.
00:14:48 [W] And I you know, I hope I've shown the the MLS is critical to that.
00:14:51 [W] Let's talk about another attack Vector where an attacker is able to steal your models.
00:15:01 [W] So in this case, you know malicious users are constantly going to be trying to take your models. Most of the most sophisticated models required many many millions of dollars to train hundreds and thousands of
00:15:09 [W] Hours and things like that.
00:15:20 [W] I think the figure that I saw recently 4G P T3 was about four point six million dollars to train so really non trivial stuff and that's for just large models.
00:15:24 [W] you have something that's particularly proprietary industry-specific. You're really going to need to be understand exactly what's going on.
00:15:31 [W] What we're going to talk about today is two types of attacks one is called a distillation attack and you can see the paper there and the second is called a model extraction tag. Did it distillation that attack is more generic one and I'll show you how it works.
00:15:46 [W] And then the model extraction pack is is more specific but it's in some of the most valuable model specifically around some of the language. So for a distillation of Tak what you're looking at is something like this you start and you say well, you know, here's a black box model.
00:15:57 [W] model. I don't know what's inside it. I'm just going to start probing it using these samples.
00:16:06 [W] First I start and I get a response this heart and it says no I detect nothing in it.
00:16:13 [W] Then I get something that's a pentagon and I say yes, there's something inside that and then I present it with a whole bunch of examples again a whole bunch of responses and I presented with even more examples and I start to really formulate what is inside that
00:16:20 [W] Being out exactly what's going on.
00:16:26 [W] Anyone want to guess what the model is trying to detect are it's try to detect Nina Simone obviously now it is in fact trying to detect a triangle and once I'm able to do that once I'm able to probe it enough to get the answers.
00:16:38 [W] I'm able to pull out and duplicate that original model and now all of those people can come and attach a use my duplicated model instead of the original now, there are a lot of other vectors here not just for usage, right?
00:16:54 [W] Could use it for more sophisticated attacks.
00:16:59 [W] I could use it to try and reverse engineer the data that went in to train the model there. Lots of other things involved here, but it's you know, it's almost impossible to defend the in something like this.
00:17:08 [W] Now. The question is how accurate is that typically in a model Well turns out that you're able to get accuracy really quickly in this case to reverse-engineer Amazon's logistic regression model for looking at either digits or adults.
00:17:19 [W] It was less than fifteen hundred and four big mlperf.
00:17:21 [W] I was decision tree.
00:17:24 [W] I was able to get it in under under 4,000.
00:17:27 [W] That's roughly speaking, you know about to a minute for two days.
00:17:34 [W] That's almost impossible to detect from noise.
00:17:37 [W] Now, that's just for the distillation model but for more sophisticated models something especially around language models, you might use an attack Vector like this. So one of the biggest advancements in machine learning models recently came from Google
00:17:50 [W] Their birth Transformers and the Bert really kicked off an entire new spread of machine learning models and understanding especially in language.
00:18:06 [W] These are the Azure cognitive services and we use derivations of some of that original research ours is have done additional training and other things like that, but the core logic that started all this comes from the same.
00:18:17 [W] And and the way you are able to attack this is first by understanding how the model works.
00:18:24 [W] So what happens is you train on a corpus of information and in that Corpus of information you start asking questions and you say how many instruments did Prince play you say 27 and it's able to detect that out of this large body of Wikipedia
00:18:36 [W] Now the way it's able to do that is by looking through and teasing apart all the sentences and figuring out like what a good responses and so it does involve a lot of training data and things like that up front.
00:18:51 [W] But once that work is done.
00:18:54 [W] All the information is in the model itself.
00:18:57 [W] And so you're able to probe in a completely different way in this case you're able to probe in one of two different ways one is you just attack it with random information. So in this case I say I just spit a way
00:19:07 [W] Random set of words against it how Workforce stop who knew of Jordan Atwood displayed the obviously a meaningless sentence, but the model doesn't understand that it's going to do its best to respond.
00:19:22 [W] And in this case, it says his singing abilities and compassed a wide range which came from the original Corpus of data.
00:19:32 [W] And so you're able to start saying well jeez, you know, if I say this word over here, then it's able to present that word over in this sentence over there or if I know what the original Corpus of data is in this case may be something like Wikipedia.
00:19:40 [W] I can just grab a random bag of words from that and apply it to the original. And in this case you're able to do it and still get a response even though that sentence is meaningless.
00:19:49 [W] But because it came from that original Corpus of data I'm able to do it much more quickly and to show you exactly how quickly it is.
00:19:59 [W] With just one tenth the total number of original queries.
00:20:22 [W] And if I do one time the original amount of queries, I'm able to get the 86% and I continue our continued able to continue to pursue that here, you know, I get all the way up to 10x meaning like I probe it ten times as many times as the original model was
00:20:26 [W] 90% almost exactly the same level of accuracy.
00:20:30 [W] Obviously this isn't great because I'm able to do that. Incredibly cheaply. We talked about 4.6 million dollars before here. I'm able to replicate all the models that you see here at a high level of accuracy for under two grand and what this really is all about.
00:20:45 [W] is is this middle point that you see here you can try and defend against it but the value is in the model or to me in the pipeline.
00:20:54 [W] not the model you want to do continuous retraining you want to do continuous improvements around your domain.
00:21:00 [W] Fine tuning faster throughput because your data will always be changing spending your time focusing on how to defend against it is probably not going to work what really is not going to be defended against is your ability to process data and
00:21:15 [W] you want to do continuous improvements around your domain fine-tuning faster throughput because your data will always be changing spending your time focusing on how to defend against it is probably not going to work
00:21:17 [W] And so, you know in short and spend the majority of your time engineering time on that left hand side using tools like get of actions, like kubeflow pipelines like so on building really efficient pipelines for this rather than on the right hand side again, you don't want to
00:21:32 [W] Building really efficient pipelines for this rather than on the right hand side again.
00:21:33 [W] You don't want to throw open the doors.
00:21:35 [W] you want to make it at least trivially hard to take but spending a lot of time on the right hand side will be will be unfortunately a Fool's errand.
00:21:41 [W] So that's that's a when the attackers are able to attack take your models.
00:21:48 [W] Now, let's find out about hidden data.
00:21:51 [W] So hidden data is a problem.
00:21:56 [W] You're probably already having we talk about data leakage all the time. In this case. What a malicious user wants to do is find out about the data you trained on so they're going to probe your model and detect in understanding about
00:22:07 [W] It's so hidden datadog Legion leakage example these already happened today. You know, let's say I'm in recommendations and you know it say where I should potentially go for my next meeting this
00:22:23 [W] Viewing something about historical events that it was able to detect maybe it's able to look through my network graph and this case.
00:22:38 [W] I don't follow these people my friends do and again this might not be public data necessarily arcs and mean I never be private knative necessarily, but it's certainly not surfaced in a way that that is as easily consumable as this
00:22:47 [W] I'm you know going out for a run and I pop up in my mapping app and it's a you know, where should I go for a run?
00:22:55 [W] And in this case? It's revealing where the community runs in the area.
00:23:02 [W] And then unfortunately that gives the community an entire map of you know, private buildings again. These are very common data leakage things certainly not the only ones and certainly not restricted to mlperf.
00:23:15 [W] So here we are in you know, every presentation needs an XKCD as an example.
00:23:25 [W] And in this case, you know, I'm in Gmail and an able to probe against the suggestion in point and so it starts by saying let the ruling classes and it fills in the rest of the information for me. And that's this is obviously something that is
00:23:38 [W] Being in useful to me, but if an attacker is able to get access to that they're now able to probe against my private Corpus of information and that is my private emails and my private correspondence and private documents that I've written.
00:23:54 [W] Now again those that's just one suggestion. It can get much much worse, you know in each of these cases.
00:24:05 [W] Maybe I'm just presenting on the left-hand side. I say my shipping address and then they the suggestion fills in the right or calling it and then it gets really bad down here where you start talking about things like credit cards and Social Security numbers where I'm able to present
00:24:17 [W] The first part of a social security area and credit card or social security number which are public information for 128 is a known starter for Visa. 262 is the starter for any social security number for someone born
00:24:32 [W] Actually comes from a small range of like 2612 267 but you get the idea that public information by doing that it's able to push the model and and suggest the rest which is private information.
00:24:48 [W] Now, there's a paper that came out to talk about how to start preventing this they very very creative ideas here where you basically automatically inject canaries, and when you do that injection of a canary, then you're able to detect
00:25:01 [W] Probing on the first part of the canary and seeing if it predicts the latter half.
00:25:15 [W] Now the bad news here is that it's only predicting or it's only detecting if you're already leaking. It is not preventing the leakage.
00:25:16 [W] that's still up to you and a sophisticated mlperf line to detect to fail when you see leakage and to move forward and there aren't we are getting better about this. There are things like differential privacy which are going to offer ways to you
00:25:30 [W] Close the predictions.
00:25:32 [W] But unfortunately, the reality is that some data will leak and the reason it will leak is because if it's doing its job, right, it sounds exactly like you it suggests routes that make sense to you it suggests.
00:25:49 [W] Meetings that you went to it suggests Network graphs that feel like real human beings.
00:25:57 [W] The problem is is you're really going to have to lock it down over time.
00:26:02 [W] But the number one thing you can do is build a pipeline that helps you understand exposure and react quickly so that the moment you are able to detect that Lee could you stop it from going into production you implement additional tools to strip out that information
00:26:12 [W] It's appropriate. It is completely appropriate not the launch if you feel like look this just isn't something that we can defend against.
00:26:21 [W] So in summary analogs gives you a lot of solutions here around building pipelines that you're going to need no matter what but especially it gives you the ability to detect to react quickly and to
00:26:36 [W] Especially from a security standpoint now, it's not free.
00:26:48 [W] There is some human and software work but tools like GitHub actions and kubeflow pipelines and other tools of that kind are designed specifically to make it much much easier than it ever has been before and the reality is we really are going to need to do this
00:26:56 [W] Learning models and everything like that will touch every person in this world in some way.
00:27:04 [W] I challenge you to open your your phone right now and look at any app that you have on your phone screen or your your home screen and and show me that it doesn't use machine learning in some way.
00:27:14 [W] It's going to touch everything but we can't ask the experts in those fields to go and and understand all the complexities that I laid out for you right here what it's really going to take is smart.
00:27:26 [W] It is people watching this the people normally I would say in this room, but you're obviously not in this room to go and like understand this and Empower them with the tools necessary to inform you and to work collaboratively around things like this.
00:27:40 [W] What did the truths you cannot avoid?
00:27:44 [W] will be attacked the basics of security don't change just because it's mlperf pipeline will have issues and the game is all about how quickly you can recover from those issues and mitigate the harms when you detect them.
00:27:58 [W] And as promised as I said every paper that you see here is listed below you are more than welcome to take your screen shots now or if you'd like. You can just reach out.
00:28:12 [W] I am always ready to talk about this kind of stuff.
00:28:19 [W] Exit he's that I laid out for you right here. What it's really going to take is smart people is people watching this that people normally I would say in this room, but you're obviously not in this room to go and like understand this and Empower them with the tools necessary
00:28:27 [W] Hi all I think you can hear me.
00:28:41 [W] I'm here to answer any questions you have.
00:28:46 [W] We have a couple of questions in the sorry.
00:28:51 [W] I think I am using my phone.
00:28:54 [W] We have a couple of questions in the thread. Oh, sorry.
00:28:56 [W] Do I think testing for model robustness will be incorporated into mlperf?
00:29:07 [W] Ml Ops offerings including kubeflow?
00:29:09 [W] Yeah.
00:29:11 [W] I can zoom back in slides and fortunately but right in the middle of the slides is where the you'll see what I expect to see a very standard mlperf.
00:29:21 [W] That's where you have a series of very Loosely coupled steps.
00:29:26 [W] Driven via ci/cd system like GitHub actions and in between that is where you'll add components that help you, you know do explain ability look for security concerns and things like that being able
00:29:41 [W] Li but right in the middle of the slides is where the you'll see what I expect to see a very standard mlperf that's where you have a series of very Loosely coupled steps driven
00:29:44 [W] Very robust and flexible mlrs pipeline is critical and will make all the difference in your ability to adapt to situations like this.
00:29:51 [W] Moving down existing tool room related to mlperf. Honestly, your best bet is first making sure that you have a great mlperf line before you get any security involved your ability to quickly.
00:30:08 [W] There are things that are happening in production and and retrain very very quickly that's going to be critical for any choice that you have outside of that.
00:30:24 [W] There are a number of different explainer models. And I think explain explanation is probably your best bet to start so whether or not it's like interpretability mlperf from Azure IBM has a very
00:30:33 [W] The I toolkit and then there are a number of like independent services that are actually a projects that take tackle days on one off basis particularly on exploitability shop and lime and things like that.
00:30:48 [W] You can see many of those are called out in the papers that you see down below.
00:30:55 [W] What you'll do is you'll take those and encapsulate them into your runtime, you know webinar Nats at kubeflow pipeline whether or not it's you know, raw binary from a good of action.
00:31:03 [W] Throw it in the container, whatever it is, and then you add it to your overall pipeline that will help you but you need that pipeline in place first. Otherwise, you know, everything will be much more complicated.
00:31:15 [W] Next question if I had a number of cached predictions from an MLP Pliny and I discovered malformed data from an attack.
00:31:26 [W] How would you re process those predictions within the kubeflow ecosystem.
00:31:31 [W] So again, you know, I know I sound like a broken record.
00:31:34 [W] The first thing is to have that pipeline.
00:31:41 [W] It's incredibly important that you're able to declaratively kick off that retraining even if it's just on the existing data that's going to be a good start.
00:31:45 [W] after that, what you want to do is, you know set up your detectors for that malformed data as it comes in Via an attack and and make sure that you're getting the right signal to noise because the difference between
00:32:00 [W] You know a malformed data that represents an attack and just about form data because it's the internet or you know, it's all huge data set or something that can be very very small your often times going to run into extremely high noise signals and alerts
00:32:16 [W] Signal to noise ratios. So making sure you have that Tooling in place, but assuming that it's all correct.
00:32:26 [W] Your next step is to go back to your model look at your data processing and your feature engineering tools, make sure to exclude them or properly represent them so that you are able to make sure to capture
00:32:39 [W] Back to your model look at your data processing and your feature engineering tools, make sure to exclude them or properly represent them so that you are able to make sure to capture that as being bad results.
00:32:43 [W] Being bad results the second thing that you can do certainly is feed in those malformed data and label them. So as you bring in that mouth from knative make sure it says hey, this is bad.
00:32:53 [W] I'm going to give it a bad result not bad result and no result or say like I'm this is a negative for whatever it was trying to detect for so that's the second thing and that's very very powerful because that means for better or worse. It is much less likely that the same
00:33:07 [W] work again because you've now labeled it the third which is a very very powerful technique is doing doing things where it's you're closer to doing restricted results and things like one hot training and things
00:33:22 [W] You don't judge and give things a gradient like this is you know, 71% cat and 31% dog.
00:33:35 [W] I know those add up to a hundred percent, but that's how a lot of these things where it can basically says the accuracy that of that prediction being a cat or a dog and you could have you know, whatever 50% tiger or something like that right?
00:33:44 [W] Like I'll have an entire array of them instead of giving those results back you instead categorize them forcibly categorize them into buckets and then judge based on that and use that as
00:33:53 [W] as your success metric or your objective function or things like that.
00:33:57 [W] So that's another technique.
00:34:00 [W] that's not always possible because if you look at things where you have generative language, or you're responding on large Corpus of data, you may have not you may not have the ability to kind of like arbitrarily Slaughters swap things, but that wouldn't stop
00:34:13 [W] That and use that as your success metric or your objective function or things like that.
00:34:14 [W] So that's another technique.
00:34:14 [W] that's not always possible because if you look at things where you have generative language, or you're responding on large Corpus of data, you may have not you may not have the ability to kind of like arbitrarily swap swap things, but that wouldn't stop
00:34:16 [W] There there's no chance something. That's it makes it much less likely that a stop sign is now categorized as speed limit fine, but you know all of those tools, you know, they really are no substitute for a very very
00:34:28 [W] pipeline where you're just continually retraining and that's really what kubeflow is designed to do and and in conjunction not just kubeflow, but all the other pipeline Tools around
00:34:39 [W] how does mlperf Scamp are two continuous machine learning?
00:34:45 [W] I'll be honest.
00:34:46 [W] I haven't dubbed Doven Dove deep and continuous machine learning.
00:34:53 [W] I'm not that familiar with it that the tools think of kubeflow and to for pipelines as an implementation of a general idea, which is you know, largely why you're here, right?
00:35:06 [W] This is all about Cloud native Technologies and using them together and and the essence.
00:35:09 [W] Building a sophisticated mlperf line is having loose services that are coupled together micro service oriented and making sure to have clean contracts between them and by that. I mean you finish your data processing you
00:35:24 [W] Of metadata that says these are the statistics for this is the number of features.
00:35:32 [W] This is the the spread for each feature.
00:35:33 [W] For example one example, I like to give a lot, you know, if you had an age column, for example, you want to have a defined schema that says hey it is not allowed for an age to be larger than whatever a hundred and fifty or
00:35:47 [W] Even you know that since those are obviously nonsensical results forcing your data into categories like that and having schemas will allow you to unit testing over things against that data and and then, you know, obviously moving on all the way down the line.
00:36:03 [W] I haven't checked out continuous machine learning but you know anything that helps you build a sophisticated Loosely coupled pipeline like that, you know is a great first step do
00:36:19 [W] Um, you know is a great first step.
00:36:19 [W] Do is for see more regulations in the future around the use of MMOs related to that gbbr and data itself.
00:36:30 [W] I do unfortunately, I think that a lot of there's a lot of political energy and things like that that are focused really on the wrong thing. A lot of this stuff right now is unfortunately very,
00:36:40 [W] Hey, it just the words the things like responsibly I and ethical II and things like that get a lot of attention and and rightly. So right machine learning is a deep opportunity to like
00:36:57 [W] Is is that that we've had for years and years we just are right, but the reality is is that your code was probably already doing this ahead of time your code your systems everything. That was
00:37:13 [W] He biased and that's not good. Right like the problem is that with with machine learning you now tools that will allow you to detect them more than ever.
00:37:28 [W] But, you know, we don't have the tools in the past and arguably machine learning, you know, we'll make up a small percent of all applications.
00:37:37 [W] They'll be everywhere but only a small percent. So you're going to have a lot of focus on machine learning and MLM models and ethical Ai and all that kind of
00:37:44 [W] Good stuff, and that is a really big positive but make no mistake that all that attention really should be applied to the industry as a whole.
00:37:58 [W] I mean, you know people were doing bias and racist and and other things long before the Advent of MLS and it's up to us as an industry to be responsive to that rather than just do the focus on mlperf
00:38:07 [W] Really should be applied to the industry as a whole. I mean, you know people were doing bias and racist and and other things long before the Advent of MLS and it's up to us as an industry to be responsive to that rather than just
00:38:09 [W] Definitely think there will be that kind of system or those kind of regulations in place continuing through how much are these attacks mitigated using
00:38:23 [W] Another algorithmic privacy techniques.
00:38:28 [W] I noticed one paper mention Federal earning look, you know, if you look through here, there's a paper that shows how Federated learning actually doesn't help and it made many ways.
00:38:40 [W] It makes it harder because now the central system can't look for specific violations.
00:38:45 [W] It can only look for aggregated violations and things like that.
00:38:48 [W] And so you could actually have an attacker submit that data that the central system can detect them.
00:38:54 [W] Push down that model everywhere and and have you know an attack up here on everyone's phones simultaneously, which is obviously a nightmare scenario.
00:39:06 [W] The reality is I don't think that that the things that they help for those that don't want large companies to see data basically and that's a very real concern and I think privacy is very important for from that perspective
00:39:17 [W] Said the the I think it these techniques will help Federated learning differential privacy other things a homomorphic encryption.
00:39:31 [W] These all will help in a variety of ways, but they should absolutely not be looked at as panaceas without a pipeline and you actively working to detect there is no way out.
00:39:43 [W] Where do we go to learn about mitigating leaky data?
00:39:50 [W] There are a ton of papers out there on this stuff. You know, my best advice for you is go and hire a red team red team is a team that goes and attacks your systems yourself mlperf.
00:40:03 [W] And especially the generative mlperf that you saw here is a very very small microcosm of data linking generally, you know almost every day and and by that, I don't mean people going and hacking your
00:40:19 [W] You know almost every day and and by that I don't mean people going and hacking your S3 bucket and downloading a bunch of stuff.
00:40:24 [W] I mean your system is operating exactly as intended and yet still you are leaking private corporate information out the side door.
00:40:34 [W] There was an amazing research project maybe about 10 years ago. AOL released a whole set of web queries about users.
00:40:44 [W] And the anonymize them they took all information off and yet security researchers were able to go and and peas apart.
00:40:58 [W] They basically group The queries by individual but they took out all the private information but security researchers were able to go and find the non in significant number of people like literally go and find their address because they would say oh, you know, you know dog
00:41:08 [W] We're able to go and and peas apart.
00:41:09 [W] They basically group The queries by individual but they took out all the private information, but security researchers were able to go and find the non insignificant number of people like literally go and find their address because they would say oh, you know, you know dog
00:41:10 [W] The Upper East Side and you know, you know how to mount a satellite dish on a you know, balcony or whatever rate and piecing that together.
00:41:24 [W] Let them go and ultimately draw, you know figure out exactly what this person was looking for and that was years and years ago that long before, you know, you saw a lot of the mlperf here today.
00:41:34 [W] Basically, there's no better tool than the human mind for going after this right now and going out and finding
00:41:39 [W] folks to think about all the private information you have and begin attacking it yourself is an extremely powerful concept.
00:41:54 [W] Like I said during the talk, the biggest issue here is not unintentional leakage.
00:42:01 [W] The biggest issue is that if the system is doing the right job, it will leak because it's supposed to leak because it's supposed to sound like you are supposed to give you very rarely.
00:42:09 [W] Levin information where it's supposed to, you know, let you have access to, you know an email you wrote seven years ago in an elegant elegantly presented way,
00:42:24 [W] That's kind of the situation right now. And and you know, the reality is that a lot of this comes down to a balance that we agree to as an industry as a society as all these things.
00:42:40 [W] How do we lock that app down so that just you have access to it or just you and your friends. But but as we release it to your friends your friends you have control over exactly what they can see and have real granularity and
00:42:53 [W] About it's a very very complicated topic the examples that I gave here are basically because it's in ml in particular.
00:43:04 [W] I think it's easier to leak things that it is in a lot of other spaces because you have a large Corpus of data that goes into a model and then is totally obvious cated you have no idea what's going on and then at the other end, you know, you get
00:43:17 [W] Results and that can be really easy to like Overlook some of the papers here give you a nice way to go and attack your own models and figure out exactly what you are leaking.
00:43:28 [W] But again, there's no easy answer.
00:43:29 [W] And with that I think I am at time.
00:43:34 [W] All right.
00:43:39 [W] Well, thank you all so much for coming. I really appreciate it.
