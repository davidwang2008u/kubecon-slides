eBPF and Kubernetes: Little Helper Minions for Scaling Microservices: PGMF-0136 - events@cncf.io - Wednesday, August 19, 2020 8:20 AM - 118 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:19 [W] That is where a ppf came from how its developed today and what it brings for the future and how it relates to kubernative and taking one example here of how it can scale kubernative Services.
00:09:24 [W] I'm doing the workmen and I'm one of the two EPF Co maintainers in the Linux kernel. So let's Dive Right In communities is basically eating the world today more and more Enterprises are moving over to
00:09:38 [W] And basically kubernative is regarded as D cloudevents these days and underneath all of that.
00:09:49 [W] are still Linux kernel as a base Foundation to provide all the building blocks for kubernative itself, but also for the containers in terms of for example, cgroup sand it with namespaces.
00:09:58 [W] in some of the
00:10:00 [W] core critical parts for those deployments. They're delivered through a so-called seen I networking plug-in if we take the networking angle as an example.
00:10:16 [W] So what does this scene I plug in in kubernative do basically it provides general part connected iot and that means when kubernative is basically spawning a new part that's in I plug in is then
00:10:27 [W] Network devices moving them into the new network name spaces and also assigning IP addresses routing and so on basically it manages even several IP addresses and IP address pool.
00:10:42 [W] And some of the cni-genie Atkins also take over kubernative service handling and that they provide load balancing for services. And as well as Network policies
00:11:00 [W] Says and as well as Network policies given all that can be quite complex.
00:11:05 [W] They also offer monitoring troubleshooting on top of all that and today we are seeing to clear Trends one is the part density is increasing
00:11:18 [W] Trends one is the part density is increasing for a given kubernative note.
00:11:21 [W] That means people are running more and more parts than the signal and the other one is decreasing the part lifespan even down to seconds or fraction of seconds for their workloads.
00:11:35 [W] There are some interesting user survey reports that also backup this data.
00:11:46 [W] There are challenges from the operating system kernel side, of course with that because the Linux kernel or general the colonel has to
00:11:51 [W] keep up with all the performance in scallop scalability requirements that users have entered data center as well as the increasing complexity of Colonel subsystems one issue with that
00:12:06 [W] Those kernels not allowed to practice space.
00:12:22 [W] That's of course a good thing. But that also means that we have to keep all the old Legacy from five or ten years ago around and keep it functioning so that we keep applications working and that of course.
00:12:25 [W] The colonel more complex and at the can affect the fast path as well.
00:12:30 [W] It means that the kernel often suffers a feature creeping. You can call it feature creeping no matter T. What does it mean?
00:12:44 [W] well users at developers add more and more features to the finops colonel. It becomes more complex and nobody's potentially able to understand all of those.
00:12:54 [W] There's actually an interesting definition on creeping the melty on Wikipedia even and it's defined as a process by which a major change.
00:13:01 [W] that would otherwise be rejected when proposed as is can be accepted as normal or seen as acceptable if it happens too small and incremental unnoticeable changes
00:13:18 [W] Stating quote on that from Linux tutorials on crazy new features for the colonel.
00:13:27 [W] He says he can work with crazy people.
00:13:34 [W] It's not a problem, but they need to sell their crazy stuff with non-crazy rational arguments in small well-defined pieces.
00:13:44 [W] So when he's asking for killer features, he wants actually to he wants actually those developers to login into safe and cozy world.
00:13:50 [W] Where the stuff that they are pushing is useful for core kernel developers first, in other words, like those crazy features should be wrapped as a trojan horse.
00:14:00 [W] They should look Obviously good at First Sight and that's pretty much what happened to EBP f of x in the days when you first get merged.
00:14:15 [W] So it's a crazy new kernel feature. But in the long term for the Linux kernel, it actually reduces the Colonel's feature creeping know,
00:14:19 [W] T because all the weird Corner cases that uses half and ask from the colonel instead of having them hard coded in the kernel and potentially in the Colonel's fast path. They can instead.
00:14:34 [W] lift inside and EPF program where no one else has to suffer from potential performance degradations meaning it allows to keep the Colonel's first / first
00:14:50 [W] It allows us to make the colonel fully and safely programmable.
00:14:58 [W] So users won't crash the kernel and EPF has been designed to solve real-world production issues and it still is with that in mind today.
00:15:08 [W] How does it look like EP F is defined?
00:15:16 [W] Okay be the can be written in C like language and that can for example either be generated or managed by another user space application or agent in our case here
00:15:26 [W] As one example, so there mr. Cooper Nettie's seen I what I mentioned earlier and that program gets compiled through a compiler in that case the llvm compiler suite and it can generate the ppf
00:15:42 [W] File is being handled by a BPF loader where it creates all the necessary steps and pieces to loaded into the kernel.
00:15:53 [W] Those BPF instructions that have been generated and they are passing inside the kernel trilio BPF verifier, which makes sure that for example memory access is not happening out of bounds. You cannot crash it
00:16:09 [W] Or all these sort of things and once it determined that the program is safe.
00:16:16 [W] It will be just in time compiled in order to generate native code that can be run on the underlying architecture.
00:16:24 [W] For example x86
00:16:26 [W] And in order to share State DPF programs can use so-called BPF Maps. Those are shared storage that is accessible from BPF program site, but also from the user space site.
00:16:40 [W] And BPF needs to be triggered to an event and in case of networking, which is the example here that the event is of course Network packet.
00:16:56 [W] So Network packet basically triggers the execution of the ppf program. The BPF program can mangle the packet that can forward it and in the end it returns a verdict so aside from forwarding what is also supported
00:17:05 [W] Case of networking which is the example here that the event is of course Network packet.
00:17:06 [W] So Network packet basically triggers the execution of the PDF program. The ppf program can mangle the packet that can forward it and in the end it returns a verdict so aside from forwarding what is also
00:17:07 [W] Pushing it up just a court and so on NT crucial part in all of that is again that it cannot crash the colonel it is as fast as a kernel module and it provides stable API guarantee. So what does that mean
00:17:21 [W] When the ppf program keeps working on an old kernel version it will also continue to keep working on a new kernel version.
00:17:31 [W] Without having to change the program itself.
00:17:36 [W] It's similar to use this base like the contract that the colonel has to use a space where we don't brake applications and Colonel updates in this similar things happening for the EPF world for
00:17:49 [W] in case
00:17:51 [W] how does the packet will flow look like in the traditional case of kubernative swear?
00:18:02 [W] It's using the cube proxy component in order to handle kubernative Services what you can see here.
00:18:07 [W] Is that packets coming from a Nick or entering the stack?
00:18:14 [W] The Linux kernel is creating a packet representation called SKB then it's like pushing this the packet through the
00:18:21 [W] Traffic control Ingress hook and then all these orange boxes are netfilter IP tables related.
00:18:34 [W] So there's pre routing hooks. Then there's the actual routing look up. It determines that the packet comes to be forwarded to a different networking device than their does post routing hooks again, TC egress one and then it can determine that the program
00:18:42 [W] Our netfilter IP tables related so there's pre routing hooks.
00:18:44 [W] Then there's actually routing look up. It determines that the packet comes to be forwarded to a different networking device in their does post routing hooks again, TC egress one and then it can determine that the program has to go to a local
00:18:45 [W] Through a local part, then it will forward it to a local we've devised for example or local service endpoint.
00:18:51 [W] Or it will go back out of the node if the packet if the packet is destined at a remote service and point on a different note can be the same network interface or a different one.
00:19:04 [W] So the same packet flow under EBP F4 example, when used with psyllium looks quite different in the sense that a lot of the overhead and orange boxes can be avoided
00:19:20 [W] in the packet from the T C inverse y to the TC goes hook and then forwarding it that way so sodium would allow with together with the ebi with its EPF data path to accomplish that and it can even go crazier than that in the sense
00:19:36 [W] Forward those packets to remote service and points at the XDP layer that XDP box basically.
00:19:44 [W] Called the express data path and it allows for running BPF programs inside the driver layer in forwarding it them directly from there without having to push it up to stick so we can save all the additional overhead and be super fast.
00:20:00 [W] So how did it look in the very early days of ppf?
00:20:08 [W] So basically we're speaking about Prix 2013.
00:20:13 [W] You can consider it as the STM lens cap for Linux at a time. If you will so back in the days there was open research project or called OBS traffic control and off the netfilter sub system consisting of for example
00:20:23 [W] You can consider it as the STM lens cap for Linux at a time. If you will so back in the days there was open V switch project or called OBS to traffic control and off the netfilter sub system consisting of for example
00:20:25 [W] Yes, and if tables was there back then and those allows you to let's say program the data path bof back then was used for tcpdump to fill the network packets inside the kernel as
00:20:39 [W] Or and without crushing it and it was also used for seccomp in order to do system call filtering, but it's used was very limited and also constraint.
00:20:50 [W] Not the same as you can as we have today with the EPF, there was also a feature creep and often also code duplication between TC and netfilter because both subsystems.
00:21:03 [W] We're sort of competing with each other and open we switch back then was considered like the most advanced data plane in the networking stack, but it didn't integrate well with networking and it was also considered as
00:21:17 [W] Sort of competing with each other and open we switched back then was considered like the most advanced data plane in the the networking stack, but it didn't integrate well with networking and it was also considered as
00:21:19 [W] By other core developers.
00:21:23 [W] So yeah. So how does it compare those subsystems still exists today?
00:21:37 [W] So how do they compare to a BPF? You can think of it this way that open we switch TC and netfilter. They allow you to program the data path. But only if the data path knows what you want to do only if those concrete building blocks are
00:21:42 [W] as coat
00:21:45 [W] and ebps instead lets you freely create the data path instead because you provide everything through BPF program.
00:21:56 [W] So the extended BPF that initial first patch set was presented by Alec system right of back in the days in 2013 as a big patch bun.
00:22:14 [W] It was like cost plenty of discussions, but it was considered too intrusive back then also NF tables was sort of under eyes.
00:22:22 [W] It was inspired by the old EPF.
00:22:25 [W] That is the datum head and people weren't quite happy.
00:22:29 [W] Because it would like that past said would add another ppf interpreter. So too would have to be maintained.
00:22:36 [W] It was sort of a hurdle eventually got rejected.
00:22:38 [W] So you can see that here that Linux Linux crazy rule a new kernel features of light that I mentioned earlier.
00:22:49 [W] You need to Chuck it up into small incremental pieces to give her safe and cozy world and in 2014.
00:23:02 [W] 2014. Basically, we reworked the whole patch set and made it more chubaofs. It also got merged into the kernel. So that's like the initial.
00:23:06 [W] We place them to Old ppf interpreter entirely.
00:23:26 [W] So the kernel wouldn't have to Old tcpdump many more but instead the new EP P F1 and the instructions had got heavily extended and the tcpdump BPF language back then got translated inside the kernel to the new.
00:23:30 [W] And later also to verify injured pieces going to edit and much more work on top of that and we basically Lexi myself basically became maintainers because we spend most of the time hacking on working on EPF and the colonel and overseeing
00:23:46 [W] in stem
00:23:49 [W] that's where it happened. Basically where David Miller at the networking Colonel maintainer applied the patch that yeah. Thanks a lot for that.
00:23:59 [W] And what else happened in that year.
00:24:00 [W] Actually, it's quite interesting 2014.
00:24:02 [W] the first kubernative patch went public.
00:24:06 [W] So in 2015 the development of EPF moved into two directions networking and tracing and a major Milestone that is still the base foundation for all the work is that they be bof back and
00:24:22 [W] Upstream llvm due to a compiler Suite so you can so that you were able to compile the C code into ppf instructions and the first patch set.
00:24:38 [W] So Lexi work mostly focused on tracing back in the day and he added the patch that you allow to attach a DPF to trace him to allow for system observability, and I was mostly focusing on networking
00:24:51 [W] Allowed to attach EPF to trace him to allow for system observability and I was mostly focusing on networking and mate TC the TC subsystem programmable to D to e
00:24:56 [W] TC the TC subsystem programmable to D to E ppf
00:24:57 [W] In order to have a networking data path that is flexible and also fast. We also made it easier luckless so that it would fast without any CPU contention and also add macstadium existing
00:25:13 [W] This hook so you have both attachment points to see all the traffic.
00:25:18 [W] Yeah, so there was plenty of other work on top of that and back in 2015. The BCC project first got announced that is a tracing front-end and used by many people today to get introspection to the system and soft production issues.
00:25:35 [W] In 2016.
00:25:39 [W] There was another major Milestone on the networking site the express datapath person and short XDP got merged and that basically allowed to attach BP often ppf programs into the drivers Ingress layer without
00:25:52 [W] Get introspection to the system and solve production issues.
00:25:53 [W] In 2016.
00:25:53 [W] There was another major Milestone on the networking site the express data path also in short XDP got merged and that basically allowed to attach BP off bro P PF programs into the drivers Ingress layer without
00:25:55 [W] To just stack and also like the first networking card came up and driver that was merged that I would allow for offloading ppf programs.
00:26:04 [W] The sodium project was first announced in 2016 as well. It was
00:26:10 [W] built for container networking and back then mainly targeted for darker because kubernative was still in the very early days notice.
00:26:22 [W] Heavily used as today and it would allow for efficient label based policy Nats X4 you would be able to create tunnel measures with VX engine 'if and basically have an e b PF based data plane whole data plane in
00:26:38 [W] Yes, and no more darker or OBS Bridge device has and so on so that could be avoided entirely.
00:26:46 [W] 2016 17 that's basically the year where EPF began taking over production environments, which is really exciting Brandon Greg from Netflix called out the Linux DPF superpowers.
00:27:01 [W] He's been working on tracing heavily on creating the necessary needed tools for the PCC projects. And he says it's compared to the dynatrace where he has like 13 years of experience EB P FS
00:27:14 [W] More and excited even though it just just in the short time stamp.
00:27:21 [W] The capabilities are staggering and Facebook announced that they are we're replacing their at that time existing IP vs. Load balancing infrastructure over and they migrated away
00:27:34 [W] Time existing IP vs.
00:27:42 [W] Load balancing infrastructure over and they migrated away from netfilter entirely to EPF and they were attaching those programs to the XDP hook in order to do layer for load balancing and Eros production and
00:27:45 [W] And since 2017 every packet that goes to facebook.com basically passes through e XT p and E PF so it really is Battle tested cloudflare basically also saw that potential
00:27:59 [W] Created from their net filter components away to EVPs as well.
00:28:10 [W] Similarly relying on exit on the xtp layer for the load balancing and DDOS mitigation and in also when fully into production in 2018, actually
00:28:16 [W] so yeah, the EPF Community grew and food that also the features in the kernel and in order to handle all this incoming kernel patch has Alexa myself and David Miller and networking maintainer.
00:28:32 [W] We sat together and basically decided that EDF has to become its own kernel subsystem.
00:28:38 [W] So since then Alexia myself, we manage our own Linux kernel trees for BPF and BPF next where all the features and fixes Lambs. We have our own mailing list and we are pushing.
00:28:49 [W] and collecting all the code that the whole Community contributes to BPF handing that over to David Miller and he pushes this to limit stalls which who merges it into the string kernel
00:29:01 [W] back in that year. We also worked on EPF for Katie less.
00:29:10 [W] What does it mean?
00:29:21 [W] So Katie less is a feature that allows to offload the TLs handling for example from openfaas L into the kernel and that allows the colonel to also gain visibility
00:29:25 [W] if programs could be attack can be attached and it can introspect to data and then enforce policy and these days actually openebs as L has it natively integrated, which is great and in the same year
00:29:41 [W] More tools could add it for allowing to introspect to Colonel about what's going on and BPF land what programs are loaded what Maps do we have and therefore BPF tool got added which is a tool that still gets extended with all the features today.
00:29:58 [W] And also in order to ease application development for users the lip UPF was added which is a sea-based library that takes over all the loading of BPF
00:30:15 [W] For users the lip EPF was added which is a sea-based library that takes over all the loading of ppf programs. So that applications don't have to deal with that because you can get quite complicated.
00:30:19 [W] In 2018 sodium 1.0 was announced and that basically first brought the ppf revolution to kubenetes networking and security.
00:30:33 [W] It was added as a cni-genie.
00:30:50 [W] A component called BTF or BPF type format was added into the kernel.
00:30:59 [W] It's basically a metadata format similar to debugging data that we have dwarf but it's significantly smaller in size and For the First Time The Colonel became self descriptive.
00:31:12 [W] What does it mean from a running kernel you can actually it ships its own data format and internal structures and you can introspect them.
00:31:19 [W] And that's basically BTF today is the base for all the future features like compiler Runs run everywhere current life patching to BPF global data handling and so on
00:31:35 [W] Noble data handling and so on are many new features being added in the all lie on the fact that we TF can provide this Rich metadata.
00:31:43 [W] In 2018, the Linux plumpers conference. Also first got the ppf micro conference and we also co-organized the networking track and it got filled almost like half the talks with BP F and X TP top except because it's an
00:32:00 [W] turn on why more people are using it and solving problems and
00:32:05 [W] Yeah, AF X TP is another socket type in Linux networking that got merged in the same year and what it basically provides as it provides zero copy for Network packets right out of the driver
00:32:20 [W] You can then push them to use the space.
00:32:33 [W] It's similar to what it offers a similar feature to what the DVD K toolkit for example does except in the case of D. BDK goes network drivers have to be written and maintained in user space and with
00:32:39 [W] Can get the same performance benefits while reusing kernel drivers so that Network device management is much much easier and all that happens at the XDP layer.
00:32:55 [W] That's why it's called a f XD P & B PF program that can be attached to it allows for steering this into the socket.
00:33:03 [W] There's also the BP filter prototype that we've worked on it and this allowed for translating IP table rules into BPF through you.
00:33:12 [W] Zermat driver, that was basically the first prototype where we can take an existing IP tables binary and generate EPF underneath without the user having to know a realize some of those
00:33:25 [W] They also for extending the kernel in other areas.
00:33:30 [W] In 2018, 19 ppf Trace first got announced next to PCC EPF traces. Basically Dosa, basically d 2 main tracing front ends for BPF Brandon.
00:33:45 [W] correct calls it dynatrace 2.0 for Linux because of this capabilities and it allows it has a easy to use front and you don't have to code C programs.
00:33:58 [W] You can simply use one liners efficiently.
00:33:58 [W] She also wrote an 880 page psych book on BPF tracing which is really exciting. The first book on first major book on ppf and he's not brain crack is also recommend you one for 2020 on his second
00:34:14 [W] Use one-liners efficiently.
00:34:15 [W] He also wrote an 880 page book on BPF tracing which is really exciting its first book on first major book on ppf and he's not brain crack is also working on new one for 2020 on his second edition of
00:34:16 [W] Performance covering large Parts with ppf tracing as well.
00:34:20 [W] In that year also the sodium 1.6 release actually appeared in for the first time.
00:34:29 [W] It was replacing the IP table-based Cube proxy component from kubernative True BPF.
00:34:35 [W] So all that service handling could be done efficiently.
00:34:36 [W] There are many other Colonel features added some of it like for the first time allowed life patching on the colonel from the BPF core infrastructure.
00:34:53 [W] It's been used to optimize the data path even more heavily in order to avoid some of the issues that
00:34:59 [W] We've ever seen from the CPU box that red balloons had to be introduced and it's also the base for several other features such as EPF trampolines and so on and if you're interested, you should look that up.
00:35:15 [W] It's really an exciting topic.
00:35:18 [W] There was also the first Invitation Only ppf conference among people ppf Colonel experts 2x2 exchange and discuss issues and features.
00:35:28 [W] And that's basically from the community side alternating with the name of the plumbers conference.
00:35:31 [W] How few Cadence the ppf back-end for GCC finally got merged as well. So both GCC and lvm, the two major compiler Suites have ppf back and support eventually.
00:35:42 [W] And the whole ppf community in features, and we are seeing a third major Direction on ppf the next networking and tracing its also ramping up on security.
00:36:02 [W] So Google Upstream there BPF LSM Linux security module support that they're using in their server Fleet.
00:36:07 [W] and
00:36:10 [W] aside from that also that was based 2019 was basically the year of all these CPU box.
00:36:26 [W] So BPF also had to protect against Spectra and that's actually quite interesting because the BPF are fire does a lot of work sometimes even more work analyzing the program's then compilers do and it's hear it even goes into analyzing the
00:36:32 [W] program paths
00:36:34 [W] then
00:36:37 [W] On the xtp land week. There was a major milestone in terms of cloud providers because AWS and Azure finally had XDP support for the networking drivers and that actually allows for easy.
00:36:53 [W] For xtp for the masses because it's super easy to just create nodes instances with accelerated networking and then you can use XD p on top of that one of those
00:37:09 [W] The benefit as well as sodium because I'm the one that ate release it added xtp based service load balancing for kubernative in order to efficiently use the CPU for networking in a better way
00:37:24 [W] With the net filter based approach for Kube proxy.
00:37:30 [W] On the colonel site many other interesting events happened here in that Year Facebook.
00:37:42 [W] For example added a BPF based TCP congestion control module that allows for experimentation for the data centers to quickly deploy and add new features for the congestion control and TCP and also
00:37:51 [W] Facebook for example added a BPF based TCP congestion control module that allows for experimentation for the data centers to quickly deploy and add new features for the congestion control and TCP and
00:37:52 [W] Yeah, Windows monitoring tools which are called system on over to Linux based on BPF. What you can see here is a tweet from the Microsoft Azure CTO. That is announcing it.
00:38:05 [W] So yeah, I like in those few years there.
00:38:11 [W] people are there was actually like second interesting to observe this tremendous change in the Linux kernel and people are calling it at one of the major OS changes in the last 50 years
00:38:25 [W] Brent Craig actually says it's one of the biggest operating system changes that he has seen as career and it's thrilling to be part of it. I can only agree with that.
00:38:35 [W] Evie bof in numbers.
00:38:48 [W] So basically the colonel Community today, if you look at how many patches have been contributed its around 5,000 pages that went into the ppf subsystem by around 350 contributors, which is quite nice to have such a large community these days they are on average
00:38:54 [W] So basically the colonel Community today, if you look at how many patches have been contributed its around 5,000 pages that went into the ppf subsystem by around 350 contributors, which is quite nice to have such a large community these days they are on
00:38:56 [W] Around 50 new emails on the ppf Kernel mailing list every day from Monday to Sunday and around for patches on the ppf colonel subsystems are applied by Alexa myself.
00:39:10 [W] There are many different program types and Maps but the interesting part is on the Linux kernel that the ppf subsystem has over 3,500 test cases usually kernel developers are not that good in writing unit tests or self test.
00:39:26 [W] But BPF is one of the most tested subsystem these days.
00:39:30 [W] And aside from Alexia myself is Colonel maintainers for BPF.
00:39:36 [W] There are also a team of six other Quarry viewers and major contributors that help with the patch load that is coming at us and they're from crunch from the companies as well in Facebook and Google.
00:39:49 [W] one of the fastest-growing subsystems in the Linux kernel
00:39:54 [W] so
00:39:58 [W] yeah, it's like a you can see it as some sort of an industry shift and you have those major adapters Facebook running it for their Lair for load balancer and you just production as well as tracing a bit basically everywhere Netflix similarly,
00:40:14 [W] So I'm to tracing site for BPF to Google you have it in running on Android phone devices, but also using it in their server Fleet for security and various other aspects cloudflare for that load balancing gitops mitigation.
00:40:29 [W] Pets cloudflare for the load balancing gitops mitigation.
00:40:32 [W] There are also a lot of large-scale Civilian users that are running it in production.
00:40:41 [W] And if you look at one of the former netfilter maintainers Rusty Russell, he actually what he actually has to say about ppf.
00:40:44 [W] It's quite interesting.
00:40:49 [W] He says like, you know iptables Performance used to be mostly good enough back then but replacing it has taken so long because it required a radically different approach.
00:40:55 [W] And it's nice to see it finally happening ppf.
00:40:59 [W] so how can you bring this EPK of Revolution to Cabinet has well basically ppf is becoming ubiquitous these days as I mentioned and if you're still relying on IP tables and you have to fiddle around in the back with large number of
00:41:15 [W] Those that are installed by Q proxy to handle Services.
00:41:25 [W] Well, there's a way out of it. You can basically replace the cube proxy component as one example would be bof.
00:41:29 [W] How is it done?
00:41:36 [W] So basically the way that sodium is handling services and the load balancing of services its architecture basically consists of two major components one is to run DPF programs at the socket
00:41:44 [W] through on DPF programs at the XDP and TC layer
00:41:49 [W] So, let's Dive Right In at the socket layer.
00:41:57 [W] This is basically for handling all the East-West traffic among so you managed nodes.
00:42:03 [W] So it's doing the service IP port to the back end translation at the socket.
00:42:09 [W] So whenever applications call connect sent message received message hooks, it will basically hand translate the actual request into the backend and the interesting thing here. Is that for this no.
00:42:22 [W] Extensive packet level Network address translation is required because at that Cisco time the packet hasn't even been created.
00:42:35 [W] So what basically the ppf program here is doing is telling the colonel that the application is actually connecting directly to the back ends.
00:42:41 [W] Whereas the application is thinking it's connecting to the service IP and the BPF program has both of these contexts and knows what to do with it and that allows for efficient service handling.
00:42:52 [W] for the East-West case
00:42:53 [W] Then there's also the north-south case where the external traffic from outside world is basically coming onto the note and that's handled through ppf programs at the XDP and TC layer.
00:43:07 [W] It's Network driver layer. And in case back ends are on a remote node.
00:43:19 [W] not on the local one.
00:43:23 [W] They have have to be pushed back out of the node again so similar as I showed you in the previous packet flow diagram and what if implemented here is basically as net handling with knative ppf and
00:43:35 [W] it s not handling with knative ppf and the functionality can be bit can basically be compiled into 4 x DP by itself as well as for the TCP layer and on the xtp layer as mentioned
00:43:44 [W] Layer as mentioned allows for high performance and low overhead load balancing. You can use those CPU cycles that you would otherwise have to use to handle the service translation in Cube proxy.
00:43:58 [W] You can use those for user workloads instead.
00:44:01 [W] That's how it should be.
00:44:08 [W] So basically the main principle here is to operate as close as possible to the socket for the East-West case in as close as possible to the driver for the nerve cells case. So how does it look in numbers we
00:44:13 [W] Did a performance comparison on XDP and EPF as well as Q proxy? So we had basically to Bare Metal nodes and we pushed from one node to another as many packets as possible with the package generator.
00:44:28 [W] So you got the load of 10 million packets per second. And we looked at how much those different systems eats OPP ippf versus Q proxy would be able to handle under this loads.
00:44:43 [W] So on those dark blue colored bars, you can see how many packets it was able to handle in the X DP and DP F case.
00:44:59 [W] We was basically they were basically zero drops in all the packets that were generated from the other node where basically being pushed out to the remote back and when the service is basically on the kubernative Note remote and
00:45:09 [W] In the TCP layer that sodium hand is able to attach as well.
00:45:18 [W] You are still able to handle around 3.5 million packets per second.
00:45:23 [W] Where is the cube proxy?
00:45:29 [W] There's still much more overhead because the net filter layer comes later in the stack then the DC layer and it was only able to handle around 2.3 million packet per second and I PV s even a slightly more overhead for
00:45:38 [W] Just a small number of service that was tested here.
00:45:45 [W] Then the IP tables one in terms of CPU capacity.
00:45:48 [W] How does it look here?
00:45:48 [W] So Viva generating a medium load to the kubernative node that is under test with the package generator and we tested it under 1 million packets per second to million packets per second and 4 million packets per second
00:46:03 [W] Here in those bars are the free CPU capacity that can be used for applications instead under XDP.
00:46:15 [W] It is the most efficient way to handle kubernative services.
00:46:19 [W] What you can see here is that the kernel spent the least overhead from all the other Solutions because the ppf program here is executed right at the driver layer without having to push it up into the stack the cube
00:46:35 [W] In the IP tables and IPv6 configuration.
00:46:41 [W] There was a lot of CPU spent to handle the software interrupt load here.
00:46:45 [W] See, how does it look EPF and kubernative in the long future so you can think of the Linux kernel becoming slow turning slowly into a ppf powered microkernel.
00:47:02 [W] That's actually a really interesting part here what you can think of in the lung future that there's just a tiny core kernel with the base functionality that is needed to boot it and to keep things running, but all the other stuff is around.
00:47:16 [W] Around that kernel is basically defined through PPS functionality instead of Kernel modules.
00:47:27 [W] For example that allows to reduce the attack surface because you have a tiny core kernel and at the same time all these BPF code that is defined the rounded the safety verify to the right
00:47:37 [W] You can get rid of all the feature creep.
00:47:42 [W] that is otherwise still in the colonel, but you're not going to use it and you can make much better use of that of the CPU resource.
00:47:52 [W] So you can think of like that potential in the long term kubernative could even ship custom kernel extensions that it would need depending on the underlying workloads that it has whether it's running on a data center or on a small embedded system.
00:48:09 [W] You can think of the cube proxy replacement that they mentioned here is one example where ppf can help just as a tiny dot on that Universe of possibilities.
00:48:18 [W] So with that said, yeah, please go and try out the cube Roxy free mode and if you can also take a look at the code and help contribute and with that.
00:48:33 [W] I'm I'm opening up for questions. Thanks a lot.
00:48:34 [W] Okay, so I'm going over some of the questions that have been asked and so one question is what is EPF support and different Unix operating systems for example,
00:48:53 [W] So as far as I'm aware, there's effort into porting EPF over to other operating systems.
00:49:06 [W] I think not only FreeBSD but also yet different bsd's but I'm not fully in the picture on the support of like
00:49:14 [W] UPF development is basically happening very quickly in the loose kernel new features are added. So it's quite hard to say like how much they are able to catch up with.
00:49:27 [W] Okay.
00:49:39 [W] Next question is the system has plans to fully integrate ask you proxy replacement. So right now it's like the with the latest actually sodium release 1.8 the support of replacing to
00:49:43 [W] Is has quite matured over time and it's the features of The Parting of the different types are have all been implemented.
00:49:59 [W] So basically, yeah, it can be used as replacement right now already.
00:50:05 [W] Okay next question.
00:50:06 [W] Doesn't the colonel do a lot of stuff that is networking related, I guess in that context.
00:50:21 [W] I'm not sure how he bof enables a microkernel.
00:50:28 [W] Yeah. Basically the idea is that I mean the majority of the code in an external is actually driver related.
00:50:35 [W] So and idea would be like that you have the really tiny core with the base functionality like schedulers the disk IO framework or networking core protocols like TCP.
00:50:42 [W] But like everything around it could be replaced with it.
00:50:45 [W] Next question.
00:50:51 [W] Is it possible to migrate the running communities cluster from Q proxy IP tables to sodium xtp.
00:50:58 [W] Yes.
00:50:59 [W] that's actually possible and you can also run both at the same time and basically for psyllium it would mean that it will intercept us requests to Services
00:51:13 [W] Even before kubeflow XE componentconfig them and translate them for example at the socket layer and x and x t p layer Q proxy will actually never see those requests when they have to go to remote
00:51:28 [W] Service endpoint and that's what I've also been running in this Benchmark that you've seen before. It was basically running alongside it so that's just fine.
00:51:37 [W] Can you recommend a good resource like book a project to get started with EPF development?
00:51:46 [W] Yeah, that's a great question.
00:51:47 [W] Um, there's a website that recently launched which is called ebf that I owe and you can learn about all the projects and there are some high-level introduction into the different kernel components and also pointers
00:52:02 [W] For like further reading for example, there's their books about EPF that I can highly recommend and yet please take a look there EPF that I owe.
00:52:14 [W] So next question EB f is lot of new kernel hooks each day. Is there an easy way to know a list of them?
00:52:27 [W] And is there an effort for better documentation on it?
00:52:29 [W] Definitely.
00:52:42 [W] there is a diode is a project. We see basically has a documentation page where they list all the new program types map types and BPF helper programs as well as attachment points there.
00:52:45 [W] based on the Kernel version
00:52:46 [W] But what does also possible is but it's probably more for developers that the ppf tool that is maintained in the kernel.
00:53:02 [W] It has a option to dump all the features and basically to probe the underlying kernel for the availability of programs helpers and attachment points. So you can also use that.
00:53:08 [W] Will there be tangible speed benefits when using it in VMS?
00:53:25 [W] Definitely because EPF still comes at a much earlier point the networking stack then for example netfilter. So you will still see benefits. Even if it's running on a VM and in terms of XDP, for example a
00:53:32 [W] As I mentioned Cloud providers, they are providing essay will be based networking.
00:53:47 [W] So in terms it's usually called accelerated networking on AWS is shorter so you can use that and then you get like pretty much knative performance from the networking card and
00:53:53 [W] When it have been having xtp attached to that.
00:53:55 [W] Okay.
00:54:00 [W] So yeah, thanks for listening. And if there are any further questions, please feel free to ask on slack.
00:54:12 [W] I'll be still in the slag and the slag channel is to keep con networking.
00:54:16 [W] So, thanks a lot.
