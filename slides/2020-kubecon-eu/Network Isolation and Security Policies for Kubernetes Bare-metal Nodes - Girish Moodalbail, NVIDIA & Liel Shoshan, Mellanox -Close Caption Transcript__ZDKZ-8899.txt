Network Isolation and Security Policies for Kubernetes Bare-metal Nodes: ZDKZ-8899 - events@cncf.io - Tuesday, August 18, 2020 11:28 AM - 69 minutes

Participant: wordly [W] English (US)
Participant: wordly [W0] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone.
00:11:54 [W0] Hello everyone. My name is Grace model Bell.
00:11:57 [W0] Hello everyone. My name is Grace model Bell.
00:16:46 [W0] My role at Nvidia is to design develop and manage.
00:16:47 [W0] STL networking for GPU accelerated compute the computer itself can be washing machines bare metal servers kubernetes pods, and each of them having Nvidia GPU networks in GPU cards in them.
00:17:03 [W0] I'm also Upstream contributor and maintain a phenomenal kubernative cni project I deal with ovhcloud.
00:17:10 [W0] Hi, my name is Leo Sean.
00:17:18 [W0] I'm a software architect at mellanox now Nvidia working on cloudbees the durations kubernative integration and switching off load.
00:17:28 [W0] Okay, great.
00:17:31 [W0] So we're going to talk about how to secure bare metal notes that form the kubenetes cluster using oven logical constructs and bump in the wire smart neck and in our case.
00:17:40 [W0] case. It's going to be mellanox Bluefield.
00:17:41 [W0] Laughter, so this is going to be the flow of our talk today.
00:17:50 [W0] GPU cards in them.
00:17:55 [W0] I'm also Upstream contributor and maintain it for a moment kubernative cni project.
00:17:55 [W0] deal with ovhcloud.
00:17:55 [W0] Hi, my name is Liang shuang I'm a software architect at mellanox. Now Nvidia working on cloudbees the durations kubernative integration and switching off load.
00:17:56 [W0] Be running the cluster on bare metal nodes will have a quick overview of oven and we'll see how one control brain runs on the smart Nick how the data path for oven gets offered it to the hardware.
00:18:09 [W0] and we'll also kind of look at other smart Nick advantages.
00:18:13 [W0] So the flies have been building Kuma discusses in VMS as nodes. And in this model, they actually take an application containerize it they run the containerless application inside a pot. The pot itself is running inside a VM and the
00:18:29 [W0] On the bare metal box.
00:18:36 [W0] So there is several layer of fencing if you will and the whole reason here it is they want to protect the data center from from application which could which can be compromised.
00:18:47 [W0] So the reason they do this fencing is because they're not confident about the container of security and the hope is that if an attacker breaks out of the container, they still trapped inside the VM and then don't ask they don't have access to the bare metal nobl9.
00:19:00 [W0] And therefore they cannot cause any harm to the data centers itself.
00:19:15 [W0] However, this comes with a huge performance cost for the application lot of the hardware resources, like the neck the GPU has to be virtualized and this virtualize thing has to be passed down to the VM and the VMS then as 2.0 us down to the
00:19:21 [W0] There's also an hypervisor hold it for the application performance.
00:19:32 [W0] So the thing is this a trade-off the trade-off between performance and security and violet and isolation.
00:19:41 [W0] So the deployers kind of tend to err on the side of security and they don't kind of focus more on the performance because security is of Paramount importance.
00:19:47 [W0] So that need not however be the case with a help from bumping the wire spotnik and distributed sdn control plane.
00:20:01 [W0] It's possible that we can get both performance as well as Security in isolation.
00:20:09 [W0] And this kind of moves the Zone the trust Zone from the hypervisor onto the spotnik hardware and we are going to explain how this bump in the wire spotnik and the distributed sdn control plane enables.
00:20:17 [W0] Achieving both performance at the end not at the same time losing Security in isolation for the pair mental notes.
00:20:26 [W0] So the way we're going with this is that so with this part Nick and bump in the distributed control plane. It is possible to push all of the apples and the firewall rules if you will to the smart snyk itself
00:20:45 [W0] The pots running on the bare metal.
00:20:48 [W0] They're totally unaware of this bump in the wire all the kubenetes network policies for the pot or implemented on the spot neck. And then the actuals for the host itself is implemented on the spot knee.
00:21:02 [W0] if if an attacker kind of jumps out of the pot and is now on the bare metal host. They cannot flush IP table rules or the natural Rose or any host-based firewall rules because all of
00:21:17 [W0] the firewall rules are on the smart neck, which is acts as the bump in the wild thing.
00:21:22 [W0] So in that way you can secure the bare metal nodes.
00:21:27 [W0] So oven the distributed STM control plane and the smart neck and of join hands here together to kind of provide Bell metal network security and isolation.
00:21:39 [W0] So Avenues open source project.
00:21:47 [W0] It's a distributed stn control plane that provides Network virtualization solution.
00:21:49 [W0] It's developed by the open V switch Community the GitHub URL here is where you can find the report for the project.
00:22:01 [W0] The thing with other one, is that the whole data path for our one can be offloaded with mellanox snyk, so obviously with the offloads you get the reduce CPU utilization and the CPU can be used for applications and
00:22:09 [W0] And in addition we also get the increased throughput how one can be better explained with a examine our with an example and a diagram and that's what we're going to do next.
00:22:23 [W0] So in this diagram, you have two things right?
00:22:34 [W0] You have a logical topology to your left and then you have and we have realized that logical topology on the to Bare Metal notes. So on the logical topology is a very simple logical topology.
00:22:42 [W0] It's a logical router with two logical switches connected to it and each logical switch has a logical port at a ch2 it so logical router is a L3 entity. It provides routing ECM petabyte-scale.
00:22:53 [W0] Nat load balancing policy-based routing logical switch on the other hand provides support for articles support for th cpv. It also provides support for load balancing and DNS The Logical
00:23:09 [W0] Is the MAC address and IP address? It captures it it's where we apply Ackles and then also it provides anti-spoofing support.
00:23:21 [W0] So the thing is The Logical Port of penalty spot network interface.
00:23:27 [W0] So The Logical Port is an API to control the parts interface attributes like so you kind of apply policies on The Logical Port it kind of reflects on the the on
00:23:39 [W0] the physical pain the pain interface on the on the Pod so so on the right side what we have a tube a metal notes on these notes you have ovhcloud.
00:23:53 [W0] I think aggression bridgecrew old be our end this integration Bridge implements the whole logical pipeline the whole amount logical topology using nobl9 using open Flow tables and flows in those
00:24:08 [W0] Was so then on this particular in this particular diagram here on Node 1 a positive got schedule on node to pot one got schedule bot zero is connected to the integration bridgecrew ovs internal, Port
00:24:23 [W0] End part 1 is connected to the obvious integration bridge to the internal Port P1.
00:24:38 [W0] So what we do here is that we associate this physical port with a logical port and that process is called Port binding and this is one of the important aspects of our one what this port binding does is it tells you the physical?
00:24:44 [W0] Location of a port for a given logical Port so with that physical location known now the the oven control plane knows a way to send the packet to for a given part
00:24:59 [W0] Example when a when pods hero wants to talk to pot 1 when the packet arrives on the be our end and this be our in implements the oven logical topology through open Flow tables. It already knows that to send the packet to part 1 it
00:25:14 [W0] Just send it to the Note 2. So we type IP address and then using open Flow flows. It just forwards the packet to that next half.
00:25:30 [W0] So the whole logical topology is realized on this integration bridge and then the the physical poor the poor binding thing enables that end-to-end connectivity between
00:25:38 [W0] Put it between the ports. So I want has a collection of demons that work together to push this logical topology information and the physical location of this ports to each node, and and then each notes are integration Bridge.
00:25:53 [W0] So in this slide, we basically this line basically talks about the oven kubenetes project.
00:26:02 [W0] I want kubenetes is an open source project contributed or worked on by the oven Community the GitHub URL for the same.
00:26:10 [W0] here as shown in the slide here.
00:26:13 [W0] It implements containerd networking spec 0.0 0.4 it basically it's a layered architecture at the higher layer.
00:26:23 [W0] at the higher layer. You have among kubernative cni-genie.
00:26:24 [W0] Depends on the oven and I won in turn depends on ovhcloud Minted as a client-server architecture.
00:26:36 [W0] It's basically equivalent is Watcher and it has set several set of pots.
00:26:42 [W0] It kind of takes the kubenetes resources and then it Maps into human resources. For example, kubenetes Network policy is mapped into our Nicole's kubenetes cluster Services is mapped into a van load balancer
00:26:51 [W0] Is map into a logical switch ports and kubenetes node is map into logical switches.
00:27:06 [W0] So it watches on these Cuban disappear resources as and when this APA objects appear it kind of translate that into oven resources.
00:27:12 [W0] So the oven control plane kind of Acts on those oven resources that create that that was created by the oven cni-genie.
00:27:21 [W0] Lows and these logical Force our intern translated into ovhcloud rules on each of the node that forms the kubenetes cluster.
00:27:36 [W0] So on the diagram to the right, you see that it kind of explain all the components running at various layers or on the conference of various layers running on various notes in the kubenetes cluster.
00:27:46 [W0] So the whole point here is that to show that you know, some of the oven control plane
00:27:52 [W0] Plants they run on the nodes and when we move to the smart Nick model, these control print components will end up running on the smart Nick itself.
00:28:01 [W0] So in here you can see that on the host. We had ovhcloud troller. Which kind of was it.
00:28:08 [W0] Adding OpenFlow flows into the ovhcloud.
00:28:39 [W0] performing an isolation for the build metal nodes
00:28:42 [W0] So this is this diagram kind of dive deep into how things look like on the smart neck.
00:28:56 [W0] So on the spotnik itself, we haven't controlled plane components running which is almond controller and we have obvious components running which is obviously switch T and obesity be server and on the oven cni-genie have agent running which acts as a kubernative watcher.
00:29:07 [W0] I'm trying to watch on certain kubenetes resources and then acting on those resources life cycle. The oven controller takes the logical flows from the oven and then translates into the open Flow tables and open Flow flows
00:29:22 [W0] And then kind of creates the data path for the pod on the bare metal host itself.
00:29:35 [W0] We have multis to orchestrate multiple interfaces into the Pod and we have a very basic cni-genie.
00:29:52 [W0] Hence the VF on the host and we add these representatives to the integration bridge and this representer maps to The Logical switch port in the oven topology.
00:30:07 [W0] So this is the port binding aspect of the oven functionality.
00:30:20 [W0] So when a representative zero for gets added to the pr ain't we we kind of annotate representative 042 tell that it belongs to certain law.
00:30:24 [W0] You can Port so now anything anytime we change the logical ports attributes. The representers attributes change through open Flow flows. And therefore the vf2 that is attached to represent a zero for it will see that
00:30:38 [W0] That changes all of these things are being done on the next hop, you know, next stop it acts like a next stop switch.
00:30:49 [W0] It's like a bump in the wire.
00:30:57 [W0] the attributes of The Logical Port gets reflected on this representer and then it gets reflected on the VF itself. So that's pretty much what this slide talks to.
00:31:03 [W0] So the this this slide basically captures how we can use our knackles to kind of add equals at a different priorities and therefore override The Echoes that
00:31:18 [W0] The tenants themselves configure so in here our knackles party range from 0 all the way up to 32,000 767 so higher the priority of the actual higher will be its precedents.
00:31:33 [W0] So in the design here, we have all the tentacles being represented with part is less than 10,000.
00:31:42 [W0] However, all the cloud provider ankles are at the priority more than 10,000. So therefore they presidents will be higher as compared to the tenant.
00:31:48 [W0] Chuckles so whenever the kubenetes network policies are translated to Urban axles. They get parties in this range the cloud providers they can come in on the side and then add
00:32:03 [W0] did axles at a higher level so that you know, we they can kind of make sure that the pink tenant here does not somehow talking to the blue tenant because the cloud provider of a didactic else to make
00:32:18 [W0] That such a communication is not possible.
00:32:21 [W0] In fact, the cloud products can add a calls to their own infrastructure services to make sure that the tenants host or the tenant spot do not talk to infrastructure services that they're not supposed to
00:32:36 [W0] so this way using our and I have priority items we kind of make sure we provide that isolation of between the Bare Metal Force and the data center services and the data center itself and all of this actives the
00:32:52 [W0] Flows with all offloaded onto the integration bridge and layouts going to talk to it in the next set of slides.
00:32:59 [W0] So then so in here what we have done is we have 2 different bare metal servers and the bare metal itself is part of Ammon logical topology and then the
00:33:15 [W0] Mirantis cluster also is part of our logical topology.
00:33:19 [W0] the paths are kind of connected together with in an oven logical topology orchestrated by the oven cni-genie.
00:33:45 [W0] LA traffic and the network policies for this overlay traffic is defined by the kubernative network policies on the other hand the East-West traffic between the bare metal server itself is achieved through the
00:33:59 [W0] Logical topology and using an overlay and and then we'll have our manacles Define which bail metaphors can talk to each other and which parameter host cannot talk to each other.
00:34:14 [W0] This is defined by the one at those see both the the green overlay and the orange overlay.
00:34:27 [W0] overlay. They are transported over the blue underlay this forms the underlay where the blue fields are all in it in blue fields in the data center. They're all interconnected together.
00:34:31 [W0] Other and they form the underlay over which the genie of Tunnel traffic is transported between the pods and between the base emitter server itself.
00:34:44 [W0] So the red underline Network here is for access to the internet if these pots want to access to the internet, they kind of exit out of the tunnel or the exit out of the oven logical topology through a oven Gateway router
00:34:55 [W0] Then they get so started into the blue feet IP and then now they're on the red underlay and heading towards the internet. The same thing is true for the bare metal server host trying to access Internet. So they exit out of their
00:35:11 [W0] Balaji through a Gateway router oven Gateway router where the source knative happens the logical topology IP source that it to the Blue Field under the IP and then the packet
00:35:26 [W0] The internet so that's this diagram.
00:35:32 [W0] So here what we are capturing is how one can provide a multi-tenancy model in a DC using oven distributed control plane and a smart Nick in it.
00:35:44 [W0] Tenants lieutenant and a ping tenant the blue tenant has bunch of bell metal servers, which are all interconnected together using an hour and logical topology on top of this bare metal servers.
00:36:00 [W0] They're running cubed is cluster, which is orchestrated by the oven cni-genie.
00:36:15 [W0] Metal servers in the DC that was assigned to them and the parameter service themselves are connected together using logical switch and logical router.
00:36:30 [W0] And then on those bare metal servers, they have their own kubenetes cluster running which is again orchestrated through our and cni as you can see they all use this overlapping eyepiece.
00:36:39 [W0] And and this is all made possible because of the oven virtual topology and they all exit out to the underlay and then
00:36:47 [W0] The reach of the internet the cloud providers can apply their Echoes on the bare metal on the smart neck now to kind of make sure that the blue tenant and the pink tenant are isolated from each other and
00:37:00 [W0] Did from the cloud provider Services itself and the next slide kind of talks to that so in here we show a DC networking with thoughts and L2 within the torch and everything
00:37:16 [W0] Versus L3.
00:37:23 [W0] So here you see Lieutenant has a bunch of Bill bare metal notes assigned to it.
00:37:25 [W0] There's a for bare metal notes assigned to it.
00:37:28 [W0] And each of the base metal node has a blue filter adapter assigned to it.
00:37:36 [W0] And then this forebay metals they form a v tap Network amongst themselves.
00:37:39 [W0] There's a concept of percent zones.
00:37:41 [W0] So you create this island of bell metal servers with blue field on them, and these are
00:37:48 [W0] the the chin of traffic or the oven traffic only kind of happens or occurs between these set of their metal nodes on top of this big metal node is where we build the Cuban this cluster
00:38:02 [W0] Kind of live within this eye line. Similarly. The pink tenant also has its own island with the set of Babymetal servers with the each of them having a blue field associated with it and they have their own logical topology and
00:38:17 [W0] The tunnel is found only between these for Pinkberry Metals in that way.
00:38:25 [W0] There is no interconnect at all between the Blue Island and the pink Island. And then in the same way the cloud provider Services the cloud provider can provide their services either on the physical Network directly or they can provide it on the oven
00:38:40 [W0] So in that way if Lieutenant can access some of the services provided by the provide the provided by the cloud provider through the logical topology or directly the exit out to the underlay and on the underlay, they kind of
00:38:57 [W0] learning on the physical Network itself
00:39:00 [W0] next slide player. Can you proceed?
00:39:05 [W0] Sure.
00:39:08 [W0] Thanks Gary.
00:39:11 [W0] So in addition to the security enhancement and isolation dissolution also enjoys data passed Hydro offload. This is done using the a sub Square framework by mellanox.
00:39:24 [W0] By mellanox accelerated switching and packet processing.
00:39:28 [W0] It's a software and Hardware integrated framework which utilizes mellanox Nick's to accelerate and also the data plane while maintaining the control plane in software.
00:39:40 [W0] So it minimizes the needed changes in kubernative cni-genie other SD ends the solution support different configuration as a review and virt AO and it's available both upstream and in bol.com.
00:39:52 [W0] box
00:39:53 [W0] It consists of three capabilities classification offload action offload and database offload, which are required for switching for switch that the path of flow.
00:40:15 [W0] So as Eurasia explained we worked with oven control plan, which is based on ovhcloud.
00:40:50 [W0] As a user space module and a curved space module and an in the regular configuration each pod is assigned of vth pervert interface.
00:41:10 [W0] So in the traditional model the first packet of a flow will go up to the user space there.
00:41:24 [W0] It will be processed and sent and then suitable rule will be inserted to the colonel.
00:41:36 [W0] So the next pockets of the same flow will go directly to the Cardinal without going through the user space.
00:41:37 [W0] With connect exhale to offload The Suitor the this rule would all not only be inserted to the colonel but also to the hardware to the Kinect is embedded switch in this case the next buckets
00:41:52 [W0] We will not go to the CPU at all, and we'll just be processed in the house other.
00:42:00 [W0] So this way we keep the first packet means architecture but using an additional how to a layer which is the embedded switch which is located on the neck. Each
00:42:22 [W0] the SRV virtual function
00:42:26 [W0] and while the controller or the user inserts the policies by using obvious open Flow rules. These rules are offloaded to the hardware in a transparent way using Linux TC rules.
00:42:41 [W0] Packets are being processed by the neck. And therefore there is no host CPU consumption and we increase throughput and decrease latency
00:42:59 [W0] Was are offloaded to the hardware in a transparent way using Linux TC rules.
00:43:01 [W0] Packets are being processed by the neck. And therefore there is no host CPU consumption and we increase throughput and decrease latency
00:43:02 [W0] Really?
00:43:02 [W0] So a sub square has many benefits it achieves uncompromised performance.
00:43:09 [W0] It saves CPU using the smart neck. You can achieve also full isolation and it facilitates the solution for both bare metal and virtualized clouds.
00:43:20 [W0] So if for virtualized cloud then both Network virtualization storage virtualization were traditionally running in the hypervisor using
00:43:46 [W0] So if for virtualized cloud that both Network virtualization storage virtualization were traditionally running in the hypervisor.
00:43:47 [W0] Using Kinect except we can offload the network data playing by a way leveraging the a sub square frame or when we use blue field. The smart.
00:43:58 [W0] Nick is a bump in a wire also the control plane of the switch can run on this morning on the arm course as well as the storage virtualization and other Security Services. This is also true for bare metal sir.
00:44:14 [W0] service servers
00:44:15 [W0] So I'll summarize bare metal clouds raises the demand for Network isolation since the host is no longer trusted.
00:44:32 [W0] It was relation can be achieved by using a dedicated Hardware which is which it with sit in front of the in front of the host.
00:44:47 [W0] We use Bluefield smart Nick as a bump in the wire to run over and control plane in a secured way running both the virtual switch and the utility is configuring it on the neck.
00:44:59 [W0] We also leverage DS of square framework in order to achieve datapath Hydro offload, which resulted in Boston drastic reduction in host CPU consumption and enhance performance.
00:45:15 [W0] So I'll summarize bare metal clouds raises the demand for Network isolation since the host is no longer trusted.
00:45:17 [W0] Network installation can be achieved by using a dedicated Hardware which is which is with sits in front of the import of the host. We use blue field smart neck as a bump in The Wire
00:45:19 [W0] So we have a few questions actually let and the first of all thank you you have as well. Thank you for attending our session. There are few questions.
00:45:35 [W0] I'm going to read it out and and some questions Leah will help me answer. And then as I try to take a stab at it, the one of the questions was I understand your points, but
00:45:47 [W0] that's what relation of oven add another layer of complexity and therefore possibilities for error in terms of configuration and and handling.
00:45:58 [W0] Yes any any abstraction layer comes with its own configuration challenges and error handling stuff. But the good news is that our one is built with a wide variety of debugging tools spin already
00:46:09 [W0] Very well used very rarely has cancer pressurization solution in openstack world.
00:46:19 [W0] It's been the default reference implementation openstack.
00:46:23 [W0] It's been active for more than a few years now.
00:46:27 [W0] So the tooling around managing the oven is pretty nice.
00:46:32 [W0] nice. So that's one thing the second thing is the virtualization itself shouldn't add any order it in fact because our one is a distributed control plane. It has the global view of your network it already
00:46:42 [W0] Knows where the pots are through that Port binding. So there is no our petal in when when a man is being used.
00:46:54 [W0] I mean not completely normal, but two significant extent the heart is reduced because we already know where the pots are and behind what tunnel endpoint the parts are.
00:47:01 [W0] There are so many instances where we do distributed the hcp distributed DNS. So not of this traffic in your network drastically reduces, so
00:47:12 [W0] I wouldn't say on the controlled from the control plane point of view.
00:47:22 [W0] There are a lot of interesting benefits you get on the data plan on the data plane point of view. The benefits are obviously you can offload the entire ovhcloud Prime to Smart next.
00:47:30 [W0] So then the next question is will this work with snyk Lea good?
00:47:38 [W0] Logged won't work.
00:47:44 [W0] with can you hear me?
00:47:46 [W0] You can hear me.
00:47:48 [W0] Yeah.
00:47:49 [W0] Okay, it won't work.
00:47:54 [W0] It will work with lag.
00:47:58 [W0] Okay, it will work with slag as I said.
00:48:00 [W0] Okay. So what is the current the next question is?
00:48:07 [W0] What is the current maximum number of virtual functions for current and Prospect two network cards?
00:48:10 [W0] Go ahead.
00:48:17 [W0] So the current to take the current limitation is 128 virtual functions purport.
00:48:24 [W0] There is a work to increase it to around 256 purport.
00:48:29 [W0] And in addition there is work on a new interface which is lighter than virtual function, but can provide the same functionalities including everything that we have mentioned here.
00:48:41 [W0] It should be it should be more customer more accustomed to Cloud native environment.
00:48:53 [W0] It's lighter than virtual functions and over there. We expect it called skeleton functions SF and over there. We supposed to scale much more.
00:49:05 [W0] I'll take the next question is well, too.
00:49:09 [W0] So what does the Nick needs to support in order for this to work?
00:49:17 [W0] What is a smart neck?
00:49:21 [W0] So regarding the datapath harder offload all mellanox has and videos Nick's from the last year's connect x 5 and above supports it and regarding the running the ovhcloud
00:49:33 [W0] For that we need CPUs embedded CPUs on the neck.
00:49:41 [W0] This is what we call the smart Nick blue field and we need it in order to run the controller.
00:49:46 [W0] Okay, so does 7:00 work on regular Nick as well or it's just for Bluefield smart make you want to take a stab at that.
00:50:00 [W0] Yeah, so over and without any offload can work on every knative course with that a person how to offload you can work on every mellanox snyk from kinetic 5 and above and that's it.
00:50:17 [W0] Yeah, so over and without any offload can work on every knative course with data path on how to offload.
00:50:18 [W0] You can work on every mellanox snyk from connected 5 and above and that's it.
00:50:19 [W0] it. Yeah.
00:50:19 [W0] Okay. Yeah. Yeah, this is this the solution that we are proposed here is not agnosticism. It's agnostic to vendor-specific Smart need any spotnik that provides ovhcloud flowed will be able to you know
00:50:35 [W0] solution that we are proposed here is not agnostic image agnostic to vendor-specific smartnet any spotnik that provides ovhcloud will be able to you know can use the solution that we presented here Blue Field is just a reference implementation
00:50:39 [W0] So the next question is how do you see about difference in openstack process kubernative?
00:50:47 [W0] It's pretty different.
00:50:48 [W0] I won the the beautiful part of our one is the logical topology.
00:50:59 [W0] So the whole logical topology is different different for kubenetes as compared to openstack.
00:51:08 [W0] So from 7 The Logical topologies different lot of things that a lot of things in how the traffic flow works like between between the V.
00:51:10 [W0] Typically in the past the East-West traffic the north-south traffic traffic to the Internet. So those all change so since the logical topology is different between the two solutions the way it works also is different.
00:51:25 [W0] I think I covered pretty much all the questions in here.
00:51:30 [W0] There are out of time.
00:51:35 [W0] So any other and we are out of time.
00:51:41 [W0] other question can be answered on the networking to work on Slack.
00:51:43 [W0] Thank you.
00:51:46 [W0] Thank you.

Transcription for wordly [W0]

00:00:00 [W] Hello everyone.
00:11:54 [W0] Hello everyone. My name is Grace model Bell.
00:11:57 [W0] Hello everyone. My name is Grace model Bell.
00:16:46 [W0] My role at Nvidia is to design develop and manage.
00:16:47 [W0] STL networking for GPU accelerated compute the computer itself can be washing machines bare metal servers kubernetes pods, and each of them having Nvidia GPU networks in GPU cards in them.
00:17:03 [W0] I'm also Upstream contributor and maintain a phenomenal kubernative cni project I deal with ovhcloud.
00:17:10 [W0] Hi, my name is Leo Sean.
00:17:18 [W0] I'm a software architect at mellanox now Nvidia working on cloudbees the durations kubernative integration and switching off load.
00:17:28 [W0] Okay, great.
00:17:31 [W0] So we're going to talk about how to secure bare metal notes that form the kubenetes cluster using oven logical constructs and bump in the wire smart neck and in our case.
00:17:40 [W0] case. It's going to be mellanox Bluefield.
00:17:41 [W0] Laughter, so this is going to be the flow of our talk today.
00:17:50 [W0] GPU cards in them.
00:17:55 [W0] I'm also Upstream contributor and maintain it for a moment kubernative cni project.
00:17:55 [W0] deal with ovhcloud.
00:17:55 [W0] Hi, my name is Liang shuang I'm a software architect at mellanox. Now Nvidia working on cloudbees the durations kubernative integration and switching off load.
00:17:56 [W0] Be running the cluster on bare metal nodes will have a quick overview of oven and we'll see how one control brain runs on the smart Nick how the data path for oven gets offered it to the hardware.
00:18:09 [W0] and we'll also kind of look at other smart Nick advantages.
00:18:13 [W0] So the flies have been building Kuma discusses in VMS as nodes. And in this model, they actually take an application containerize it they run the containerless application inside a pot. The pot itself is running inside a VM and the
00:18:29 [W0] On the bare metal box.
00:18:36 [W0] So there is several layer of fencing if you will and the whole reason here it is they want to protect the data center from from application which could which can be compromised.
00:18:47 [W0] So the reason they do this fencing is because they're not confident about the container of security and the hope is that if an attacker breaks out of the container, they still trapped inside the VM and then don't ask they don't have access to the bare metal nobl9.
00:19:00 [W0] And therefore they cannot cause any harm to the data centers itself.
00:19:15 [W0] However, this comes with a huge performance cost for the application lot of the hardware resources, like the neck the GPU has to be virtualized and this virtualize thing has to be passed down to the VM and the VMS then as 2.0 us down to the
00:19:21 [W0] There's also an hypervisor hold it for the application performance.
00:19:32 [W0] So the thing is this a trade-off the trade-off between performance and security and violet and isolation.
00:19:41 [W0] So the deployers kind of tend to err on the side of security and they don't kind of focus more on the performance because security is of Paramount importance.
00:19:47 [W0] So that need not however be the case with a help from bumping the wire spotnik and distributed sdn control plane.
00:20:01 [W0] It's possible that we can get both performance as well as Security in isolation.
00:20:09 [W0] And this kind of moves the Zone the trust Zone from the hypervisor onto the spotnik hardware and we are going to explain how this bump in the wire spotnik and the distributed sdn control plane enables.
00:20:17 [W0] Achieving both performance at the end not at the same time losing Security in isolation for the pair mental notes.
00:20:26 [W0] So the way we're going with this is that so with this part Nick and bump in the distributed control plane. It is possible to push all of the apples and the firewall rules if you will to the smart snyk itself
00:20:45 [W0] The pots running on the bare metal.
00:20:48 [W0] They're totally unaware of this bump in the wire all the kubenetes network policies for the pot or implemented on the spot neck. And then the actuals for the host itself is implemented on the spot knee.
00:21:02 [W0] if if an attacker kind of jumps out of the pot and is now on the bare metal host. They cannot flush IP table rules or the natural Rose or any host-based firewall rules because all of
00:21:17 [W0] the firewall rules are on the smart neck, which is acts as the bump in the wild thing.
00:21:22 [W0] So in that way you can secure the bare metal nodes.
00:21:27 [W0] So oven the distributed STM control plane and the smart neck and of join hands here together to kind of provide Bell metal network security and isolation.
00:21:39 [W0] So Avenues open source project.
00:21:47 [W0] It's a distributed stn control plane that provides Network virtualization solution.
00:21:49 [W0] It's developed by the open V switch Community the GitHub URL here is where you can find the report for the project.
00:22:01 [W0] The thing with other one, is that the whole data path for our one can be offloaded with mellanox snyk, so obviously with the offloads you get the reduce CPU utilization and the CPU can be used for applications and
00:22:09 [W0] And in addition we also get the increased throughput how one can be better explained with a examine our with an example and a diagram and that's what we're going to do next.
00:22:23 [W0] So in this diagram, you have two things right?
00:22:34 [W0] You have a logical topology to your left and then you have and we have realized that logical topology on the to Bare Metal notes. So on the logical topology is a very simple logical topology.
00:22:42 [W0] It's a logical router with two logical switches connected to it and each logical switch has a logical port at a ch2 it so logical router is a L3 entity. It provides routing ECM petabyte-scale.
00:22:53 [W0] Nat load balancing policy-based routing logical switch on the other hand provides support for articles support for th cpv. It also provides support for load balancing and DNS The Logical
00:23:09 [W0] Is the MAC address and IP address? It captures it it's where we apply Ackles and then also it provides anti-spoofing support.
00:23:21 [W0] So the thing is The Logical Port of penalty spot network interface.
00:23:27 [W0] So The Logical Port is an API to control the parts interface attributes like so you kind of apply policies on The Logical Port it kind of reflects on the the on
00:23:39 [W0] the physical pain the pain interface on the on the Pod so so on the right side what we have a tube a metal notes on these notes you have ovhcloud.
00:23:53 [W0] I think aggression bridgecrew old be our end this integration Bridge implements the whole logical pipeline the whole amount logical topology using nobl9 using open Flow tables and flows in those
00:24:08 [W0] Was so then on this particular in this particular diagram here on Node 1 a positive got schedule on node to pot one got schedule bot zero is connected to the integration bridgecrew ovs internal, Port
00:24:23 [W0] End part 1 is connected to the obvious integration bridge to the internal Port P1.
00:24:38 [W0] So what we do here is that we associate this physical port with a logical port and that process is called Port binding and this is one of the important aspects of our one what this port binding does is it tells you the physical?
00:24:44 [W0] Location of a port for a given logical Port so with that physical location known now the the oven control plane knows a way to send the packet to for a given part
00:24:59 [W0] Example when a when pods hero wants to talk to pot 1 when the packet arrives on the be our end and this be our in implements the oven logical topology through open Flow tables. It already knows that to send the packet to part 1 it
00:25:14 [W0] Just send it to the Note 2. So we type IP address and then using open Flow flows. It just forwards the packet to that next half.
00:25:30 [W0] So the whole logical topology is realized on this integration bridge and then the the physical poor the poor binding thing enables that end-to-end connectivity between
00:25:38 [W0] Put it between the ports. So I want has a collection of demons that work together to push this logical topology information and the physical location of this ports to each node, and and then each notes are integration Bridge.
00:25:53 [W0] So in this slide, we basically this line basically talks about the oven kubenetes project.
00:26:02 [W0] I want kubenetes is an open source project contributed or worked on by the oven Community the GitHub URL for the same.
00:26:10 [W0] here as shown in the slide here.
00:26:13 [W0] It implements containerd networking spec 0.0 0.4 it basically it's a layered architecture at the higher layer.
00:26:23 [W0] at the higher layer. You have among kubernative cni-genie.
00:26:24 [W0] Depends on the oven and I won in turn depends on ovhcloud Minted as a client-server architecture.
00:26:36 [W0] It's basically equivalent is Watcher and it has set several set of pots.
00:26:42 [W0] It kind of takes the kubenetes resources and then it Maps into human resources. For example, kubenetes Network policy is mapped into our Nicole's kubenetes cluster Services is mapped into a van load balancer
00:26:51 [W0] Is map into a logical switch ports and kubenetes node is map into logical switches.
00:27:06 [W0] So it watches on these Cuban disappear resources as and when this APA objects appear it kind of translate that into oven resources.
00:27:12 [W0] So the oven control plane kind of Acts on those oven resources that create that that was created by the oven cni-genie.
00:27:21 [W0] Lows and these logical Force our intern translated into ovhcloud rules on each of the node that forms the kubenetes cluster.
00:27:36 [W0] So on the diagram to the right, you see that it kind of explain all the components running at various layers or on the conference of various layers running on various notes in the kubenetes cluster.
00:27:46 [W0] So the whole point here is that to show that you know, some of the oven control plane
00:27:52 [W0] Plants they run on the nodes and when we move to the smart Nick model, these control print components will end up running on the smart Nick itself.
00:28:01 [W0] So in here you can see that on the host. We had ovhcloud troller. Which kind of was it.
00:28:08 [W0] Adding OpenFlow flows into the ovhcloud.
00:28:39 [W0] performing an isolation for the build metal nodes
00:28:42 [W0] So this is this diagram kind of dive deep into how things look like on the smart neck.
00:28:56 [W0] So on the spotnik itself, we haven't controlled plane components running which is almond controller and we have obvious components running which is obviously switch T and obesity be server and on the oven cni-genie have agent running which acts as a kubernative watcher.
00:29:07 [W0] I'm trying to watch on certain kubenetes resources and then acting on those resources life cycle. The oven controller takes the logical flows from the oven and then translates into the open Flow tables and open Flow flows
00:29:22 [W0] And then kind of creates the data path for the pod on the bare metal host itself.
00:29:35 [W0] We have multis to orchestrate multiple interfaces into the Pod and we have a very basic cni-genie.
00:29:52 [W0] Hence the VF on the host and we add these representatives to the integration bridge and this representer maps to The Logical switch port in the oven topology.
00:30:07 [W0] So this is the port binding aspect of the oven functionality.
00:30:20 [W0] So when a representative zero for gets added to the pr ain't we we kind of annotate representative 042 tell that it belongs to certain law.
00:30:24 [W0] You can Port so now anything anytime we change the logical ports attributes. The representers attributes change through open Flow flows. And therefore the vf2 that is attached to represent a zero for it will see that
00:30:38 [W0] That changes all of these things are being done on the next hop, you know, next stop it acts like a next stop switch.
00:30:49 [W0] It's like a bump in the wire.
00:30:57 [W0] the attributes of The Logical Port gets reflected on this representer and then it gets reflected on the VF itself. So that's pretty much what this slide talks to.
00:31:03 [W0] So the this this slide basically captures how we can use our knackles to kind of add equals at a different priorities and therefore override The Echoes that
00:31:18 [W0] The tenants themselves configure so in here our knackles party range from 0 all the way up to 32,000 767 so higher the priority of the actual higher will be its precedents.
00:31:33 [W0] So in the design here, we have all the tentacles being represented with part is less than 10,000.
00:31:42 [W0] However, all the cloud provider ankles are at the priority more than 10,000. So therefore they presidents will be higher as compared to the tenant.
00:31:48 [W0] Chuckles so whenever the kubenetes network policies are translated to Urban axles. They get parties in this range the cloud providers they can come in on the side and then add
00:32:03 [W0] did axles at a higher level so that you know, we they can kind of make sure that the pink tenant here does not somehow talking to the blue tenant because the cloud provider of a didactic else to make
00:32:18 [W0] That such a communication is not possible.
00:32:21 [W0] In fact, the cloud products can add a calls to their own infrastructure services to make sure that the tenants host or the tenant spot do not talk to infrastructure services that they're not supposed to
00:32:36 [W0] so this way using our and I have priority items we kind of make sure we provide that isolation of between the Bare Metal Force and the data center services and the data center itself and all of this actives the
00:32:52 [W0] Flows with all offloaded onto the integration bridge and layouts going to talk to it in the next set of slides.
00:32:59 [W0] So then so in here what we have done is we have 2 different bare metal servers and the bare metal itself is part of Ammon logical topology and then the
00:33:15 [W0] Mirantis cluster also is part of our logical topology.
00:33:19 [W0] the paths are kind of connected together with in an oven logical topology orchestrated by the oven cni-genie.
00:33:45 [W0] LA traffic and the network policies for this overlay traffic is defined by the kubernative network policies on the other hand the East-West traffic between the bare metal server itself is achieved through the
00:33:59 [W0] Logical topology and using an overlay and and then we'll have our manacles Define which bail metaphors can talk to each other and which parameter host cannot talk to each other.
00:34:14 [W0] This is defined by the one at those see both the the green overlay and the orange overlay.
00:34:27 [W0] overlay. They are transported over the blue underlay this forms the underlay where the blue fields are all in it in blue fields in the data center. They're all interconnected together.
00:34:31 [W0] Other and they form the underlay over which the genie of Tunnel traffic is transported between the pods and between the base emitter server itself.
00:34:44 [W0] So the red underline Network here is for access to the internet if these pots want to access to the internet, they kind of exit out of the tunnel or the exit out of the oven logical topology through a oven Gateway router
00:34:55 [W0] Then they get so started into the blue feet IP and then now they're on the red underlay and heading towards the internet. The same thing is true for the bare metal server host trying to access Internet. So they exit out of their
00:35:11 [W0] Balaji through a Gateway router oven Gateway router where the source knative happens the logical topology IP source that it to the Blue Field under the IP and then the packet
00:35:26 [W0] The internet so that's this diagram.
00:35:32 [W0] So here what we are capturing is how one can provide a multi-tenancy model in a DC using oven distributed control plane and a smart Nick in it.
00:35:44 [W0] Tenants lieutenant and a ping tenant the blue tenant has bunch of bell metal servers, which are all interconnected together using an hour and logical topology on top of this bare metal servers.
00:36:00 [W0] They're running cubed is cluster, which is orchestrated by the oven cni-genie.
00:36:15 [W0] Metal servers in the DC that was assigned to them and the parameter service themselves are connected together using logical switch and logical router.
00:36:30 [W0] And then on those bare metal servers, they have their own kubenetes cluster running which is again orchestrated through our and cni as you can see they all use this overlapping eyepiece.
00:36:39 [W0] And and this is all made possible because of the oven virtual topology and they all exit out to the underlay and then
00:36:47 [W0] The reach of the internet the cloud providers can apply their Echoes on the bare metal on the smart neck now to kind of make sure that the blue tenant and the pink tenant are isolated from each other and
00:37:00 [W0] Did from the cloud provider Services itself and the next slide kind of talks to that so in here we show a DC networking with thoughts and L2 within the torch and everything
00:37:16 [W0] Versus L3.
00:37:23 [W0] So here you see Lieutenant has a bunch of Bill bare metal notes assigned to it.
00:37:25 [W0] There's a for bare metal notes assigned to it.
00:37:28 [W0] And each of the base metal node has a blue filter adapter assigned to it.
00:37:36 [W0] And then this forebay metals they form a v tap Network amongst themselves.
00:37:39 [W0] There's a concept of percent zones.
00:37:41 [W0] So you create this island of bell metal servers with blue field on them, and these are
00:37:48 [W0] the the chin of traffic or the oven traffic only kind of happens or occurs between these set of their metal nodes on top of this big metal node is where we build the Cuban this cluster
00:38:02 [W0] Kind of live within this eye line. Similarly. The pink tenant also has its own island with the set of Babymetal servers with the each of them having a blue field associated with it and they have their own logical topology and
00:38:17 [W0] The tunnel is found only between these for Pinkberry Metals in that way.
00:38:25 [W0] There is no interconnect at all between the Blue Island and the pink Island. And then in the same way the cloud provider Services the cloud provider can provide their services either on the physical Network directly or they can provide it on the oven
00:38:40 [W0] So in that way if Lieutenant can access some of the services provided by the provide the provided by the cloud provider through the logical topology or directly the exit out to the underlay and on the underlay, they kind of
00:38:57 [W0] learning on the physical Network itself
00:39:00 [W0] next slide player. Can you proceed?
00:39:05 [W0] Sure.
00:39:08 [W0] Thanks Gary.
00:39:11 [W0] So in addition to the security enhancement and isolation dissolution also enjoys data passed Hydro offload. This is done using the a sub Square framework by mellanox.
00:39:24 [W0] By mellanox accelerated switching and packet processing.
00:39:28 [W0] It's a software and Hardware integrated framework which utilizes mellanox Nick's to accelerate and also the data plane while maintaining the control plane in software.
00:39:40 [W0] So it minimizes the needed changes in kubernative cni-genie other SD ends the solution support different configuration as a review and virt AO and it's available both upstream and in bol.com.
00:39:52 [W0] box
00:39:53 [W0] It consists of three capabilities classification offload action offload and database offload, which are required for switching for switch that the path of flow.
00:40:15 [W0] So as Eurasia explained we worked with oven control plan, which is based on ovhcloud.
00:40:50 [W0] As a user space module and a curved space module and an in the regular configuration each pod is assigned of vth pervert interface.
00:41:10 [W0] So in the traditional model the first packet of a flow will go up to the user space there.
00:41:24 [W0] It will be processed and sent and then suitable rule will be inserted to the colonel.
00:41:36 [W0] So the next pockets of the same flow will go directly to the Cardinal without going through the user space.
00:41:37 [W0] With connect exhale to offload The Suitor the this rule would all not only be inserted to the colonel but also to the hardware to the Kinect is embedded switch in this case the next buckets
00:41:52 [W0] We will not go to the CPU at all, and we'll just be processed in the house other.
00:42:00 [W0] So this way we keep the first packet means architecture but using an additional how to a layer which is the embedded switch which is located on the neck. Each
00:42:22 [W0] the SRV virtual function
00:42:26 [W0] and while the controller or the user inserts the policies by using obvious open Flow rules. These rules are offloaded to the hardware in a transparent way using Linux TC rules.
00:42:41 [W0] Packets are being processed by the neck. And therefore there is no host CPU consumption and we increase throughput and decrease latency
00:42:59 [W0] Was are offloaded to the hardware in a transparent way using Linux TC rules.
00:43:01 [W0] Packets are being processed by the neck. And therefore there is no host CPU consumption and we increase throughput and decrease latency
00:43:02 [W0] Really?
00:43:02 [W0] So a sub square has many benefits it achieves uncompromised performance.
00:43:09 [W0] It saves CPU using the smart neck. You can achieve also full isolation and it facilitates the solution for both bare metal and virtualized clouds.
00:43:20 [W0] So if for virtualized cloud then both Network virtualization storage virtualization were traditionally running in the hypervisor using
00:43:46 [W0] So if for virtualized cloud that both Network virtualization storage virtualization were traditionally running in the hypervisor.
00:43:47 [W0] Using Kinect except we can offload the network data playing by a way leveraging the a sub square frame or when we use blue field. The smart.
00:43:58 [W0] Nick is a bump in a wire also the control plane of the switch can run on this morning on the arm course as well as the storage virtualization and other Security Services. This is also true for bare metal sir.
00:44:14 [W0] service servers
00:44:15 [W0] So I'll summarize bare metal clouds raises the demand for Network isolation since the host is no longer trusted.
00:44:32 [W0] It was relation can be achieved by using a dedicated Hardware which is which it with sit in front of the in front of the host.
00:44:47 [W0] We use Bluefield smart Nick as a bump in the wire to run over and control plane in a secured way running both the virtual switch and the utility is configuring it on the neck.
00:44:59 [W0] We also leverage DS of square framework in order to achieve datapath Hydro offload, which resulted in Boston drastic reduction in host CPU consumption and enhance performance.
00:45:15 [W0] So I'll summarize bare metal clouds raises the demand for Network isolation since the host is no longer trusted.
00:45:17 [W0] Network installation can be achieved by using a dedicated Hardware which is which is with sits in front of the import of the host. We use blue field smart neck as a bump in The Wire
00:45:19 [W0] So we have a few questions actually let and the first of all thank you you have as well. Thank you for attending our session. There are few questions.
00:45:35 [W0] I'm going to read it out and and some questions Leah will help me answer. And then as I try to take a stab at it, the one of the questions was I understand your points, but
00:45:47 [W0] that's what relation of oven add another layer of complexity and therefore possibilities for error in terms of configuration and and handling.
00:45:58 [W0] Yes any any abstraction layer comes with its own configuration challenges and error handling stuff. But the good news is that our one is built with a wide variety of debugging tools spin already
00:46:09 [W0] Very well used very rarely has cancer pressurization solution in openstack world.
00:46:19 [W0] It's been the default reference implementation openstack.
00:46:23 [W0] It's been active for more than a few years now.
00:46:27 [W0] So the tooling around managing the oven is pretty nice.
00:46:32 [W0] nice. So that's one thing the second thing is the virtualization itself shouldn't add any order it in fact because our one is a distributed control plane. It has the global view of your network it already
00:46:42 [W0] Knows where the pots are through that Port binding. So there is no our petal in when when a man is being used.
00:46:54 [W0] I mean not completely normal, but two significant extent the heart is reduced because we already know where the pots are and behind what tunnel endpoint the parts are.
00:47:01 [W0] There are so many instances where we do distributed the hcp distributed DNS. So not of this traffic in your network drastically reduces, so
00:47:12 [W0] I wouldn't say on the controlled from the control plane point of view.
00:47:22 [W0] There are a lot of interesting benefits you get on the data plan on the data plane point of view. The benefits are obviously you can offload the entire ovhcloud Prime to Smart next.
00:47:30 [W0] So then the next question is will this work with snyk Lea good?
00:47:38 [W0] Logged won't work.
00:47:44 [W0] with can you hear me?
00:47:46 [W0] You can hear me.
00:47:48 [W0] Yeah.
00:47:49 [W0] Okay, it won't work.
00:47:54 [W0] It will work with lag.
00:47:58 [W0] Okay, it will work with slag as I said.
00:48:00 [W0] Okay. So what is the current the next question is?
00:48:07 [W0] What is the current maximum number of virtual functions for current and Prospect two network cards?
00:48:10 [W0] Go ahead.
00:48:17 [W0] So the current to take the current limitation is 128 virtual functions purport.
00:48:24 [W0] There is a work to increase it to around 256 purport.
00:48:29 [W0] And in addition there is work on a new interface which is lighter than virtual function, but can provide the same functionalities including everything that we have mentioned here.
00:48:41 [W0] It should be it should be more customer more accustomed to Cloud native environment.
00:48:53 [W0] It's lighter than virtual functions and over there. We expect it called skeleton functions SF and over there. We supposed to scale much more.
00:49:05 [W0] I'll take the next question is well, too.
00:49:09 [W0] So what does the Nick needs to support in order for this to work?
00:49:17 [W0] What is a smart neck?
00:49:21 [W0] So regarding the datapath harder offload all mellanox has and videos Nick's from the last year's connect x 5 and above supports it and regarding the running the ovhcloud
00:49:33 [W0] For that we need CPUs embedded CPUs on the neck.
00:49:41 [W0] This is what we call the smart Nick blue field and we need it in order to run the controller.
00:49:46 [W0] Okay, so does 7:00 work on regular Nick as well or it's just for Bluefield smart make you want to take a stab at that.
00:50:00 [W0] Yeah, so over and without any offload can work on every knative course with that a person how to offload you can work on every mellanox snyk from kinetic 5 and above and that's it.
00:50:17 [W0] Yeah, so over and without any offload can work on every knative course with data path on how to offload.
00:50:18 [W0] You can work on every mellanox snyk from connected 5 and above and that's it.
00:50:19 [W0] it. Yeah.
00:50:19 [W0] Okay. Yeah. Yeah, this is this the solution that we are proposed here is not agnosticism. It's agnostic to vendor-specific Smart need any spotnik that provides ovhcloud flowed will be able to you know
00:50:35 [W0] solution that we are proposed here is not agnostic image agnostic to vendor-specific smartnet any spotnik that provides ovhcloud will be able to you know can use the solution that we presented here Blue Field is just a reference implementation
00:50:39 [W0] So the next question is how do you see about difference in openstack process kubernative?
00:50:47 [W0] It's pretty different.
00:50:48 [W0] I won the the beautiful part of our one is the logical topology.
00:50:59 [W0] So the whole logical topology is different different for kubenetes as compared to openstack.
00:51:08 [W0] So from 7 The Logical topologies different lot of things that a lot of things in how the traffic flow works like between between the V.
00:51:10 [W0] Typically in the past the East-West traffic the north-south traffic traffic to the Internet. So those all change so since the logical topology is different between the two solutions the way it works also is different.
00:51:25 [W0] I think I covered pretty much all the questions in here.
00:51:30 [W0] There are out of time.
00:51:35 [W0] So any other and we are out of time.
00:51:41 [W0] other question can be answered on the networking to work on Slack.
00:51:43 [W0] Thank you.
00:51:46 [W0] Thank you.
