Discreetly Studying the Effects of Individual Traffic Control Functions: FPJJ-6275 - events@cncf.io - Thursday, August 20, 2020 8:21 AM - 376 minutes

Participant: wordly [W] English (US)
Participant: wordly [W0] English (US)

Transcription for wordly [W]

00:02:58 [W] Hello.
00:04:11 [W0] Hello.
00:04:13 [W0] Hello welcome.
00:08:43 [W0] Thank you for coming how we are here today to talk about one of my favorite subjects servicemeshcon.
00:09:08 [W0] And I was actually looking at service not looking at servicemeshcon.
00:09:17 [W0] I was looking at function as a service and serverless applications about a year and half back when I met Lee and then we started looking down this path of servicemeshcon performance impacts
00:09:29 [W0] Here we are.
00:09:31 [W0] Yeah, pretty cake. Clearly you were focused on the well, I don't know are not on the right thing. We have to servicemeshcon is where it's at.
00:09:48 [W0] So that was what about a year ago now, so I'm just ecstatic that we're going to share some of what we've been studying some we've kind of I've sort of talked in this space for a while
00:09:57 [W0] actually a couple of books on on servicemeshcon leave it or not and authoring to concurrently at the moment and
00:10:08 [W0] well, we'll see if I'm I guess still married it out by the time that they publish but servicemeshcon Civ been, you know a real focus of mine.
00:10:19 [W0] There's some questions that we've been that as I've been focusing there and as you and I have been working together now that we've been trying to answer I'm excited that we get to answer some of those today some that we haven't gotten to speak to
00:10:31 [W0] That has I've been focusing there and as you and I have been working together now that we've been trying to answer. I'm excited that we get to answer some of those today some that we haven't gotten to speak to before so
00:10:34 [W0] so just I guess the by the way the one takeaway from this slide here is the URL there is where you can find a copy of this deck and other talks that I give but again,
00:10:50 [W0] Happy that to be joined by pratik if I can get the slide to advance.
00:10:56 [W0] Or Advanced five slides all at once so about that party.
00:11:05 [W0] No worries. So my I am kind of looking into the research aspects of servicemeshcon and looking into interesting findings.
00:11:18 [W0] Are with you folks?
00:11:23 [W0] I have been working on servicemeshcon a while and there are a lot of research interesting research topics in this area, but we will only focus on a couple of them in this particular talk and
00:11:35 [W0] Feel free to reach out to us and join the community to know more and contribute and discuss interesting stuff.
00:11:45 [W0] The interesting thing is this is well a top-level list of why it is that people consider servicemeshcon and this is really more in hard terms of why it is that they consider servicemeshcon interesting.
00:12:02 [W0] Different people break this down in different ways, but there's buckets of functionality that servicemeshcon.
00:12:33 [W0] Observability we like to talk about that in terms of the observability being uniform.
00:12:41 [W0] So the Telemetry that a servicemeshcon will generate based on the requests that it's he's really helpful really enlightening takes the blinders off for a lot of a lot of people and there's a whole category around security
00:12:55 [W0] Of what requests can be made of what services and when and how often how much and there's you know a lot that can be done inside of a mesh and I'm convinced or are I know for sure.
00:13:11 [W0] most of us out there have yet to go horribly deep a lot of folks have yet to
00:13:19 [W0] go beyond even to have accounted for all of the four things that I'm talking about here many people yet to go further than that.
00:13:34 [W0] A lot of people are attracted because of those, you know, those various features other people are attracted to a mesh because sort of similar to what they've experienced with Docker in modernizing their applications in
00:13:44 [W0] Structure your they are able to modernize existing apps with by bringing them onto the mesh again.
00:13:57 [W0] We often speak about servicemeshcon in context of microservices and it's true that sort of the more services you have the more that you will benefit that you will find that you'll benefit from servicemeshcon.
00:14:14 [W0] ins that are not containerized they do a now having said that not all meshes provide for
00:14:23 [W0] Non containerized services, but many of them do.
00:14:27 [W0] The nice thing is you can kinda sort of with an with an Asterix you can kind of get there for free.
00:14:34 [W0] I'm just noticing a behavior of the system. By the way, when I try to advance the slide, it takes about 10 seconds to event. So I'm going to try to click 10 seconds ahead and we'll see if it works other reasons why people come to a mash is
00:14:49 [W0] Baking in infrastructure concerns into their code.
00:14:59 [W0] They don't want to expend a bunch of energy on duplicative infrastructure concerns that that the infrastructure can actually take care of that in the presence of a servicemeshcon lot of those features that we just talked about are
00:15:11 [W0] Avoid having to bake that right use a client Library, which we're going to take a look at one here later to address those concerns rather. They can get retries and and force deadlines cancellations
00:15:26 [W0] Many other things that they might have otherwise had to put in to code your infrastructure can handle that for you.
00:15:38 [W0] So one of the interesting things I think that is arguably the most powerful phenomenon about a servicemeshcon.
00:15:57 [W0] What rate limits should be set for a service?
00:16:01 [W0] How often a service should retry how or any number of these configurations like whether or not a service should.
00:16:11 [W0] Be assigned an individual certification certificate have its own identity have that identity rolled have that identity managed that all of those things?
00:16:26 [W0] I think who's responsible for that in a given environment is often diffused and and Confused between which which of these two teams. That is
00:16:37 [W0] Be assigned an individual certification certificate have its own identity have that identity rolled have that identity managed that all of those things?
00:16:38 [W0] I think who's responsible for that in a given environment is often diffused and and Confused between which which of these two teams. That is
00:16:39 [W0] Is that when you've got a piece of infrastructure like a servicemeshcon to identify all of how you'd like for that Litany of the concerns that I just prattle off how you'd like for those to be configured and behave when you've got that in a set of
00:16:52 [W0] Move it is no longer a finger pointing game rather.
00:17:03 [W0] It's a it's a singularly addressable concern and it becomes self-evident who's taking care of those things.
00:17:10 [W0] So I think that's really powerful the way that those two teams are able to begin to iterate independently of one another are able to move more quickly and they're able to both of them rapidly identify. What a given you know, retry count is
00:17:24 [W0] for a service for example, because it's there and the configuration it's not buried in code or it's not even in the code is not even present.
00:17:35 [W0] So really interesting thing about a servicemeshcon.
00:17:53 [W0] And to understand that there are a variety of different servicemeshcon textures and that's in part because there's a variety of different servicemeshcon what their yes is a bit messy out there.
00:18:08 [W0] Is and how servicemeshcon Lon I'd various Network filters various traffic filters.
00:18:24 [W0] It's important to consider and it's important to understand that there are a variety of different servicemeshcon textures and that's in part because there's a variety of different servicemeshcon what their yes is a bit messy out there.
00:18:25 [W0] He's out.
00:18:26 [W0] I'm a really exciting announcement.
00:18:26 [W0] but it'll make for the 21st servicemeshcon.
00:18:44 [W0] Servicemeshcon said to like your other tools, if you there's a reason that people purchase toolboxes because you generally, you know can't just use one tool for all the jobs that you have and it's kind of the same thing for servicemeshcon.
00:19:15 [W0] Choice out there
00:19:19 [W0] It's not a one-size-fits-all necessarily but because of that there are servicemeshcon specifications that are coming forth. I participate as a maintainer in the servicemeshcon her face, which is essentially a standard.
00:19:34 [W0] Well, it's almost like a if I can if I can.
00:19:38 [W0] We'll see if it will see if people get upset with me for this.
00:19:47 [W0] Please that there's Choice out there.
00:19:55 [W0] It's not a one-size-fits-all necessarily but because of that there are servicemeshcon specifications that are coming forth. I participate as a maintainer in the servicemeshcon her face, which is essentially a standard.
00:19:57 [W0] Well, it's almost like a if I can if I can we'll see if we'll see if people get upset with me for this. But if it's almost like terraform is as a utility
00:19:58 [W0] Probably better analogies here or something like C. And I that containerd network interface gives you one API want one surface to for you as an adapter or you as an integrator to get at servicemeshcon.
00:20:09 [W0] ality irrespective of what servicemeshcon using so that's great.
00:20:13 [W0] There's another one here called Hamlet.
00:20:18 [W0] It is focused on Federation of servicemeshcon logs, if you will so one given servicemeshcon have any number of services in a number of workloads. It is supporting
00:20:25 [W0] when and whether you've got when you've got a second servicemeshcon not
00:20:32 [W0] You often want those things to be able to speak to one another and and that's kind of the focus of Hamlet.
00:20:39 [W0] There's a third specification of their deputies traction that we're going to talk about today and it's the servicemeshcon specification or smps.
00:20:53 [W0] It's really a format that standardizes the way in which you capture and characterize the performance of a servicemeshcon.
00:20:58 [W0] So smps is a vendor neutral standard.
00:21:06 [W0] It's one that layer 5 which is the community that pratik was talking about the servicemeshcon unity has been working on for a while.
00:21:13 [W0] really had gotten started in conversation with the sto performance and scalability team.
00:21:23 [W0] So with the googlers there and since we have been speaking about it talking about it in advancing it and
00:21:28 [W0] We began to discuss it inside of the cncf inside of Sig Network in the cncf.
00:21:36 [W0] So there's a servicemeshcon working group that all are welcome to come and participate in but where we also Advance this specification this spec. Like I said, it directly provides a description how to
00:21:50 [W0] how to characterize the performance of not just your servicemeshcon also the fact that that's highly dependent upon what type of workloads your mesh supporting how your mesh is configured how much you're asking it to do
00:22:08 [W0] Big or small of an environment how many nodes and what size and nodes you're running so many many different variables and so it helps standardize how you capture that from there?
00:22:24 [W0] You can then facilitates some interesting things and one of those is an Apples to Apples comparison be either between servicemeshcon deployments of the same type are different types, but maybe more importantly it be
00:22:36 [W0] Gauging that for yourself and having a standard way to capture the performance of your mesh the configuration of your mesh.
00:22:51 [W0] There is an implementation of this specification today the canonical implementation, you know tool called measure e masteries the servicemeshcon Judgment playing it's a multi measurement plane is
00:23:02 [W0] He's the servicemeshcon Judgment plane. It's a multi measurement plane is compatible with SMI.
00:23:06 [W0] It was a that compatibility was announced.
00:23:09 [W0] So we're launched or announced that the launch of SMI is now compatible with the servicemeshcon formants spec. There are well, gosh, I think three interns being sort of sort of working around SMI
00:23:21 [W0] Around Envoy and around measure e through the Google summer of code through the cncf and through Community Bridge.
00:23:36 [W0] Hopefully through Google season of Doc's here shortly as well.
00:23:45 [W0] And so the part of the there's a few different goals of this tool and it's important that we talk about this tool because it's what's been facilitating our performance analysis the analysis that we're going to speak about here in a moment.
00:23:52 [W0] Originally, there was a lot of asked in the community when people are first learning a match. They often ask themselves.
00:24:01 [W0] Well, hey, what's the overhead of One Versus the next which one should I use and we've publicly spoken on this, you know a number of times, please disregard these like,
00:24:13 [W0] You're in something ago old stats rather.
00:24:22 [W0] It was really just trying to show that like hey when console first came out really it was only facilitating in TLS.
00:24:27 [W0] It's really wasn't doing much in comparison to maybe sto that was you asking it to do a lot more. And so anyway, one of the things that it was intended to do is that it's still still does that it also does a bit of best practices analysis.
00:24:41 [W0] So if you're running a mesh and doing it, you're using mesh Ritu
00:24:44 [W0] Do some Performance Management. You can also do some configuration management.
00:24:54 [W0] It will analyze the configuration of your mesh and tell you if you're you're doing it wrong, or maybe it'll tell you you're doing it right so but well, it's I think interesting to look at where a tool like this fits in
00:25:04 [W0] Servicemeshcon you talk about the data plane and the control plane. This is a generic view of what a data plan and control plane.
00:25:15 [W0] Often look like you can also layer on a management plane.
00:25:26 [W0] For those of you who are network Engineers or have done Network Administration.
00:25:27 [W0] I think that that's a familiar term and in this meshmark planes are capable of any number of different things that the functionality that we're talking about here is really around load generation and statistical analysis of the performance of a mesh.
00:25:41 [W0] What were showing here is that measure e is Will gives you sort of choice of load balancer.
00:25:49 [W0] I'm sorry.
00:25:53 [W0] I'm not load balancer load generator between 40 o w RK to Nighthawk.
00:25:56 [W0] And it will generate load and profit do an analysis today.
00:26:02 [W0] It'll do that against any one of these six servicemeshcon.
00:26:26 [W0] In the course of this has been a discussion of pratik and mine and some others for quite some time on.
00:26:34 [W0] well trying to make sure that we're using representative workloads as we're studying and analyzing traffic servicemeshcon formance and it kind of traffic statistics that
00:26:49 [W0] Highly dependent upon the environment that you're doing performing your study in part of that environment is your workload your app.
00:27:01 [W0] And so it was just a couple of a few months ago that we'd work to create a sample app called image Hub.
00:27:06 [W0] It's actually if you're familiar with Docker Hub.
00:27:10 [W0] It's really similar is it's from something of a ripoff of the docker Hub.
00:27:12 [W0] It doesn't actually work.
00:27:13 [W0] It's just a sample app, but the interesting thing in the reason that we had created this sample app as opposed to using the
00:27:23 [W0] ER sample apps that that measure he's supports is because we wanted to highlight the fact that it's not just fine grain traffic control that a servicemeshcon facilitate but rather
00:27:35 [W0] close to using the other sample apps that that measure he's supports is because we wanted to highlight the fact that it's not just
00:27:36 [W0] fine grain traffic control that a servicemeshcon facilitate but rather not rather, but but it is also the fact that a servicemeshcon.
00:27:42 [W0] Be told to have quite a bit of intelligence in its data plane.
00:27:47 [W0] And the example here that we that we won't take you through today, but we'll take you through some Network performance instead. But the example here is the notion that the app has much like any sass
00:28:03 [W0] But the example here is the notion that the app has much like any sass offering that you might use you'd sign up as a user and it and you have different subscription plans that you sign up for. Well this app has subscription plans,
00:28:12 [W0] any way of enforcing people to stay within the bounds of those plans and so we use a servicemeshcon intelligently rate limit and kind of filter a user's usage based on
00:28:28 [W0] And and that intelligence happens not within the app, but rather within the data plane.
00:28:37 [W0] So hopefully this sample app enlightens expand some of your horizons to well expect more from your infrastructure to leverage your infrastructure a bit more.
00:28:51 [W0] This is it's a very simple app.
00:28:53 [W0] It's a to Docker containers.
00:28:59 [W0] There's a view UI or UI written in view running in one container.
00:29:03 [W0] There's a back-end. That's your image your Docker image Storage storage service.
00:29:07 [W0] And it runs a set of apis and runs and storage service.
00:29:17 [W0] We're able to take this sample app and deploy it on top of a mesh and the mesh that we've chosen is console for a couple different reasons one of which is because console uses Envoy as its data plane
00:29:26 [W0] and Envoy is capable of
00:29:30 [W0] incorporating webassembly modules while some modules will talk about webassembly more in a moment, but there's a lot of power in the ability to load
00:29:45 [W0] McCleary load webassembly filters these filters are are highly capable.
00:29:57 [W0] They're able to see the same network traffic the same requests that come through that are addressed to your services.
00:30:02 [W0] They're able to interpret grab those requests facilitated through Envoy in this case and apply some logic to them.
00:30:09 [W0] And an example of that logic was what? I was just speaking about a moment ago.
00:30:14 [W0] which is to say that the this image of example app uses webassembly filter runs inside of envoy and this particular filter is written in Rust
00:30:31 [W0] About a moment ago.
00:30:31 [W0] which is to say that the this image how big's a sample app uses webassembly filter runs inside of envoy and this particular filter is written in Rust
00:30:33 [W0] reasons it and I won't read describe what the sample app does because that's kind of for a different talk but suffice to say on I guess I should note that today console
00:30:47 [W0] With this configuration and the use of wasum filters out of the box and I won't speak on behalf of the product management team there. I would just I would say that I think you know your best get
00:31:03 [W0] I would say to pay attention and maybe I'm look out for that possibility.
00:31:11 [W0] So with that stage set we're going to talk about performance and there's a number of interesting things that pratik and I and the collection of those who are working in this community
00:31:23 [W0] Um uncovered and we're here to share a few of these things with you today to the extent that we can I think as we do it's probably important to recognize that, you know, when we're talking about performance and measuring performance.
00:31:38 [W0] Wow, look at the time fly.
00:31:42 [W0] I just realized that I've been talking that so we'll all move.
00:31:45 [W0] will pick up the pace when we're talking about performance.
00:31:48 [W0] It's not easy to hold all these variables in in place.
00:32:02 [W0] And so that's also why when you see a like a bunch of Benchmark of Statistics published its sometimes I will cringe I'll just speak for myself there because it's so easy to get a performance
00:32:12 [W0] Wrong to do one very well, although sometimes takes a PhD student.
00:32:19 [W0] So thank goodness for pratik.
00:32:21 [W0] I said that we were going to talk about webassembly very interesting technology that's been around for a little while and is now coming toward the servicemeshcon.
00:32:42 [W0] us here have been a couple of ways of looking at it sort of assessing the overhead of the the power of a mesh like hey the and you would expect probably that the more that you asked a servicemeshcon do the more
00:32:57 [W0] Set of those traffic that traffic that the more overhead that might be incurred that there's different ways of measuring overhead.
00:33:09 [W0] We're going to talk a lot about latency today latency being one of those critical pieces of information that you know, you want to provide your users a good experience with or consumers of your services with a good experience and what we've begun to do
00:33:21 [W0] Kind of three different ways of providing filtering traffic filtering in your data plane.
00:33:37 [W0] One is to is to well continue to use client libraries language specific client libraries, or just the standard libraries of the language itself to facilitate.
00:33:45 [W0] Network functions in this case, what we're talking about here is rate limiting specifically, this is just what we had studied here and the statistics that we're seeing is in the environment that we were within we went
00:34:01 [W0] Nations set the rate limit to be well, so yeah, we have run.
00:34:17 [W0] Yeah, let me take up the
00:34:21 [W0] Results in the way that we tested this in two different environments if I may like we implemented a rate limiting feature inside
00:34:37 [W0] Application itself and then we also implemented a rate limiting feature as in our rust webassembly and plugged it in inside the envoy sidecar proxy and remove the
00:34:53 [W0] Occasion what we would have expected is that if the rate limiting is inside the application itself, then it should perform better. But since it's webassembly and the
00:35:09 [W0] Of then it should perform better. But since it's webassembly and the network traffic that is coming through the sidecar proxy and being routed through the sidecar proxy to the application can be
00:35:18 [W0] To the sidecar proxy to the application can be processed in the site card itself and it does not need to go have the overhead of being transferred to the application again because of
00:35:28 [W0] The fair amount of benefits the experiment the rate limiting is set at $100 per second. So when we send a hundred
00:35:44 [W0] No errors. And when in the knative go client, we see latency of about three milliseconds.
00:35:58 [W0] Whereas if we do not have the rate limiting feature in the go but we put it inside the walls.
00:36:07 [W0] Mm. We see a lower latency of 2.1 milliseconds again to reiterate there are no errors in here, like none of the requests are being stopped and still we see
00:36:14 [W0] See a benefit in latency as in as we go higher to a higher requests like 500 requests per second.
00:36:28 [W0] We expect hundred of them to pass and 400 of them to fail and you can see that there is even more significant benefits when we see the request being filtered out
00:36:39 [W0] sidecar proxy itself
00:36:45 [W0] measure has this amazing feature where you can leave the RPS blank and it will automatically figure try to load the service as much as possible like try to measure how many requested
00:37:01 [W0] Try to load the service as much as possible. Like try to measure how many requested can actually send in a closed loop circuit and with the knative go pliant.
00:37:09 [W0] We saw that it can only service 4400 requests.
00:37:17 [W0] Well a hundred of them are go through and the rest 4100 are errors error messages, but since they go to the client library and then return the error.
00:37:24 [W0] Are they incur a larger overhead? And hence a lower rate requests per second on the other hand since we have a was on filter inside the sidecar container.
00:37:39 [W0] The requests were being filtered way more efficiently when we use this feature and we saw almost like a thousand more
00:37:51 [W0] I although that this is a very simplistic example and but when we see that even in a simple example, we see such significant difference in Layton sees so as and when the
00:38:09 [W0] Gets larger and more complex. These Layton sees will grow only more significant one.
00:38:25 [W0] You think like not new but like good thing about was mm is you can only use was mm filters as and when you need yeah, like we do with
00:38:33 [W0] The thing about was mm is you can only use was mm filters as and when you need like we do with kernel modules in our old age days. We can insert corn kernel modules and remove them
00:38:39 [W0] All these days we can insert corn kernel modules and remove them to lower our binary overhead.
00:38:45 [W0] We can do the same with was mmm, if we need a particular feature we can load it up and if we don't need it, we can remove it and that speaks to the power that was a provides us
00:38:55 [W0] R to the power that was a provides us with in servicemeshcon these microservices infrastructures, and so
00:39:06 [W0] And so we'll try to get the the slide Advanced.
00:39:10 [W0] I'll take a moment.
00:39:11 [W0] We were just realize we're back when and so to Karthik. Please interrupt me as I go here, but outside of just looking at the
00:39:25 [W0] In how you take a given Network function like rate limiting and how you implement that and the difference is in the overhead of those implementations.
00:39:39 [W0] You also can take different types of network functions like path based routing round-robin load balancing or context-based path routing and analyze compare them as
00:39:52 [W0] This and compare and contrast their speed and in again in the same environment that we were using for the other tests that we just talked about.
00:40:08 [W0] We were seeing it really just a single path based routing being having a higher performance while en route rambha drowned Robin load balancing taking
00:40:19 [W0] based routing being having a higher performance while en route Ramadan round-robin load balancing taking a slight more hit in terms of latency and in terms of throughput actually
00:40:25 [W0] in terms of latency and in terms of throughput actually for the given time period and then context based routing as you might expect is a little more powerful and it also is comparatively slower than the other two
00:40:36 [W0] and so we'll see if
00:40:38 [W0] by the time we take some Q&A will have some data points to share their specifically.
00:40:47 [W0] lastly
00:40:49 [W0] we will say that.
00:40:53 [W0] And pretty cat, you know again feel free to interrupt on this.
00:41:05 [W0] I'm I'm trying to hurry up but just yeah, so this was a great experiment that came by us while we were running other experiments is how do we optimize our average response time? So,
00:41:13 [W0] He tried to run one of the tests. And as you can see in this graph, we saw that delete the response that we got could be bucketed in separate chunks that is in bucket one.
00:41:29 [W0] Could be pocketed in separate chunks that is in bucket one.
00:41:31 [W0] We did get a few of these responses but majorette majority of the responses came back in bucket 3 that means our services able to process requests as low as I'm sorry for the small Figures.
00:41:44 [W0] Process requests as low as I'm sorry for the small Figures.
00:41:47 [W0] It's around like fourteen to eighteen twelve to eighteen milliseconds, but still majority of our requests come back around 60 50 to 60 milliseconds.
00:41:59 [W0] That means that there is potential in our service to respond in 12 milliseconds and we try to figure out how do we optimize this?
00:42:09 [W0] this and if we take the the smallest segment bucket that is bucket 1 and we try to see what is the maximum number of requests that were handled in the histogram
00:42:24 [W0] And if we take the smallest segment bucket that is bucket 1 and we try to see what is the maximum number of requests that were handled in the histogram.
00:42:25 [W0] At 14 is was around 45 to 50 requests and we divided by it by the number of threads that we were running that is two threads and when we did that lo and behold we got most of our
00:42:39 [W0] In the bucket 1 range Lee. Could you share that?
00:42:51 [W0] Yeah, so we reran the tests with a lawyer requests per second around 25 to 30 and we saw that most of the responses received in bucket
00:43:02 [W0] That is Dunn's of potential that we should Gather By looking at these performance metrics and there is tons of optimizations that we can do in this scenario as well,
00:43:19 [W0] Tons of optimizations that we can do in this scenario as well.
00:43:20 [W0] You know and to share just a couple of those best practices given give them the time that we have left here.
00:43:30 [W0] It's to probably go take advantage.
00:43:34 [W0] There's actually a long list of servicemeshcon.
00:43:49 [W0] bought as many people have questions about performance as they do that measure e as an open source tool has been created to empower people to easily reproduce these tests to persist your results to Baseline your environments to use different load generators because
00:44:04 [W0] Colin Alice's differently to compare your configuration changes to test your workloads on and off the mesh to do it against different meshes, like sort of on and on with that
00:44:20 [W0] I commend you give give that to a shot but also recommend that you jump into this community here where pratik and I both hang out what a bit and you can have hanging out.
00:44:33 [W0] We're going to be hanging out taking QA right now. So please please bring your questions.
00:44:39 [W0] Thanks.
00:44:41 [W0] Boo.
00:44:51 [W0] Hey guys, we're here to answer any questions.
00:45:00 [W0] Sorry about the screen share for some reason.
00:45:03 [W0] It was off. Hope you all followed the link shed by lie on his page.
00:45:08 [W0] Boy Kudo, 'z to those who stuck it out throughout this talk.
00:45:25 [W0] Yeah, the TM video being out of sync has been well, it's been impressive to see those stick around for the end.
00:45:28 [W0] So I know we've seen a couple of good questions coming through.
00:45:31 [W0] So we're trying to answer a few of those.
00:45:33 [W0] It'll be a little bit easier.
00:45:34 [W0] Actually I think to verbally speak to those so that's nice.
00:45:37 [W0] One of them was just about rust as you know, in terms of its performance and and using rust based.
00:45:52 [W0] Well Russ paste filters inside of awesome bm2 to characterize part of that answer.
00:45:58 [W0] Is that Envoy itself is written in C++.
00:46:09 [W0] There are certainly performance comparisons that you can do between popular languages in this space go.
00:46:11 [W0] Let's plus rust, you know, if there were folks from the linkerd e project here, they would espouse the the performance and other characteristics
00:46:26 [W0] Rust and I think that by and large is probably is the case that rust would be have some both performance characteristics and some others that that if you're doing intense intense
00:46:41 [W0] Our traffic within within a wasp environment that you're seeking out rust or even outside of wasp environments linkerd EU surface. Proxy is written in rest if you don't use wasum now and
00:46:56 [W0] If you are using Envoy as your service proxy Envoy is written in C++.
00:47:06 [W0] And so writing a knative Envoy filter in C++, you know, well
00:47:11 [W0] He's going to be a bit quicker. And and the reason I pause for a moment.
00:47:21 [W0] It's a that pratik as well in the time that we've recorded. This talk published additional metrics are some additional.
00:47:27 [W0] Analysis, maybe I'll try to share the link. I think it's fresh as if
00:47:36 [W0] yeah links are we expect drugs to be quite performance because the language was developed to be performant. But definitely it's a great question that we can do
00:47:55 [W0] Research to see which among as a war zone filter itself if rust or C++, which of them is performed but like I would expect trust or C C++ to be one of the best
00:48:10 [W0] in terms of performance
00:48:12 [W0] So there's a question does the mesh really help to manage and configure was on filters. If not extension able to do it.
00:48:35 [W0] It's a very interesting question.
00:48:37 [W0] was some filters in servicemeshcon ill very much an active Development Little scope for a lot of servicemeshcon.
00:48:52 [W0] These servicemeshcon Das and when the servicemeshcon adds supporting was in filters, we will try to integrate the best we can and if there are apis will try to write an adapter so that you can configure
00:49:07 [W0] in the servicemeshcon generic fashion
00:49:12 [W0] for their critiques answer. I'll go ahead and publish a
00:49:21 [W0] Link where those that are curious about use of measure e and measure easily injection or management of the loading and sort of dynamically reloading of
00:49:37 [W0] Some filters in Envoy based servicemeshcon is the link that was just shared is to sample app called image Hub.
00:49:51 [W0] And so that sample app is orchestrated by measuring it deploys actually into a console-based servicemeshcon.
00:50:07 [W0] The use of wasum filters but because it is using Envoy has it sidecar technology it inherently does and so near as I understand.
00:50:21 [W0] that's a fairly near term roadmap item for the console team. But that image of that sample app is an interesting one to play with in part because one part because I think it
00:50:32 [W0] Help expand people's Horizons in part to think Deepak and had a question earlier about filtering Ingress traffic and manipulating and doing some things with it.
00:50:43 [W0] It's an image of that sample app is an interesting take on it actually enforces multi-tenancy and a bit of a subscription based pricing. That intelligence
00:50:59 [W0] Is not built into the sample app itself, but that intelligence comes from the wasps and filter that you can see in that sample app.
00:51:11 [W0] So pretty cool to explore the power of the data plane that way.
00:51:14 [W0] Which I'm you know, so just I don't know if there's I guess there is another question.
00:51:27 [W0] And so the question is in multi-os clusters.
00:51:38 [W0] You have a recommendation for Windows notes.
00:51:39 [W0] I don't know.
00:51:46 [W0] I don't know. I don't know how to answer that because like I always use Linux.
00:51:54 [W0] Yeah, there is a particular particular attending with us Docker Captain. You know who's in attendance
00:52:05 [W0] Much more of a WSL to Windows expert and so I had no doubt.
00:52:13 [W0] He'll be in the same slack Channel with us as we conclude.
00:52:19 [W0] I'll will point you to The Experts there.
00:52:21 [W0] We are not able to hear you.
00:52:52 [W0] Oh, okay.
00:52:59 [W0] Yeah, it's not a volume thing, huh?
00:53:00 [W0] Legends
00:53:08 [W0] Yeah, so feel free to connect with us on the slack and any additional questions.
00:53:25 [W0] will try to get the recording fixed before we make it available for you for with the viewing General viewing.
00:53:33 [W0] Cool, very good.
00:53:43 [W0] I don't know if you guys can hear me now or not, but I'm going to say something anyway, which is yeah, I would.
00:53:47 [W0] okay good. Well since this talk couldn't have been any more of a disaster. Friendliness.
00:53:53 [W0] Yeah.
00:53:58 [W0] I don't think things and the microphone going to Ed and I'll just I'll say that I always say thanks pratik actually for all the work that you've done this very interesting analysis that you've done.
00:54:08 [W0] So please do
00:54:08 [W0] Um that you were able to use Mastery as much as you did. I think you about war the rubber off the tires on it, but I'm really excited about empowering people with the knowledge of how much it each of these functions costs.
00:54:24 [W0] And it's thanks to your work that that they'll be able to know you're going to know what it costs to.
00:54:32 [W0] Weight limit of something.
00:54:36 [W0] Thanks leave for the opportunity.
00:54:38 [W0] It was great.
00:54:39 [W0] Very good. Well, I think we're out of here.
00:54:45 [W0] We'll see you guys in slack.
00:54:46 [W0] Yeah.
00:54:47 [W0] See you guys.

Transcription for wordly [W0]

00:02:58 [W] Hello.
00:04:11 [W0] Hello.
00:04:13 [W0] Hello welcome.
00:08:43 [W0] Thank you for coming how we are here today to talk about one of my favorite subjects servicemeshcon.
00:09:08 [W0] And I was actually looking at service not looking at servicemeshcon.
00:09:17 [W0] I was looking at function as a service and serverless applications about a year and half back when I met Lee and then we started looking down this path of servicemeshcon performance impacts
00:09:29 [W0] Here we are.
00:09:31 [W0] Yeah, pretty cake. Clearly you were focused on the well, I don't know are not on the right thing. We have to servicemeshcon is where it's at.
00:09:48 [W0] So that was what about a year ago now, so I'm just ecstatic that we're going to share some of what we've been studying some we've kind of I've sort of talked in this space for a while
00:09:57 [W0] actually a couple of books on on servicemeshcon leave it or not and authoring to concurrently at the moment and
00:10:08 [W0] well, we'll see if I'm I guess still married it out by the time that they publish but servicemeshcon Civ been, you know a real focus of mine.
00:10:19 [W0] There's some questions that we've been that as I've been focusing there and as you and I have been working together now that we've been trying to answer I'm excited that we get to answer some of those today some that we haven't gotten to speak to
00:10:31 [W0] That has I've been focusing there and as you and I have been working together now that we've been trying to answer. I'm excited that we get to answer some of those today some that we haven't gotten to speak to before so
00:10:34 [W0] so just I guess the by the way the one takeaway from this slide here is the URL there is where you can find a copy of this deck and other talks that I give but again,
00:10:50 [W0] Happy that to be joined by pratik if I can get the slide to advance.
00:10:56 [W0] Or Advanced five slides all at once so about that party.
00:11:05 [W0] No worries. So my I am kind of looking into the research aspects of servicemeshcon and looking into interesting findings.
00:11:18 [W0] Are with you folks?
00:11:23 [W0] I have been working on servicemeshcon a while and there are a lot of research interesting research topics in this area, but we will only focus on a couple of them in this particular talk and
00:11:35 [W0] Feel free to reach out to us and join the community to know more and contribute and discuss interesting stuff.
00:11:45 [W0] The interesting thing is this is well a top-level list of why it is that people consider servicemeshcon and this is really more in hard terms of why it is that they consider servicemeshcon interesting.
00:12:02 [W0] Different people break this down in different ways, but there's buckets of functionality that servicemeshcon.
00:12:33 [W0] Observability we like to talk about that in terms of the observability being uniform.
00:12:41 [W0] So the Telemetry that a servicemeshcon will generate based on the requests that it's he's really helpful really enlightening takes the blinders off for a lot of a lot of people and there's a whole category around security
00:12:55 [W0] Of what requests can be made of what services and when and how often how much and there's you know a lot that can be done inside of a mesh and I'm convinced or are I know for sure.
00:13:11 [W0] most of us out there have yet to go horribly deep a lot of folks have yet to
00:13:19 [W0] go beyond even to have accounted for all of the four things that I'm talking about here many people yet to go further than that.
00:13:34 [W0] A lot of people are attracted because of those, you know, those various features other people are attracted to a mesh because sort of similar to what they've experienced with Docker in modernizing their applications in
00:13:44 [W0] Structure your they are able to modernize existing apps with by bringing them onto the mesh again.
00:13:57 [W0] We often speak about servicemeshcon in context of microservices and it's true that sort of the more services you have the more that you will benefit that you will find that you'll benefit from servicemeshcon.
00:14:14 [W0] ins that are not containerized they do a now having said that not all meshes provide for
00:14:23 [W0] Non containerized services, but many of them do.
00:14:27 [W0] The nice thing is you can kinda sort of with an with an Asterix you can kind of get there for free.
00:14:34 [W0] I'm just noticing a behavior of the system. By the way, when I try to advance the slide, it takes about 10 seconds to event. So I'm going to try to click 10 seconds ahead and we'll see if it works other reasons why people come to a mash is
00:14:49 [W0] Baking in infrastructure concerns into their code.
00:14:59 [W0] They don't want to expend a bunch of energy on duplicative infrastructure concerns that that the infrastructure can actually take care of that in the presence of a servicemeshcon lot of those features that we just talked about are
00:15:11 [W0] Avoid having to bake that right use a client Library, which we're going to take a look at one here later to address those concerns rather. They can get retries and and force deadlines cancellations
00:15:26 [W0] Many other things that they might have otherwise had to put in to code your infrastructure can handle that for you.
00:15:38 [W0] So one of the interesting things I think that is arguably the most powerful phenomenon about a servicemeshcon.
00:15:57 [W0] What rate limits should be set for a service?
00:16:01 [W0] How often a service should retry how or any number of these configurations like whether or not a service should.
00:16:11 [W0] Be assigned an individual certification certificate have its own identity have that identity rolled have that identity managed that all of those things?
00:16:26 [W0] I think who's responsible for that in a given environment is often diffused and and Confused between which which of these two teams. That is
00:16:37 [W0] Be assigned an individual certification certificate have its own identity have that identity rolled have that identity managed that all of those things?
00:16:38 [W0] I think who's responsible for that in a given environment is often diffused and and Confused between which which of these two teams. That is
00:16:39 [W0] Is that when you've got a piece of infrastructure like a servicemeshcon to identify all of how you'd like for that Litany of the concerns that I just prattle off how you'd like for those to be configured and behave when you've got that in a set of
00:16:52 [W0] Move it is no longer a finger pointing game rather.
00:17:03 [W0] It's a it's a singularly addressable concern and it becomes self-evident who's taking care of those things.
00:17:10 [W0] So I think that's really powerful the way that those two teams are able to begin to iterate independently of one another are able to move more quickly and they're able to both of them rapidly identify. What a given you know, retry count is
00:17:24 [W0] for a service for example, because it's there and the configuration it's not buried in code or it's not even in the code is not even present.
00:17:35 [W0] So really interesting thing about a servicemeshcon.
00:17:53 [W0] And to understand that there are a variety of different servicemeshcon textures and that's in part because there's a variety of different servicemeshcon what their yes is a bit messy out there.
00:18:08 [W0] Is and how servicemeshcon Lon I'd various Network filters various traffic filters.
00:18:24 [W0] It's important to consider and it's important to understand that there are a variety of different servicemeshcon textures and that's in part because there's a variety of different servicemeshcon what their yes is a bit messy out there.
00:18:25 [W0] He's out.
00:18:26 [W0] I'm a really exciting announcement.
00:18:26 [W0] but it'll make for the 21st servicemeshcon.
00:18:44 [W0] Servicemeshcon said to like your other tools, if you there's a reason that people purchase toolboxes because you generally, you know can't just use one tool for all the jobs that you have and it's kind of the same thing for servicemeshcon.
00:19:15 [W0] Choice out there
00:19:19 [W0] It's not a one-size-fits-all necessarily but because of that there are servicemeshcon specifications that are coming forth. I participate as a maintainer in the servicemeshcon her face, which is essentially a standard.
00:19:34 [W0] Well, it's almost like a if I can if I can.
00:19:38 [W0] We'll see if it will see if people get upset with me for this.
00:19:47 [W0] Please that there's Choice out there.
00:19:55 [W0] It's not a one-size-fits-all necessarily but because of that there are servicemeshcon specifications that are coming forth. I participate as a maintainer in the servicemeshcon her face, which is essentially a standard.
00:19:57 [W0] Well, it's almost like a if I can if I can we'll see if we'll see if people get upset with me for this. But if it's almost like terraform is as a utility
00:19:58 [W0] Probably better analogies here or something like C. And I that containerd network interface gives you one API want one surface to for you as an adapter or you as an integrator to get at servicemeshcon.
00:20:09 [W0] ality irrespective of what servicemeshcon using so that's great.
00:20:13 [W0] There's another one here called Hamlet.
00:20:18 [W0] It is focused on Federation of servicemeshcon logs, if you will so one given servicemeshcon have any number of services in a number of workloads. It is supporting
00:20:25 [W0] when and whether you've got when you've got a second servicemeshcon not
00:20:32 [W0] You often want those things to be able to speak to one another and and that's kind of the focus of Hamlet.
00:20:39 [W0] There's a third specification of their deputies traction that we're going to talk about today and it's the servicemeshcon specification or smps.
00:20:53 [W0] It's really a format that standardizes the way in which you capture and characterize the performance of a servicemeshcon.
00:20:58 [W0] So smps is a vendor neutral standard.
00:21:06 [W0] It's one that layer 5 which is the community that pratik was talking about the servicemeshcon unity has been working on for a while.
00:21:13 [W0] really had gotten started in conversation with the sto performance and scalability team.
00:21:23 [W0] So with the googlers there and since we have been speaking about it talking about it in advancing it and
00:21:28 [W0] We began to discuss it inside of the cncf inside of Sig Network in the cncf.
00:21:36 [W0] So there's a servicemeshcon working group that all are welcome to come and participate in but where we also Advance this specification this spec. Like I said, it directly provides a description how to
00:21:50 [W0] how to characterize the performance of not just your servicemeshcon also the fact that that's highly dependent upon what type of workloads your mesh supporting how your mesh is configured how much you're asking it to do
00:22:08 [W0] Big or small of an environment how many nodes and what size and nodes you're running so many many different variables and so it helps standardize how you capture that from there?
00:22:24 [W0] You can then facilitates some interesting things and one of those is an Apples to Apples comparison be either between servicemeshcon deployments of the same type are different types, but maybe more importantly it be
00:22:36 [W0] Gauging that for yourself and having a standard way to capture the performance of your mesh the configuration of your mesh.
00:22:51 [W0] There is an implementation of this specification today the canonical implementation, you know tool called measure e masteries the servicemeshcon Judgment playing it's a multi measurement plane is
00:23:02 [W0] He's the servicemeshcon Judgment plane. It's a multi measurement plane is compatible with SMI.
00:23:06 [W0] It was a that compatibility was announced.
00:23:09 [W0] So we're launched or announced that the launch of SMI is now compatible with the servicemeshcon formants spec. There are well, gosh, I think three interns being sort of sort of working around SMI
00:23:21 [W0] Around Envoy and around measure e through the Google summer of code through the cncf and through Community Bridge.
00:23:36 [W0] Hopefully through Google season of Doc's here shortly as well.
00:23:45 [W0] And so the part of the there's a few different goals of this tool and it's important that we talk about this tool because it's what's been facilitating our performance analysis the analysis that we're going to speak about here in a moment.
00:23:52 [W0] Originally, there was a lot of asked in the community when people are first learning a match. They often ask themselves.
00:24:01 [W0] Well, hey, what's the overhead of One Versus the next which one should I use and we've publicly spoken on this, you know a number of times, please disregard these like,
00:24:13 [W0] You're in something ago old stats rather.
00:24:22 [W0] It was really just trying to show that like hey when console first came out really it was only facilitating in TLS.
00:24:27 [W0] It's really wasn't doing much in comparison to maybe sto that was you asking it to do a lot more. And so anyway, one of the things that it was intended to do is that it's still still does that it also does a bit of best practices analysis.
00:24:41 [W0] So if you're running a mesh and doing it, you're using mesh Ritu
00:24:44 [W0] Do some Performance Management. You can also do some configuration management.
00:24:54 [W0] It will analyze the configuration of your mesh and tell you if you're you're doing it wrong, or maybe it'll tell you you're doing it right so but well, it's I think interesting to look at where a tool like this fits in
00:25:04 [W0] Servicemeshcon you talk about the data plane and the control plane. This is a generic view of what a data plan and control plane.
00:25:15 [W0] Often look like you can also layer on a management plane.
00:25:26 [W0] For those of you who are network Engineers or have done Network Administration.
00:25:27 [W0] I think that that's a familiar term and in this meshmark planes are capable of any number of different things that the functionality that we're talking about here is really around load generation and statistical analysis of the performance of a mesh.
00:25:41 [W0] What were showing here is that measure e is Will gives you sort of choice of load balancer.
00:25:49 [W0] I'm sorry.
00:25:53 [W0] I'm not load balancer load generator between 40 o w RK to Nighthawk.
00:25:56 [W0] And it will generate load and profit do an analysis today.
00:26:02 [W0] It'll do that against any one of these six servicemeshcon.
00:26:26 [W0] In the course of this has been a discussion of pratik and mine and some others for quite some time on.
00:26:34 [W0] well trying to make sure that we're using representative workloads as we're studying and analyzing traffic servicemeshcon formance and it kind of traffic statistics that
00:26:49 [W0] Highly dependent upon the environment that you're doing performing your study in part of that environment is your workload your app.
00:27:01 [W0] And so it was just a couple of a few months ago that we'd work to create a sample app called image Hub.
00:27:06 [W0] It's actually if you're familiar with Docker Hub.
00:27:10 [W0] It's really similar is it's from something of a ripoff of the docker Hub.
00:27:12 [W0] It doesn't actually work.
00:27:13 [W0] It's just a sample app, but the interesting thing in the reason that we had created this sample app as opposed to using the
00:27:23 [W0] ER sample apps that that measure he's supports is because we wanted to highlight the fact that it's not just fine grain traffic control that a servicemeshcon facilitate but rather
00:27:35 [W0] close to using the other sample apps that that measure he's supports is because we wanted to highlight the fact that it's not just
00:27:36 [W0] fine grain traffic control that a servicemeshcon facilitate but rather not rather, but but it is also the fact that a servicemeshcon.
00:27:42 [W0] Be told to have quite a bit of intelligence in its data plane.
00:27:47 [W0] And the example here that we that we won't take you through today, but we'll take you through some Network performance instead. But the example here is the notion that the app has much like any sass
00:28:03 [W0] But the example here is the notion that the app has much like any sass offering that you might use you'd sign up as a user and it and you have different subscription plans that you sign up for. Well this app has subscription plans,
00:28:12 [W0] any way of enforcing people to stay within the bounds of those plans and so we use a servicemeshcon intelligently rate limit and kind of filter a user's usage based on
00:28:28 [W0] And and that intelligence happens not within the app, but rather within the data plane.
00:28:37 [W0] So hopefully this sample app enlightens expand some of your horizons to well expect more from your infrastructure to leverage your infrastructure a bit more.
00:28:51 [W0] This is it's a very simple app.
00:28:53 [W0] It's a to Docker containers.
00:28:59 [W0] There's a view UI or UI written in view running in one container.
00:29:03 [W0] There's a back-end. That's your image your Docker image Storage storage service.
00:29:07 [W0] And it runs a set of apis and runs and storage service.
00:29:17 [W0] We're able to take this sample app and deploy it on top of a mesh and the mesh that we've chosen is console for a couple different reasons one of which is because console uses Envoy as its data plane
00:29:26 [W0] and Envoy is capable of
00:29:30 [W0] incorporating webassembly modules while some modules will talk about webassembly more in a moment, but there's a lot of power in the ability to load
00:29:45 [W0] McCleary load webassembly filters these filters are are highly capable.
00:29:57 [W0] They're able to see the same network traffic the same requests that come through that are addressed to your services.
00:30:02 [W0] They're able to interpret grab those requests facilitated through Envoy in this case and apply some logic to them.
00:30:09 [W0] And an example of that logic was what? I was just speaking about a moment ago.
00:30:14 [W0] which is to say that the this image of example app uses webassembly filter runs inside of envoy and this particular filter is written in Rust
00:30:31 [W0] About a moment ago.
00:30:31 [W0] which is to say that the this image how big's a sample app uses webassembly filter runs inside of envoy and this particular filter is written in Rust
00:30:33 [W0] reasons it and I won't read describe what the sample app does because that's kind of for a different talk but suffice to say on I guess I should note that today console
00:30:47 [W0] With this configuration and the use of wasum filters out of the box and I won't speak on behalf of the product management team there. I would just I would say that I think you know your best get
00:31:03 [W0] I would say to pay attention and maybe I'm look out for that possibility.
00:31:11 [W0] So with that stage set we're going to talk about performance and there's a number of interesting things that pratik and I and the collection of those who are working in this community
00:31:23 [W0] Um uncovered and we're here to share a few of these things with you today to the extent that we can I think as we do it's probably important to recognize that, you know, when we're talking about performance and measuring performance.
00:31:38 [W0] Wow, look at the time fly.
00:31:42 [W0] I just realized that I've been talking that so we'll all move.
00:31:45 [W0] will pick up the pace when we're talking about performance.
00:31:48 [W0] It's not easy to hold all these variables in in place.
00:32:02 [W0] And so that's also why when you see a like a bunch of Benchmark of Statistics published its sometimes I will cringe I'll just speak for myself there because it's so easy to get a performance
00:32:12 [W0] Wrong to do one very well, although sometimes takes a PhD student.
00:32:19 [W0] So thank goodness for pratik.
00:32:21 [W0] I said that we were going to talk about webassembly very interesting technology that's been around for a little while and is now coming toward the servicemeshcon.
00:32:42 [W0] us here have been a couple of ways of looking at it sort of assessing the overhead of the the power of a mesh like hey the and you would expect probably that the more that you asked a servicemeshcon do the more
00:32:57 [W0] Set of those traffic that traffic that the more overhead that might be incurred that there's different ways of measuring overhead.
00:33:09 [W0] We're going to talk a lot about latency today latency being one of those critical pieces of information that you know, you want to provide your users a good experience with or consumers of your services with a good experience and what we've begun to do
00:33:21 [W0] Kind of three different ways of providing filtering traffic filtering in your data plane.
00:33:37 [W0] One is to is to well continue to use client libraries language specific client libraries, or just the standard libraries of the language itself to facilitate.
00:33:45 [W0] Network functions in this case, what we're talking about here is rate limiting specifically, this is just what we had studied here and the statistics that we're seeing is in the environment that we were within we went
00:34:01 [W0] Nations set the rate limit to be well, so yeah, we have run.
00:34:17 [W0] Yeah, let me take up the
00:34:21 [W0] Results in the way that we tested this in two different environments if I may like we implemented a rate limiting feature inside
00:34:37 [W0] Application itself and then we also implemented a rate limiting feature as in our rust webassembly and plugged it in inside the envoy sidecar proxy and remove the
00:34:53 [W0] Occasion what we would have expected is that if the rate limiting is inside the application itself, then it should perform better. But since it's webassembly and the
00:35:09 [W0] Of then it should perform better. But since it's webassembly and the network traffic that is coming through the sidecar proxy and being routed through the sidecar proxy to the application can be
00:35:18 [W0] To the sidecar proxy to the application can be processed in the site card itself and it does not need to go have the overhead of being transferred to the application again because of
00:35:28 [W0] The fair amount of benefits the experiment the rate limiting is set at $100 per second. So when we send a hundred
00:35:44 [W0] No errors. And when in the knative go client, we see latency of about three milliseconds.
00:35:58 [W0] Whereas if we do not have the rate limiting feature in the go but we put it inside the walls.
00:36:07 [W0] Mm. We see a lower latency of 2.1 milliseconds again to reiterate there are no errors in here, like none of the requests are being stopped and still we see
00:36:14 [W0] See a benefit in latency as in as we go higher to a higher requests like 500 requests per second.
00:36:28 [W0] We expect hundred of them to pass and 400 of them to fail and you can see that there is even more significant benefits when we see the request being filtered out
00:36:39 [W0] sidecar proxy itself
00:36:45 [W0] measure has this amazing feature where you can leave the RPS blank and it will automatically figure try to load the service as much as possible like try to measure how many requested
00:37:01 [W0] Try to load the service as much as possible. Like try to measure how many requested can actually send in a closed loop circuit and with the knative go pliant.
00:37:09 [W0] We saw that it can only service 4400 requests.
00:37:17 [W0] Well a hundred of them are go through and the rest 4100 are errors error messages, but since they go to the client library and then return the error.
00:37:24 [W0] Are they incur a larger overhead? And hence a lower rate requests per second on the other hand since we have a was on filter inside the sidecar container.
00:37:39 [W0] The requests were being filtered way more efficiently when we use this feature and we saw almost like a thousand more
00:37:51 [W0] I although that this is a very simplistic example and but when we see that even in a simple example, we see such significant difference in Layton sees so as and when the
00:38:09 [W0] Gets larger and more complex. These Layton sees will grow only more significant one.
00:38:25 [W0] You think like not new but like good thing about was mm is you can only use was mm filters as and when you need yeah, like we do with
00:38:33 [W0] The thing about was mm is you can only use was mm filters as and when you need like we do with kernel modules in our old age days. We can insert corn kernel modules and remove them
00:38:39 [W0] All these days we can insert corn kernel modules and remove them to lower our binary overhead.
00:38:45 [W0] We can do the same with was mmm, if we need a particular feature we can load it up and if we don't need it, we can remove it and that speaks to the power that was a provides us
00:38:55 [W0] R to the power that was a provides us with in servicemeshcon these microservices infrastructures, and so
00:39:06 [W0] And so we'll try to get the the slide Advanced.
00:39:10 [W0] I'll take a moment.
00:39:11 [W0] We were just realize we're back when and so to Karthik. Please interrupt me as I go here, but outside of just looking at the
00:39:25 [W0] In how you take a given Network function like rate limiting and how you implement that and the difference is in the overhead of those implementations.
00:39:39 [W0] You also can take different types of network functions like path based routing round-robin load balancing or context-based path routing and analyze compare them as
00:39:52 [W0] This and compare and contrast their speed and in again in the same environment that we were using for the other tests that we just talked about.
00:40:08 [W0] We were seeing it really just a single path based routing being having a higher performance while en route rambha drowned Robin load balancing taking
00:40:19 [W0] based routing being having a higher performance while en route Ramadan round-robin load balancing taking a slight more hit in terms of latency and in terms of throughput actually
00:40:25 [W0] in terms of latency and in terms of throughput actually for the given time period and then context based routing as you might expect is a little more powerful and it also is comparatively slower than the other two
00:40:36 [W0] and so we'll see if
00:40:38 [W0] by the time we take some Q&A will have some data points to share their specifically.
00:40:47 [W0] lastly
00:40:49 [W0] we will say that.
00:40:53 [W0] And pretty cat, you know again feel free to interrupt on this.
00:41:05 [W0] I'm I'm trying to hurry up but just yeah, so this was a great experiment that came by us while we were running other experiments is how do we optimize our average response time? So,
00:41:13 [W0] He tried to run one of the tests. And as you can see in this graph, we saw that delete the response that we got could be bucketed in separate chunks that is in bucket one.
00:41:29 [W0] Could be pocketed in separate chunks that is in bucket one.
00:41:31 [W0] We did get a few of these responses but majorette majority of the responses came back in bucket 3 that means our services able to process requests as low as I'm sorry for the small Figures.
00:41:44 [W0] Process requests as low as I'm sorry for the small Figures.
00:41:47 [W0] It's around like fourteen to eighteen twelve to eighteen milliseconds, but still majority of our requests come back around 60 50 to 60 milliseconds.
00:41:59 [W0] That means that there is potential in our service to respond in 12 milliseconds and we try to figure out how do we optimize this?
00:42:09 [W0] this and if we take the the smallest segment bucket that is bucket 1 and we try to see what is the maximum number of requests that were handled in the histogram
00:42:24 [W0] And if we take the smallest segment bucket that is bucket 1 and we try to see what is the maximum number of requests that were handled in the histogram.
00:42:25 [W0] At 14 is was around 45 to 50 requests and we divided by it by the number of threads that we were running that is two threads and when we did that lo and behold we got most of our
00:42:39 [W0] In the bucket 1 range Lee. Could you share that?
00:42:51 [W0] Yeah, so we reran the tests with a lawyer requests per second around 25 to 30 and we saw that most of the responses received in bucket
00:43:02 [W0] That is Dunn's of potential that we should Gather By looking at these performance metrics and there is tons of optimizations that we can do in this scenario as well,
00:43:19 [W0] Tons of optimizations that we can do in this scenario as well.
00:43:20 [W0] You know and to share just a couple of those best practices given give them the time that we have left here.
00:43:30 [W0] It's to probably go take advantage.
00:43:34 [W0] There's actually a long list of servicemeshcon.
00:43:49 [W0] bought as many people have questions about performance as they do that measure e as an open source tool has been created to empower people to easily reproduce these tests to persist your results to Baseline your environments to use different load generators because
00:44:04 [W0] Colin Alice's differently to compare your configuration changes to test your workloads on and off the mesh to do it against different meshes, like sort of on and on with that
00:44:20 [W0] I commend you give give that to a shot but also recommend that you jump into this community here where pratik and I both hang out what a bit and you can have hanging out.
00:44:33 [W0] We're going to be hanging out taking QA right now. So please please bring your questions.
00:44:39 [W0] Thanks.
00:44:41 [W0] Boo.
00:44:51 [W0] Hey guys, we're here to answer any questions.
00:45:00 [W0] Sorry about the screen share for some reason.
00:45:03 [W0] It was off. Hope you all followed the link shed by lie on his page.
00:45:08 [W0] Boy Kudo, 'z to those who stuck it out throughout this talk.
00:45:25 [W0] Yeah, the TM video being out of sync has been well, it's been impressive to see those stick around for the end.
00:45:28 [W0] So I know we've seen a couple of good questions coming through.
00:45:31 [W0] So we're trying to answer a few of those.
00:45:33 [W0] It'll be a little bit easier.
00:45:34 [W0] Actually I think to verbally speak to those so that's nice.
00:45:37 [W0] One of them was just about rust as you know, in terms of its performance and and using rust based.
00:45:52 [W0] Well Russ paste filters inside of awesome bm2 to characterize part of that answer.
00:45:58 [W0] Is that Envoy itself is written in C++.
00:46:09 [W0] There are certainly performance comparisons that you can do between popular languages in this space go.
00:46:11 [W0] Let's plus rust, you know, if there were folks from the linkerd e project here, they would espouse the the performance and other characteristics
00:46:26 [W0] Rust and I think that by and large is probably is the case that rust would be have some both performance characteristics and some others that that if you're doing intense intense
00:46:41 [W0] Our traffic within within a wasp environment that you're seeking out rust or even outside of wasp environments linkerd EU surface. Proxy is written in rest if you don't use wasum now and
00:46:56 [W0] If you are using Envoy as your service proxy Envoy is written in C++.
00:47:06 [W0] And so writing a knative Envoy filter in C++, you know, well
00:47:11 [W0] He's going to be a bit quicker. And and the reason I pause for a moment.
00:47:21 [W0] It's a that pratik as well in the time that we've recorded. This talk published additional metrics are some additional.
00:47:27 [W0] Analysis, maybe I'll try to share the link. I think it's fresh as if
00:47:36 [W0] yeah links are we expect drugs to be quite performance because the language was developed to be performant. But definitely it's a great question that we can do
00:47:55 [W0] Research to see which among as a war zone filter itself if rust or C++, which of them is performed but like I would expect trust or C C++ to be one of the best
00:48:10 [W0] in terms of performance
00:48:12 [W0] So there's a question does the mesh really help to manage and configure was on filters. If not extension able to do it.
00:48:35 [W0] It's a very interesting question.
00:48:37 [W0] was some filters in servicemeshcon ill very much an active Development Little scope for a lot of servicemeshcon.
00:48:52 [W0] These servicemeshcon Das and when the servicemeshcon adds supporting was in filters, we will try to integrate the best we can and if there are apis will try to write an adapter so that you can configure
00:49:07 [W0] in the servicemeshcon generic fashion
00:49:12 [W0] for their critiques answer. I'll go ahead and publish a
00:49:21 [W0] Link where those that are curious about use of measure e and measure easily injection or management of the loading and sort of dynamically reloading of
00:49:37 [W0] Some filters in Envoy based servicemeshcon is the link that was just shared is to sample app called image Hub.
00:49:51 [W0] And so that sample app is orchestrated by measuring it deploys actually into a console-based servicemeshcon.
00:50:07 [W0] The use of wasum filters but because it is using Envoy has it sidecar technology it inherently does and so near as I understand.
00:50:21 [W0] that's a fairly near term roadmap item for the console team. But that image of that sample app is an interesting one to play with in part because one part because I think it
00:50:32 [W0] Help expand people's Horizons in part to think Deepak and had a question earlier about filtering Ingress traffic and manipulating and doing some things with it.
00:50:43 [W0] It's an image of that sample app is an interesting take on it actually enforces multi-tenancy and a bit of a subscription based pricing. That intelligence
00:50:59 [W0] Is not built into the sample app itself, but that intelligence comes from the wasps and filter that you can see in that sample app.
00:51:11 [W0] So pretty cool to explore the power of the data plane that way.
00:51:14 [W0] Which I'm you know, so just I don't know if there's I guess there is another question.
00:51:27 [W0] And so the question is in multi-os clusters.
00:51:38 [W0] You have a recommendation for Windows notes.
00:51:39 [W0] I don't know.
00:51:46 [W0] I don't know. I don't know how to answer that because like I always use Linux.
00:51:54 [W0] Yeah, there is a particular particular attending with us Docker Captain. You know who's in attendance
00:52:05 [W0] Much more of a WSL to Windows expert and so I had no doubt.
00:52:13 [W0] He'll be in the same slack Channel with us as we conclude.
00:52:19 [W0] I'll will point you to The Experts there.
00:52:21 [W0] We are not able to hear you.
00:52:52 [W0] Oh, okay.
00:52:59 [W0] Yeah, it's not a volume thing, huh?
00:53:00 [W0] Legends
00:53:08 [W0] Yeah, so feel free to connect with us on the slack and any additional questions.
00:53:25 [W0] will try to get the recording fixed before we make it available for you for with the viewing General viewing.
00:53:33 [W0] Cool, very good.
00:53:43 [W0] I don't know if you guys can hear me now or not, but I'm going to say something anyway, which is yeah, I would.
00:53:47 [W0] okay good. Well since this talk couldn't have been any more of a disaster. Friendliness.
00:53:53 [W0] Yeah.
00:53:58 [W0] I don't think things and the microphone going to Ed and I'll just I'll say that I always say thanks pratik actually for all the work that you've done this very interesting analysis that you've done.
00:54:08 [W0] So please do
00:54:08 [W0] Um that you were able to use Mastery as much as you did. I think you about war the rubber off the tires on it, but I'm really excited about empowering people with the knowledge of how much it each of these functions costs.
00:54:24 [W0] And it's thanks to your work that that they'll be able to know you're going to know what it costs to.
00:54:32 [W0] Weight limit of something.
00:54:36 [W0] Thanks leave for the opportunity.
00:54:38 [W0] It was great.
00:54:39 [W0] Very good. Well, I think we're out of here.
00:54:45 [W0] We'll see you guys in slack.
00:54:46 [W0] Yeah.
00:54:47 [W0] See you guys.
