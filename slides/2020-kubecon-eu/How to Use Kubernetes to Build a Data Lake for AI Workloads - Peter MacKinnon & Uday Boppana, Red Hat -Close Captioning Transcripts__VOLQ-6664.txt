How to Use Kubernetes to Build a Data Lake for AI Workloads: VOLQ-6664 - events@cncf.io - Wednesday, August 19, 2020 8:21 AM - 115 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:08:44 [W] Hello, everyone. Welcome to the session about are using kubernative to build a data leak for our Ai and machine learning workloads on also how we used it for the entire day.
00:09:00 [W] I and mln constructed stackrox.
00:09:15 [W] Center of Excellence as a principal software engineer working on AI and machine learning Technologies and we have collaborated on this project for close to a couple of years.
00:09:30 [W] So this top is this about our journey that we went through and experiences. We had while we were trying to build an AI and machine learning platform at red hat and also about the challenges we faced on how we solve them.
00:09:40 [W] We also open source the platform now it's available for everyone to download and
00:09:45 [W] get it up and running and try it out and Pete will talk to you about those details to so let's get started.
00:09:52 [W] As you know Ai and machine learning or growing workloads within the Enterprise today.
00:10:05 [W] And as we started more of this workloads pop up with and Enterprises and at Red Hat.
00:10:07 [W] With me is Peter McCannon and he works and the red hat a center of excellence as a principal software engineer working on AI and machine learning Technologies. And we have collaborated on this project for close to a couple of years.
00:10:14 [W] So this top is this about our journey that we went through and the experiences we had while we were trying to build an AI and machine learning platform at red hat and also about the challenges we faced on how we solve them we also
00:10:16 [W] Really usable data all of the data in the company was siloed in different locations by departments or by the owners of the data sets and authentication and authorization policies were different and preventing access this
00:10:28 [W] It's also created a challenge for teams that wanted to collaborate and it prevented teams from collaborating and working together.
00:10:37 [W] We also saw that data scientists especially were not happy with the long turn around times. They had using the traditional it ticketing process has to get infrastructure up and running. They wanted something that's real time quick Andrade York on the cloud.
00:10:53 [W] The the final challenge was also lack of at the right tools and infrastructure for the data engineers and data scientists data Engineers needed something that was easy to cover the data and transform the data as they needed and data scientists wanted cloud like experience
00:11:09 [W] And their execution so once we listed a listen to them and figured out the kind of common challenges and high priority issues.
00:11:25 [W] We are facing we came up with a set of design points around what we needed to do in our in our next Generation architecture.
00:11:32 [W] So we set up three goals first is we needed a centralized shared infrastructure that all the teams could use this centralized shared infrastructure of broke down the silos for data and also enable collaboration.
00:11:40 [W] Across the teams because they were all working off of the common platforms that they had.
00:11:47 [W] We also wanted something that was more self-service driven.
00:12:00 [W] So the customers can get a more cloud like experience and also have a real-time a workflow like they're used to we also wanted to build an infrastructure that was flexible in the sense that it could allow Vigneron.
00:12:04 [W] That forms that they had we also wanted something that was more self-service driven.
00:12:06 [W] So the customers can get a more cloud like experience and also have a real-time a workflow like they're used to we also wanted to build an infrastructure that was flexible in the sense that it could allow bring
00:12:06 [W] It's kind of a work flow but also provide with a set of standard libraries that can be used on standard tooling that can be used.
00:12:17 [W] So data scientists can get off the ground and get their work done faster.
00:12:22 [W] So once we set those goals for the challenges, we identified. We also looked at that end-to-end Ai and machine learning workflow within our customers and internally within our company.
00:12:32 [W] So we wanted to figure out the different personas that were emerging in the alien machine.
00:12:37 [W] Earning landscape and we figured out its anywhere from the business side of the world, which is business leadership that has financial goals to your traditional. It operations that has a broad swath of responsibilities across the entire day and
00:12:50 [W] And your app developers who create the end-user applications that are being used that use the machine learning workflows in the hair models.
00:13:01 [W] We also saw the emergence and and prominence of new rules like data Engineers that work with a lot of the data sets coming in the mln.
00:13:13 [W] that are really worried about and look at the tools to call open the models for machine learning and also data scientists who are the real end users of AI and machine learning platforms to create.
00:13:23 [W] Models and predictive predictions. So we took the personas that we develop and put it against the fabric of at the AI and machine learning workflow to figure out where the different touch points were and where each role interacts with the system
00:13:35 [W] We needed to do to optimize the system for each of those roads.
00:13:40 [W] So with that in mind we set about trying to solve the problem. Now that we have a set of challenges are the personas and user requirements and our goals. We set out to solve the problem and find the right Technologies pretty early on in the game
00:13:56 [W] Our containerd Rising on kubernative was the right solution for us going forward and would help us to build the next generation of AI and machine learning platform kubernative gave us pretty much most of the design points.
00:14:12 [W] We wanted that we set us both it gave us the authority to quickly respond to end users and and also provide it in a self-service way.
00:14:21 [W] So customers can compute manage Computer Resources as they needed. It also gave us the portability of a
00:14:27 [W] and runtime that can be used to move containers between or between company at the agents on the poor and across different departments or across the company.
00:14:42 [W] It also gave us more importantly the flexibility we needed in today's carrion mln want moments to be agile and flexible. Like for example, we wanted to run multiple versions of the same application or the same
00:14:52 [W] From departments or across the company.
00:14:53 [W] It also gave us more importantly the flexibility we needed in today's carrion mln want moments to be agile and flexible.
00:14:53 [W] Like for example, we wanted to run multiple versions of the same application or the same model in parallel to figure out the different outcomes and what the impact would be and open it is and containers gave us an easy way to do it
00:15:00 [W] Elotl to figure out the different outcomes and what the impact would be and open it isn't containers gave us an easy way to do it.
00:15:04 [W] it. And of course kubernative is highly scalable and it's known for scalability. So that was that was a really good thing to have to
00:15:08 [W] So for this part of my presentation, I will for I will focus on how we build the data Lake and the data management capabilities for AML and frustrated that we have and and then hand over to P to talk about the higher layers of the machine learning
00:15:24 [W] Capabilities that we build on top of this plate early from a technology perspective. Like I said, which was kubernative and for the storage layer, we also chose to build it on SEF and rook and we'll talk more about it.
00:15:37 [W] So when we started to set a design the data Lake on the data architecture for our Ai and machine learning infrastructure and it red hat we set out to understand the workflows and a couple of workflow stood out as the must support.
00:15:56 [W] So we we learned early on that.
00:15:58 [W] There is a requirement for certain kubernative clusters to have storage that was a localized within the kubernative cluster. So this would either be for reasons of security and compliance.
00:16:09 [W] are certain data sets need to be siloed off or for data that was temporal or not of long term use or also for a things like persistent storage for time based operations and such and for such refer these clusters
00:16:24 [W] Rich has to run with the kubernative cluster running the storage instead kubernative was at the best way for us to go forward and that gave us the ease of use and simplicity for the cluster.
00:16:36 [W] admin to manage both the compute and storage as one entity.
00:16:48 [W] But like I said in the initial part, we also had a goal to build a centralized data Lake that could be used across the organization at data like that could be shared across multiple kubernative clusters, and that would be the single source of
00:16:55 [W] Truth for all data sets being used in the organization.
00:17:03 [W] So what we really wanted in the long term is what you see sort of in the rightmost rectangle in this picture.
00:17:09 [W] We realize that both apps and users will need a combination of some local storage within their communities cluster that's running that's easy to manage and easy to access but also the need for a long-term centralized
00:17:20 [W] All the data can be funneled into and stored for things like machine learning and Analysis.
00:17:26 [W] So with that in mind we set out to choose the right technology. Like we said we chose f for a building the data Lake SEF, as you know is an open-source storage project that provides a uniform that provides multiple protocol support for
00:17:45 [W] It also is software defined and is highly scalable Sosa fit our needs from that aspect.
00:17:55 [W] We also chose to use the Rook operator, which is the operator that will enable us to spin up and manage access cluster within an existing qualities cluster for those locations where they wanted a converging life offering
00:18:07 [W] What running in the same prodyna discuss turn Sophia's been around for a couple of decades and is a highly known in the industry and is a very vibrant open source project. So for a for now, I will focus a little bit more on group and what it does because it's relatively new.
00:18:23 [W] Rook is a kubernative operator that can enable customers to spin up entire self textures within their existing kubernative cluster.
00:18:39 [W] So Rook will take the right demons required Force F. Like morons mgrs.
00:18:44 [W] OSG is and managers and spin them spindles containers are inside a kubernative cluster and create a full fledge ceph cluster within an existing communities cluster, which means you have a full functional storageos.
00:18:53 [W] Cluster to use right away for all your applications running in that review plus term Rook. Also more excitingly does all the required configuration and back and operations required to support the knative QV of accessing
00:19:08 [W] Create a full fledge Seth cluster within an existing kubernative cluster, which means you have a full functional storage cluster to use right away for all your applications running in that review cluster Rook. Also more excitingly does
00:19:10 [W] Rwex volume claims are wo volume claims and the newly Emerging Market planes.
00:19:24 [W] So this combination of the ability to spin up an entire cluster and automatically configure it first the kubernative storage, but flows made a rook the obvious choice for us to manage staff clusters Within openfaas.
00:19:28 [W] And of course, like I said for we also needed a centralized data Lake that will store all the long-term data assets of a company and for that we chose to build it on SEF also running on Linux.
00:19:50 [W] And in this case, the majority of the data was stored in object storage, as you know, object storage is emerging and standard that is becoming pervasive for storing large data sets, especially and structured data and staff has a rich set of object storage
00:20:00 [W] And of course, like I said for we also needed a centralized data Lake that will store all the long-term data assets of a company and for that we chose to build it on SEF also running on Linux. And in this case, the majority of the data was stored in
00:20:03 [W] give us a really good flexibility again to recap we use Rook to use to instantiate subclusters inside poverty is for localized storage of relations and clusters and we use self object storage as the centralized data
00:20:16 [W] dog position
00:20:18 [W] We also did something really exciting to he's a data engineers at task of when we built this infrastructure and data Lake. We provided a way to automatically process incoming data into the cluster and place it in the right location within the day today.
00:20:34 [W] Using a combination of stuff bucket notifications cafta and K Naidu serverless. So in this architecture multiple data streams would bring in data from external sources into a self bucket sephardim trigger a bucket notification
00:20:50 [W] The presence of new data are the changes in existing data sets and these bucket notifications would be received by calf which would process them and send them over to k8s serverless k- serverless would then spin up the right serverless function to
00:21:06 [W] Most of the things like removing pii information gagging any required metadata tags adding the right security policies and then move this new data set into the right location in the data layer.
00:21:22 [W] So any data scientists and Engineers can have access to it in real time.
00:21:27 [W] So this operation freed up the data engineer from having to manually process all the incoming data on almost created a real time data in just pipeline that automatically processes the data sanitizes. It does the required metadata operations and put
00:21:39 [W] she's in the right location.
00:21:51 [W] And for example, if you're in the medical field, we prototype this with an x-ray engine where this multiple x-rays or CAT scan vertical images coming into a cephalopod packet. And as soon as the images are stored on the bucket and notification
00:21:53 [W] took half and then 2 k- + K knative would spin up the right function that looks at these images and optimizes them removes all the pi information and then move them to the right bucket that can be used for machine learning Engineers that work on this and 1 minus beta
00:22:09 [W] Finops F and rook and there's our data pipelining just solution that we created gave us a pretty good benefit for creating the data layer and the data infrastructure and also a good basic framework that we can build more workloads on.
00:22:25 [W] At this point I'll transfer him to Pete to go over some of the eye and machine learning workflows and towards thanks.
00:22:33 [W] Oh to you Pete.
00:22:34 [W] Thanks. You read the open data Hub project started internally within Red Hat as the data store for our own data engineers and data scientists.
00:22:50 [W] Hence, the name data Hub early on we realized that the data scientists and the data Engineers requirements for tools and AI am L. Components are different than devops requirements.
00:23:03 [W] There are mostly UI driven they avoid using terminal commands and expect the tools to include all their favorite.
00:23:08 [W] a IML libraries that they are accustomed to using
00:23:11 [W] collaboration and sharing is also important requirement for their workflows to successfully deliver models to production.
00:23:25 [W] The main pain points are shearing machine learning work initially done in notebooks moving the model to production from those notebooks and of course managing the model while in production this includes monitoring it
00:23:35 [W] Shins are accurate watching for data drift managing resource usage for CPU GPU memory and lots more.
00:23:45 [W] Open data Hub has an upstream and downstream relationship with many open source projects open data Hub Drive some functionality from the Upstream kubeflow project, which we'll talk about more in a bit.
00:24:03 [W] However, open data Hub also down streams from other open source projects such as Selden Kafka that we talked about earlier spark as well or fauna and Prometheus.
00:24:16 [W] All this to provide a comprehensive end-to-end AI mlperf form that runs on openshift in many cases enhancements and changes made in the open data Hub project are also offered back Upstream to the original open
00:24:32 [W] Such as a variety of changes that we sent Upstream to kubeflow for proper openshift platform support.
00:24:41 [W] Talking about the personas that we day laid out for us in the context of open data hub for the data analyst Persona open data Hub provides integration for data Lakes such as an S3 interface to
00:25:01 [W] Talking about the personas that we day laid out for us in the context of open data hub for the data analyst Persona open data Hub provides integration for data Lakes such as an S3 interface to self
00:25:03 [W] there's SQL databases such as postgresql and MySQL and data streaming using Kafka strimzi for data exploration open data Hub includes the superset and Hugh projects superset
00:25:17 [W] Of as an open source version of Tableau for data processing open data Hub provides spark in the spark SQL Thrift server.
00:25:31 [W] Finally, there are meta data tools such as the hive meta store the data scientists accesses the data through the various storage interfaces who they just described
00:25:42 [W] Through the various storage interfaces.
00:25:43 [W] They just described Jupiter Hub is also provided for Notebook development environment integrated with openshift authentication and can also make use of available GP resources
00:25:54 [W] labeled on the kubernative nodes
00:25:58 [W] multiple tools for model training and verification are provided such as for the Frameworks tensorflow and by torch and of course Spark
00:26:09 [W] Open data Hub. Also provides pre-trained models as part of its a ilibrary component. So pre-trained models for things like fraud detection and various other types of machine learning algorithms
00:26:24 [W] In machine learning pipelines data scientists can use Argo and we're going to talk about kubeflow bit more but it can also make use of kubeflow pipelines for the devops engineer's their provide with monitoring tools such as Prometheus and
00:26:40 [W] For the observation of all the components in the AI am L and N platform and also there's model serving tools such as tensorflow serving and Selden that are provided
00:26:56 [W] The open data Hub project is a meta project to integrate all these open source tools and provide an end-to-end AI mlperf Foreman openshift. So the meta project integrates these open source projects into one project that makes it easier
00:27:17 [W] That makes it easier to be consumed by users.
00:27:20 [W] Going back to sort of where all these projects come into play in the overall mlperf flow.
00:27:29 [W] It starts with data prepping and ET Elling that data into a data lake or storage of some kind and making it accessible to the data scientist.
00:27:43 [W] The next phase is the model development which includes feature selection model creation training and validation. Finally. The last phase is moving that model out and serving the model in production.
00:27:51 [W] And this is not sort of a One-Stop model serving thing, but it's goes through a constant Loop of optimization.
00:28:04 [W] So models in production are actually monitored to see how the model is performing against live data. So that cycle monitoring optimizing serving that's constant and happens for
00:28:17 [W] And that encompasses a collaboration between devops data scientist data engineers and business developers.
00:28:28 [W] So open data Hub sort of provides a unifying.
00:28:32 [W] Umbrella, if you will for the activities of all those different personas.
00:28:39 [W] As part of the data the open data Hub project.
00:28:48 [W] We see a great deal of value in the kubeflow project.
00:28:54 [W] So we dedicate our efforts to enable kubeflow on openshift and integrate the open data Hub with kubeflow in an installation based on the operator lifecycle manager, which is a key component in
00:29:04 [W] On the operator lifecycle manager, which is a key component in openshift for it is not it is now integrated to open data Hub and runs on openshift kubeflow brings
00:29:14 [W] Mel capabilities and features so for distributed model training, we have tensorflow jobs and by towards jobs MPI jobs for models serving again, there's Selden but there's also a new project called KF serving which
00:29:30 [W] Action layer over the various different types of serving Frameworks for pipelines.
00:29:39 [W] We have kubeflow pipelines based on Argo and as part of the goal of all this we Upstream any enhancements back to the kubeflow project.
00:29:48 [W] So we also work with the kubeflow community to add openshift as one of the supported platforms for kubeflow is we show the red arrow in the website menu on the right you see a list of cloud platforms
00:30:00 [W] Ben shift as one of the supported platforms for kubeflow is we show the red arrow in the website menu on the right? You see a list of cloud platforms and openshift is one of those that is
00:30:04 [W] Openshift is one of those that is provided as supported for the latest kubeflow versions.
00:30:10 [W] So we've released the open data Hub 0.7 beta operator with kubeflow integrated into that.
00:30:20 [W] Open data Hub is an operator currently available in the open shift operator Hub community.
00:30:32 [W] It's one of the community operators.
00:30:34 [W] You will find it in the embedded open shift operator Hub that ships with openshift for we recently released up and data Hub version 052 that was built using the
00:30:47 [W] includes many of the a IML tools we talked about previously but we also support a dually a new version of the operator that is based on the kubeflow golang KF cuddle operator
00:31:03 [W] Support a dually a new version of the operator that is based on the kubeflow golang KF cuddle operator and that gives us this mechanism for better integration with Google flow.
00:31:10 [W] So this duel mode these two operators.
00:31:15 [W] 052 is to support so-called Legacy deployments.
00:31:20 [W] That going forward will only have bug fixes. However with the open data Hub 0.7 operator buildpacks.
00:31:25 [W] Based on Kuma kubeflow, it integrates with kubeflow includes kubeflow components as well. As some of the Legacy open data Hub tools that have been migrated to the KF cuddle custom resource definition.
00:31:40 [W] So that version will be the target for all new feature development.
00:31:44 [W] Open data Hub is an open source community and we welcome contributions and engagement.
00:32:01 [W] Our main site is located at open data Hub dot IO and our code base can be found in gitlab and GitHub.
00:32:04 [W] There's two areas for that gitlab sort of hosts. Some of the the Legacy features of open data Hub.
00:32:14 [W] where the Manifest development for features in-toto.
00:32:16 [W] Open data Hub 0.7 or host it and get Hub.
00:32:27 [W] Tutorials we do as well as mailing lists as listed here on this slide.
00:32:40 [W] Thank you for joining our talk today, and now we'll be happy to take any questions.
00:32:48 [W] So you did do you want to take that question number two there?
00:33:08 [W] Yep it yeah, so so the question question number two, is there redundancy between the data stored on the data Lake and the local storage if yes how we synchronization performed.
00:33:23 [W] Yes. There is a way to synchronize data between your local storage and the network data Lake to using active directory replication which supports so you could set it up for data to go both the ways or to just go one way and stay in the data
00:33:35 [W] Verb, it's primarily driven by object and we are making more enhancement since f with the community for block and also for file, but that capability does exist today.
00:33:47 [W] And I will take number three in the question is could you provide links to the product or download page?
00:33:58 [W] It's important to emphasize the open data Hub project and I'm assuming we're talking about open data up as opposed to the ceph project the information for that product can be found or
00:34:12 [W] Can be found at open data Hub dot IO, so we have documentation their blog posts and we host in gitlab the the source for the various components
00:34:27 [W] There are pieces that are hosted on GitHub for for legacy reasons also with openshift for it has an embedded operator lifecycle manager.
00:34:43 [W] And in fact, the easiest way to install open data Hub is just to make use of that. So you go to the openshift for console and you would bring up the catalog the first item.
00:34:56 [W] There will be Ai and machine learning and you can just type in
00:34:57 [W] Open data Hub and they will come up as one of the community operators and you can click on that tile and it will sort of work you through basically the steps for installing that into the cluster directly.
00:35:12 [W] Hopefully that answers the question.
00:35:16 [W] - take question number four repeat the question is in the automated ingestion pipeline.
00:35:23 [W] Does the chain knative serverless function read directly from self or does the message consumed from calf contained within itself the data payload as well?
00:35:34 [W] So both the modes are supported in fact, so today chefs enter notification to Kafka and Kafka initiates the canoe to interaction but that is all the metadata flow is the real data still unsafe and so
00:35:45 [W] Starts the notification process in bucket notifications where there is also a way that we prototyped Upstream where Seth can directly send a notification to KN knative and Trigger the helped trigger the right function that is being tested right now.
00:36:00 [W] And we will product phase it in the next coming days.
00:36:09 [W] So depending on your scalability need and product needs operator user message broker like halfway in the middle or you could directly hookups F buckets to kill knative.
00:36:13 [W] Number five I guess we day we can both sort of have a stab at it says the question is do you back up your data?
00:36:27 [W] If yes, how do you back up your data? And probably you day that speaks to this F architecture. I would I would say should not be so the backing up data part today.
00:36:39 [W] It's through the replication again, step supports knative replication for all the protocols so I can't be ready for block and
00:36:46 [W] for object and since most of the data we have is an object the way we back up is using the multi-site feature where we create a second side and keep a copy of all the data in and and Pete can talk about some of the Next Generation
00:37:01 [W] Data part at today.
00:37:02 [W] It's through replication again, except supports knative replication for all the protocols.
00:37:03 [W] So I can't be ready for block and primarily for object. And since most of the data we have is an object the way we back up is using the multi-site feature where we create a second side and keep a copy of all the data in and and Pete can talk about some of
00:37:04 [W] Taking on more around time-based backup and restoration and auto restoration stuff and we are making some progress there. It's a work in progress for more real time back up on stuff.
00:37:14 [W] Okay, and number 63 pictures all yours.
00:37:22 [W] Yeah, thank you.
00:37:33 [W] The question is with respect to the open data Hub solution architecture is the hive metadata catalog used as the primary data catalog for example for governance purposes or does it have more of an internal use case it is
00:37:39 [W] Um, there are other solutions that were looking to incorporate in open data Hub with respect to governance and there's a value evaluation going on of some other
00:37:55 [W] Components that could fulfill server browser governance solution there.
00:38:06 [W] So things like Atlas Ranger egeria.
00:38:10 [W] These are all components that were actively looking at for providing sort of a broader data governance and Self Service type of capability for
00:38:21 [W] that we're actively looking at for providing sort of a broader data governance and Self Service type of capability for open data Hub with respect to
00:38:23 [W] spec to data cataloging
00:38:25 [W] Do you want me to read that X1?
00:38:33 [W] The next question is?
00:38:34 [W] Looking to use this for a research product on large archive feeding data into such design as SEF chosen for weather data. So the use case is large volumes of weather data
00:38:51 [W] Is pulling our access into our archive using this design given that it's 40 petabyte-scale?
00:39:22 [W] Eject, it is not currently affiliated with cncf in terms of under any of the umbrella projects for that.
00:39:33 [W] We'd be probably interested in the community would be interesting in hearing more about this use case the community meets bi-weekly on Mondays and on our open data Hub iof site.
00:39:49 [W] In schedule, I believe for the community meeting. So we'd welcome this this use case and other use cases like it to to have for further discussion.
00:40:01 [W] Number 8.
00:40:05 [W] Yeah. Nobody is a few questions.
00:40:12 [W] on your feet.
00:40:15 [W] Yeah.
00:40:19 [W] So one form of the question is that openshift only the other form is does this run on stock kubernative openshift is kubernative.
00:40:29 [W] is kubernative. However, it does have extensions and some of those extensions are used knative.
00:40:33 [W] Lovely and open data Hub.
00:40:38 [W] So it is targeted for openshift. A lot of the constructs will be obviously familiar in terms of deployments and pods and services and things like that.
00:40:51 [W] But we also make use of resource objects and openshift called routes which are actually a higher level of abstraction of Ingress resources.
00:41:02 [W] So there's various sort of areas where
00:41:04 [W] where we are specifically providing a tool a suite of tools basically specifically designed for open openshift.
00:41:14 [W] Used natively and open data Hub.
00:41:19 [W] So it is targeted for openshift. A lot of the constructs will be obviously familiar in terms of deployments and pods and services and things like that.
00:41:20 [W] But we also make use of resource objects and openshift called routes which are actually a higher level of abstraction of Ingress resources. So there's various
00:41:22 [W] sort of areas where we are specifically providing a tool a suite of tools basically specifically designed for open openshift.
00:41:24 [W] So question number nine is more of a comment.
00:41:26 [W] So yes piloting now on using many of the same tools.
00:41:27 [W] We would love to have more feedback and even quite a have you collaborate with the community like Pete was referring we do have regular meetings for the community that's are on the website Edge.
00:41:33 [W] So, please feel free to join and share feedback and we can learn from you and you can get more information from the community to and question number 10 is the same the session the slides. Yes, we will follow the regular cncf path of making everything available.
00:41:45 [W] As soon as that cncf makes all of the sessions available the session and the slides.
00:41:51 [W] I think question 11 feet just answered what versions of this run on and 12.
00:41:58 [W] We just answered.
00:41:59 [W] 13 sopt 13 is again similar to what we're talking about can this run or knative kubernative and you talked about the extensions on stuff? So yes.
00:42:09 [W] Here's one.
00:42:16 [W] Sorry, there's multiple pages to the kubenetes Carson does open data Hub support GP resource sharing scheduling.
00:42:28 [W] It does provide GPU support in terms of automatically building GPU enabled notebooks that can be launched from Jupiter Hub, which is one of the components and in open data,
00:42:37 [W] Looking for the community that's are on the website.
00:42:38 [W] So, please feel free to join and share feedback and we can learn from you and you can get more information from the community to and question number 10 is the same the session the slides just we will follow the regular cncf path of making everything available as
00:42:40 [W] The actual details of GPU enablement. I've done talks about this in other venues like Deb nation and stuff like that.
00:42:55 [W] It's enabled as an operator in openshift and kubernative Nvidia actually provides the the GPU operator for that. So that's how the enablement is done.
00:43:11 [W] Shin we're just closing up here running out of time.
00:43:18 [W] We need disconnected install for odh, it is not supported yet.
00:43:19 [W] Yes.
00:43:20 [W] We're aware of that.
00:43:24 [W] It's it's an issue that we're still looking into and also we have data sets and NFS large seismic data sets.
00:43:33 [W] How do you picture we can consume these data?
00:43:36 [W] Maybe that's the last question for you to shoot?
00:43:37 [W] Yeah, so you can consume them using the Dell knative p v+
00:43:42 [W] Volume construct that kubernative provides an open ship provide all you can also do out-of-band access but we recommend that the persistent volume a way of consuming the data sets are already in NFS because that follows with the data on control
00:43:57 [W] This way to read and write persistent data.
00:44:01 [W] And I believe that's all we have time for and we do appreciate all the the attendees joining our talk today.
00:44:15 [W] Yep, thanks folks. It's been a good interactive session. So thank for the questions and looking forward to interacting with most of you on one of the community meeting this.
