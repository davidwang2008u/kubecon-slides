Predictable Performance Through Prometheus and Topology Aware Scheduling: QPWP-7899 - events@cncf.io - Thursday, August 20, 2020 8:25 AM - 372 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:35 [W] Hi, I'm Kelly Muldoon.
00:05:00 [W] Mainly work on implementing infrastructure needed to bring kind of high-performance networking applications networking workloads to kubernative.
00:05:10 [W] Hi.
00:05:12 [W] I'm Tom go away from you a Packard Enterprise and I work in Advanced R&D organization where I'm focused on ways to provide infrastructure that supports disaggregated application architectures today Killian and I will be discussing the need for a greater awareness of
00:05:26 [W] Disaggregated application architectures today Killian and I will be discussing the need for a greater awareness of infrastructure capabilities in the scheduling process to look to achieve predictable service level guarantees for applications.
00:05:34 [W] Digital disruption and digital transformation have been viral topics for quite some time.
00:05:42 [W] How many times have you heard? A CEO say we don't think of ourselves as a bank or whatever we think of cells as a technology company, but those same companies are evolving their business models and processes much in the same way as they always have done.
00:05:54 [W] Cells of it as a bank or whatever.
00:05:55 [W] we think of cells as a technology company, but those same companies are evolving their business models and processes much in the same way as they always have done but we do see a shift in that we're seeing Innovation at the business level
00:06:00 [W] but we do see a shift in that we're seeing Innovation at the business level becoming more digitally aware and they thought progression is assuming that Computing with definable service levels will be both ubiquitous and pervasive
00:06:11 [W] Price is the innovate on the business side.
00:06:18 [W] We see a shift towards Cloud native and micro-service architectures.
00:06:21 [W] is going to be changing the way it resources consumed and the expectations of service levels for applications.
00:06:25 [W] The shift in application architecture Lee is leading us to look that building the underlying infrastructure to support what I call infrastructure plasticity.
00:06:39 [W] Now that means delivering the infrastructure model that's presenting itself to Applications with the capability of undergoing organic changes on the Fly based on the applications class of service.
00:06:50 [W] So think about anything from sift system level changes to large-scale network topological remapping.
00:06:55 [W] Data is key and the where and how data is consumed is changing.
00:07:03 [W] So let's take a look at that system of record traditionally is where you are recording what has happened applications are typically architected either monolithic or using some simple client server paradigms if you
00:07:17 [W] We typically see let's say a typical retail database with all historic transactions is roughly around 40 petabyte-scale.
00:07:47 [W] New value more relevant to their customers application architectures more complex, which some levels of distribution of compute though data tends to gravitate to centralized databases and large data Lakes.
00:08:03 [W] Look at data volumes typical per day posting of social media is about for petabyte-scale.
00:08:33 [W] A Car application architecture is a highly distributed recompute moves to the point where data is ingested to support greater real-time execution compute needs to be both pervasive and contextually aware
00:08:48 [W] The driven by evolving business requirements.
00:08:54 [W] So does the need to increase the performance and support higher application classes service with tighter control over the variation of service according to IDC.
00:09:07 [W] The volume of data created is doubling every two years that growth challenges.
00:09:12 [W] The conventional is ways that we currently build and deliver Solutions.
00:09:14 [W] The variety of data is changing as well with a heterogeneous mix of data types and both structured and unstructured formats.
00:09:24 [W] A big reason for this growth is due to the increased number of sources where data is created and ingested into the application ecosystem. Think about the proliferation of iot with its unique data models and
00:09:40 [W] little significance
00:09:42 [W] usage of the data changes as more diverse sets of data are introduced each potentially with a being unique rates of Decay the value of the data changes directly with respect to
00:09:58 [W] Unique rates of Decay the value of the data changes directly with respect to its rate of Decay.
00:10:05 [W] So if the higher rate of Decay the more emphasis on producing actionable outcomes in real time data has gravity which is influencing. We're Computing needs to be located if ephemeral contextual data tends to exert the greatest
00:10:15 [W] I'm data has gravity which is influencing.
00:10:15 [W] where computer needs to be located if ephemeral contextual data tends to exert the greatest gravity. Typically pulling compute to the source.
00:10:19 [W] Data that is persistent and has broad applications with a low rate of Decay tends to have the low gravity and tends to move where the computers located with Cloud native application architectures and
00:10:34 [W] Is the veracity of data is a significant challenge.
00:10:41 [W] We see an increased need for trust in the data with a sub focus on protection of data both in motion.
00:10:46 [W] and at rest, but we're also seeing a trend towards protection of data in use.
00:10:53 [W] Application architectures are shifting towards a more disaggregated model offering greater agility supporting elasticity and providing greater control for software quality assurance by leveraging a scale out disaggregated model.
00:11:09 [W] have the ability to be deployed using a composable model with a mix of core code libraries open source, external applications and microservices, but the individual components of the application will be executed on the
00:11:25 [W] Open source external applications and microservices, but the individual components of the application will be executed on the platform that best fits. Its function given the context of the overall class of service
00:11:31 [W] Fitz it's function given the context of the overall class of service the benefit of this are substantial.
00:11:37 [W] However, we brings a lot of challenges application topologies and flows will have greater complexity ranging from tree structures to moored complex distributed meshmark.
00:11:55 [W] Consequential for an individual micro service, but when aggregated to the application Level it'll impact business process service levels and not all applications are equal.
00:12:09 [W] So supporting differential differentiated service levels are required to support the digitally where Enterprise
00:12:16 [W] The expectations of the digitally where Enterprise is that as we move towards a cloud native architecture servicemeshcon slyke performance latency, and Jitter will need to be improved while supporting application class of service guarantees.
00:12:31 [W] Turn it over to Killian cool discuss how we support the mapping of application classes service to the infrastructure capabilities and by using topology aware scheduling.
00:12:45 [W] Thanks, as Tom was saying there we're starting to get a good picture of what application the applications would need to work at new ways to express that but we're still falling really short in the hardware side kubernative eyes from the orchestration level.
00:13:00 [W] Which is made of but it doesn't know what it actually do. Right?
00:13:14 [W] So the scheduler can take requests for a number of CPU cores, but it doesn't actually know or doesn't care about how much processing an individual core can do memory topology, which is a complete blocker for truly predictable performance in workloads.
00:13:20 [W] So by the control plane up by understood about the schedule.
00:13:31 [W] So the work that you know of trying to figure out what a what a platform is actually able to do what the hardware is able to do is push to kind of cluster operators and sometimes even worse it can be application developers that end up doing this
00:13:38 [W] Form is actually able to do what the hardware is able to do is push to kind of cluster operators and sometimes even worse it can be application developers that end up doing this work, right?
00:13:41 [W] This really doesn't scale.
00:13:43 [W] Well, it leaves us with kind of secret recipe configuration files and we're directories are only ever get understood by a couple of people in the organization kind of litigate keeping issues, but just overall just organizational issues right Communications issues.
00:13:56 [W] So Tom laid out there kind of a way to express
00:14:02 [W] What applications might need and we need to match that by providing the way to describe Hardware in terms of in terms of applications can really understand. Right so meet them on applications terms rather than expressing Hardware as a bunch of
00:14:14 [W] Can litigate every issues but just overall just organizational issues right Communications issues.
00:14:15 [W] So Tom laid out there kind of a way to express what applications might need and we need to match that by providing the way to describe Hardware in terms of in terms of applications can really understand right so meet them on
00:14:16 [W] So I'm mostly talking about packet processing workloads.
00:14:22 [W] They have intense performance characteristics conflicts get really complicated as a result and this can be really really hard to manage.
00:14:31 [W] So here on the left hand side of the bottom of the pot requirements.
00:14:38 [W] There's just a kind of a model spec for resource request does a load of different resources in their CPUs virtual functions specific speeds in their specific power Headroom needed and there's new malignant needed as well something kubernative.
00:14:46 [W] It's just can't do right now, even though it has massive performance impact so we can reduce the spec if we build in kind of Hardware knowledge into a traits model that matches the application model that Tom was
00:15:01 [W] Match, the requirements and application has here, you know fast Network and maybe some encryption acceleration with power needs whatever it is and express that as a single trait.
00:15:15 [W] So in order to actually make this work, we kind of need two components.
00:15:22 [W] So one of those is an old trick Discovery agent which is able to actually describe pop Platforms in terms of their capabilities.
00:15:33 [W] And then you need a matching engine which is able to do the actual matching work between the application expression and the platform expression to find out what platform can actually meet the class of service required by specific application.
00:15:43 [W] So here we have a kind of a prototype working of this.
00:15:47 [W] So it used Prometheus in order to set up rules and I'm do the no trait Discovery part of this and then tell them to wear scheduling is able to match the workloads requirements to the the Prometheus expression of the trait.
00:16:00 [W] So when we describe Platforms in terms of capabilities, you get to push a lot of the complexity of Hardware configurations to a level where you can share it and kind of a consumable way across organizations publicly even
00:16:13 [W] It allows upgrades we made so.
00:16:16 [W] Able to match the workloads requirements to the the Prometheus expression of the trait.
00:16:31 [W] So when we describe Platforms in terms of capabilities, you get to push a lot of the complexity of Hardware configurations to a level where you can share it and kind of a consumed the way across organizations publicly, even it allows upgrades we made
00:16:33 [W] So you can actually control what level of performance is being given been offered to a specific application the most important part of this still is you get to keep application developers away from Hardware. They don't have to learn about different CPUs.
00:16:47 [W] They don't have to learn about the wattage of platforms cooling. None of that Hardware specific needs to be in a pod spec. If you get to this capability expression, so this is kind of a proof of concept of this but there are a bunch of caveats that
00:17:01 [W] we'll keep it from being a true working system.
00:17:05 [W] Really importantly here. This does kind of an ad hoc management of all the complexity of Numa topology management of memory all the scheduling for new methodologies done in Prometheus rules and intellectual rescheduling and
00:17:21 [W] Even though that's absolutely necessary for predictable performance.
00:17:32 [W] It's not going to be scalable across all sorts of different Hardware to be doing that through prompt ql queries.
00:17:35 [W] So the question is how do we actually get this up and running?
00:17:38 [W] So here we're looking at a dashboard.
00:17:41 [W] This is Griffon on the left-hand side in terms of Windows on the right hand side the top three boxes here.
00:17:48 [W] You can see the names of the notes Master One Node 1 node 2.
00:17:50 [W] They're an indicator of the actual trait that we're working with. So that's the 10-gigabit Nats.
00:17:52 [W] Network in a 1 million packets per second. We can see that they're all red. That means no surprise that they're in negative right there in -1 as well. That means that they're not currently advertising that traced to the cluster.
00:18:05 [W] We can see the reason for that as well just below. So this is the power head room. We know from from looking at the trace that power Headroom is actually important part of it.
00:18:14 [W] You need a hundred and fifty Watts available in order to run this kind of workload of code per spec. So we can't actually place on any of those notes. So this because we have kind of power hungry workloads across the cluster. So on the right-hand side in our terminal, I'm going to
00:18:30 [W] Function and I'm going to watch its progress through the cluster and then on the bottom right are the logs out of the Telemetry.
00:18:38 [W] We're scheduling system. So down there you will see the actual decision being made in real-time by the scheduler based on these traits.
00:18:45 [W] So a bunch of logs are after popping up there at the bottom, right?
00:18:54 [W] You can see for each node.
00:18:59 [W] So Master One node 102.
00:18:59 [W] Tell Em to our scheduling is aware that the node is actually violating a rule and that rule is the name of our trade.
00:19:09 [W] It's 10 gigabit networking at 1 make 1 million packets per second.
00:19:11 [W] So again this because we don't have enough power Headroom on the system to learn to rescheduling would continually as the the normal kubernative scheduler the default scheduler does continually retry this scheduling with the back off
00:19:25 [W] Wait for the power to actually come back.
00:19:30 [W] We're not going to wait for that workloads finish.
00:19:32 [W] We're just going to manually pre-emptive workloads.
00:19:33 [W] is just deleting the Pod. So that's a power-hungry application is going down and we're just back watching on our Cloud Network function again.
00:19:41 [W] So that workloads gonna take a minute to spin down.
00:19:50 [W] So what we do that we can have a look at some of the complexity that's actually being buried here.
00:19:56 [W] in this text box doesn't only to read all of this you can see the
00:19:58 [W] The actual makeup of a single query that satisfies this trait is the definition of that trait. So we have just setting Minimax value.
00:20:08 [W] So it's plus minus 1 we check what free CPUs are there.
00:20:18 [W] So which ones are being currently used by pads and that their new my line. So this is free pre CPUs / Newman old we check for the CPU frequency. Again, / Newman old huge Pages powerhead room, which is the important one here.
00:20:27 [W] We can check for number devices.
00:20:31 [W] This is our virtual functions are networked devices check that enough bandwidth is actually available on those devices.
00:20:40 [W] And then at the bottom, we're just adding a negative score for the missing nodes because this is a filter effectively but we want to make sure that we have minus ones for the notes that don't meet the trade as well.
00:20:50 [W] So in a couple of minutes, hopefully a few seconds to them for rescheduling will once again try to schedule our networking workloads.
00:21:08 [W] So this is our Cloud Network function. We can see it's in pending at the top right but we actually have a note to place it on now, so
00:21:14 [W] Our query there are Prometheus queries doing a few things that are normal scheduler can't do it. All one is it's looking at life contention, which is what we have with our power and the second thing is doing Newman alignment currently.
00:21:27 [W] There's no new alignment.
00:21:28 [W] that's memory topology in kubernative set the control plane level. There is an apology manager at the node level.
00:21:33 [W] So our workload has just been scheduled judging by the top right so its up and running we can see at the bottom right the last line of our logs. There is that the filter nodes for our Hardware trades master one, so that
00:21:49 [W] No been scheduled on Node 1.
00:21:52 [W] So just to show it up and running we're going to just run from the post.
00:22:03 [W] We're just gonna run a packet processing workloads on it will be able to see some of the packets coming through.
00:22:04 [W] This is a user space DVD K workload. So it's actually avoiding the colonel in order to get really high high packet rates in and out.
00:22:15 [W] So we use SRA v-necks and all that.
00:22:17 [W] So we'll see the workload on the third down here at the bottom.
00:22:21 [W] You can show packet through / so all of these components are available. They're all
00:22:25 [W] Source and there's a document on the Denver scheduling repo with the collateral for this demo and showing you how to set it up and implemented.
00:22:31 [W] And we can see there as the packet rate increases the trade indicator again turns the - 1 turns to red and that means that no workloads will actually be scheduled and consume that bandwidth.
00:22:42 [W] So Tom put forward new ways to think about application requirements in terms of classes servicemeshcon, and we've shown here that it's actually possible to meet that neither Side by generating kind of hired where traits and then matching them off each other.
00:22:58 [W] So the really kind of complex requirements that you get him perform of performance in terms of applications right now. You get loads of node labels you get nodes that have specific resources tainted you get big long list of resources all
00:23:12 [W] Cluster management with more accelerators more types of processors, you know, your people are talking about the theater of the XP right which is where you're going to have multiple different processing units in a machine.
00:23:26 [W] This is going to create more and more configuration overheads, right?
00:23:29 [W] You got more room for error more resources and locked up and just kind of managing these recipes to get your workload the performance that it requires just have this kind of spaghetti Amo building up over time. So
00:23:43 [W] even though it's complicated this particular performance is going to be incredibly important to getting certain workloads working kubernative Network packet packet processing is one of those also high performance computing workloads.
00:23:56 [W] The convicts that you that you're getting now, they're causing real trouble today and they're only going to get more complicated over time.
00:24:08 [W] It's going to be more taxing its covering more work.
00:24:16 [W] So the matching that we've shown here which is kind of application class service to live platform traits. It's one way to hurt in this obstacle, right?
00:24:20 [W] but there's a huge amount of work left to be done what we shown here today isn't going to be deployed on production anytime soon. So
00:24:27 [W] Does it feel things really missing in kubernative that just cause pain so memory quality management major roadblock seen that throughout this.
00:24:34 [W] There are kind of efforts going on through the signaled Resource Management Group to help fix these issues.
00:24:41 [W] Solutions for a those discussions.
00:24:48 [W] We hope that we'll get support for those at the cube that at this scheduling level maybe elsewhere in the control plane as well. If it needs to go into the the API or something like that.
00:24:54 [W] But the bottom line is if we don't want developers and coaster administrators and whoever else to have to add kind of Hardware engineering skills to their to their skill set. We need to change the way that we think about scheduling the way
00:25:09 [W] patience of hardware
00:25:11 [W] This is a first step down the road. But an order to move further we're going to need changes into kubernative.
00:25:20 [W] And I think Tom and I are around to take questions now.
00:25:27 [W] If anybody has anything.
00:25:30 [W] Hey Killian.
00:25:40 [W] I have a question potentially for you around the support in the community and how far how far things discussions have gone. And where do you see this advancing or how you see this advancing?
00:25:54 [W] Yeah, so as I mentioned there there are a couple of caps gone into kubernative.
00:26:05 [W] So right now what's being looked at is it mostly and out of tree model for so there's about new memory placement.
00:26:14 [W] So the ask is to kind of allow the kublr to advertise the the new impositions of devices and CPUs on the system so that we can then take that and turn into a coronary scheduling extender that would work like turn four.
00:26:24 [W] Scheduling but what be focused on Numa placement or yeah, so I think it's in the early stages so far.
00:26:38 [W] Under topology work scheduling that's actually addressing a lot of these issues.
00:26:48 [W] So yeah, hopefully we'll be following along over the next few months with those.
00:26:48 [W] I was wondering as well from your point of view.
00:26:54 [W] From the quality service side.
00:26:58 [W] what what aspects of quality of service do you think are really going to be the drivers to make applications kind of change? What what's going to be the first flag that they're going to wave?
00:27:12 [W] Well, I think you know, I mean we're heading into kind of the end of Moore's Law, which everybody talks about so looking at trying to get you know, better performance, you know, for example, or
00:27:22 [W] guaranteed Jitter requirements on applications and we see that in a lot of cases as you mentioned like packet processing and everything else, but maybe you know, if you think about you know, if you kind of deconstruct an application and look at that
00:27:37 [W] um, but maybe you know, if you think about you know, if you kind of deconstruct an application and look at that scaling it out one of the reasons why you might want to go with microservices is, you know, one two, obviously the quality control and everything, but also
00:27:46 [W] Obviously the quality control and everything, but also looking at running microservices that might run asynchronously to kind of the main main system.
00:27:59 [W] So you're running kind of a pseudo parallel environment, but the other aspect to is is potentially taken advantage of accelerators and other types of CPU architectures.
00:28:08 [W] So if you think about, you know kind of the monolithic application approach you running on let's say an x86 x86 does a lot of good things, but but it doesn't do you know
00:28:13 [W] A floating Point calculations or other types of things that maybe a GPU or fpga or arm chips might actually do better.
00:28:27 [W] So moving those, you know this that's one way to break in and out and then moving those over to the correct.
00:28:29 [W] Let's say workloads optimize processing.
00:28:35 [W] But but even that's you know, kind of not enough, I guess, you know because you really need to first of all you need you don't want the application developers to basically they have to
00:28:43 [W] putting the rules to you know to do that orchestration at the operation level which want to be able to do is to say it this particular workload is particular code or micro service requires certain latency guarantees certain Jitter guarantees as accident needs access to
00:28:58 [W] The and and really what you're doing is you're putting in those requirements as classes service requirements as opposed to coding them directly and put that into kind of metadata and then have the orchestration engine pick that up and say okay. I
00:29:14 [W] Stand the capabilities of a particular piece of Hardware particular CPU a particular where data might be and then orchestrate that you know going forward again.
00:29:26 [W] It's a little bit, you know, it's kind of a bigger picture of where you're scaling out things. But the three big things really that performance the the latency latency application application Communications and and obviously an application to data and also the Jitter requirements
00:29:41 [W] I see we have a question for you kill you.
00:29:45 [W] Yeah, so I think this is fired here.
00:29:57 [W] So Bart says kidding mentions that you might not want a stories and Dave some designed Hardware this being one of the objectives of topology where scheduling he says he disagrees with this goal and things that I saw you should totally understand their Hardware in HPV
00:30:05 [W] So I think that there's multiple types of dams and there's multiple types of reliability engineers and some of them are going to have to understand Hardware, but the way that configuration ends up looking for so
00:30:22 [W] Doesn't workloads it just turns into a spaghetti of the animal. It becomes very difficult to manage.
00:30:30 [W] There are tools to manage it of course. I mean Helm is there which is kind of those templating and there's other tools to generate configurations.
00:30:45 [W] But these end up being problems even with correct Version Control and everything because the hardware can drift under the software. So I think what I'm talking about here more than abstracting all x-rays away from Hardware
00:30:52 [W] Derby a lair
00:30:55 [W] between the hardware and most expertise I guess where there would be an understanding paired where an expression of Hardware in a more application friendly way so that s res.
00:31:07 [W] who who don't understand.
00:31:14 [W] There's too much hard work to really understand in in some of these deployments.
00:31:15 [W] There's too many accelerators.
00:31:17 [W] There's too many different profiles with smart Nixon other programmable accelerators. So you want a trusted layer where you can actually call on it.
00:31:27 [W] They just an application would need and yeah, so I don't think that a sorry Zora developer shouldn't understand Hardware at all. But I think there's a level Beyond where it just is going to become less and less useful and more and more difficult.
00:31:40 [W] So yeah, I don't think we have any more questions there.
00:31:55 [W] So after two o'clock, I guess I'm going to be in the Intel boot.
00:32:03 [W] We have a zoom chats a top of every wants to come and discuss some of this further that's in the expo hall in silver be where so if anyone wants to pop by the boot and have a chat. I'll be there.
00:32:12 [W] So yeah, I don't think we have any more questions there.
00:32:23 [W] So after two o'clock, I guess I'm going to be in the Intel boot.
00:32:24 [W] We have a zoom chat said up if anybody wants to come and discuss some of this further that's in the expo hall in silver be where it so if anyone wants to pop by the boot and have a chat. I'll be there.
00:32:25 [W] He'll be an HP booth in about two hours from now depending on the time zone you're in which I can calculate which time zone this is supposed to be an inverse is my time zone.
00:32:28 [W] But yeah, and also you can always reach out to me directly or Killian to you directly, you know, the capability of this platform allows you to do that and feel free to do that and open up, you know request a chat
00:32:41 [W] This is supposed to be an inverse is my time zone.
00:32:41 [W] But yeah, and also you can always reach out to me directly or Killian to you directly, you know, the capability of this platform allows you to do that and feel free to do that and open up, you know request a chat with
00:32:42 [W] Take your questions or just have a discussion.
00:32:46 [W] Yeah, actually, thanks for letting me Tom. I'll be in the booth about five minutes.
00:32:54 [W] So that's times on universal I think okay.
00:33:00 [W] Thanks very much.
