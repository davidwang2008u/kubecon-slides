In a Container, Nobody Hears Your Screams: Next Generation Process Isolation: KDRL-2654 - events@cncf.io - Wednesday, August 19, 2020 7:30 AM - 59 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:03:47 [W] Hello and welcome to in a container.
00:14:13 [W] No one can hear you scream Next Generation process isolation.
00:14:20 [W] I am Andy, I love to break stuff and then put it back together again on so know what it is made of and I'm also very proud to be an official trainer and occasional course author
00:14:32 [W] watch out for the upcoming SEC 584 hashicorp Docker O'Reilly and of course control planes detailed and extensive low-level Cloud knative security courses and as you may recognize from the title this
00:14:47 [W] Contains aliens.
00:14:52 [W] I'm a founder of control plane seccomp for Cuba Nettie's focusing on regulated Industries, including Financial Services.
00:15:00 [W] We espouse continuous infrastructure and security practices with a focus on cloud native deployments farm to table or get to Cuba Nettie's.
00:15:10 [W] And then containers no one can hear you scream.
00:15:17 [W] So what is this talk about? Well, we love Linux.
00:15:22 [W] It is a commonly used and well tested operating system containers are a form of isolation provided by the Linux kernel, however, like all software. It has had exploitable vulnerabilities and advanced sandboxing and isolation
00:15:37 [W] Up to mitigate the risk of unknown existing or future vulnerabilities while these techniques may also protect against Cuba Nettie's misconfigurations robust software and infrastructure delivery pipelines are a less
00:15:52 [W] To drink that risk for high sensitivity infrastructure workloads or data and new generation of sandboxing Technologies has emerged sometimes referred to as micro VMS. These Technologies are not
00:16:08 [W] It's crosed as they use multiple isolation mechanisms in addition to Virtual machines, including containers.
00:16:17 [W] And so the generic term sandboxing is used to cover the entire Spectrum.
00:16:25 [W] So the value of virtualization is a balance of its performance with security.
00:16:35 [W] Linux kernel is written in C a language with security bugs that have proven notoriously difficult to entirely eradicate virtualization could be thought of
00:16:38 [W] as a way to abstract a process as far from the Linux system call interface as possible to reduce the chance of exploitable vulnerabilities using an assortment of lsms and kernel modules containers and capabilities
00:16:53 [W] We're and hardware-assisted virtualization use of a use of a type safe language for the virtual machine and security such as golang or rust reduces the likelihood of bugs much more difficult to avoid in software
00:17:09 [W] see
00:17:11 [W] So a brief glossary for this talk the most important thing in a container is either the asset it contains all the data. It has access to be that Pi Financial records passwords that may be reused in other systems it
00:17:27 [W] The algorithms within including Quant trading algorithms machine learning models or an organization's secret source, so an untrusted workloads.
00:17:49 [W] untrusted workloads are commonly running virtual machines that public Cloud providers Rent To Us in hosted build and ci/cd infrastructure and uploads to video and audio transcoding Services untrusted workloads
00:18:05 [W] It's with a GP G or similar trusted thing untrusted workloads are commonly running virtual machines. The public Cloud providers Rent To Us in hosted build and ci/cd infrastructure and uploads to video and
00:18:06 [W] software with published zero days cves if no patch is available and the workload is business-critical isolating it further May decrease the potential impact of the vulnerability if exploited
00:18:20 [W] Of the vulnerability if exploited a container image build can be considered untrusted if it's desired contents are unknown as it may contain malicious code to scan Network and points exfiltrate code in
00:18:31 [W] Or tack the host container images and code from unauthenticated external sources may also be considered untrusted for risk-averse organizations a sensitive workloads.
00:19:02 [W] ER tenant may be considered untrusted mitigation with kubernative knative Solutions such as admission control and network policy may be sufficient in some cases, but for advanced isolation from unknown vulnerabilities
00:19:17 [W] Thrice access to this may include fraud detection systems pricing engines or high frequency trading algorithms on a Cuban esas cluster a single workloads.
00:19:20 [W] See demon sensitive or untrusted workloads may be isolated further with next-generation sandboxing.
00:19:27 [W] So what is wrong with containers?
00:19:35 [W] Well, let's define firstly what we mean by containers.
00:19:46 [W] We starts with the high-level nth faces the contain a manager's kubernative Docker and Padma an which interact with their respective libraries to perform useful container management features, including pushing and pulling container images
00:19:50 [W] Opa and the Run see demon sensitive or untrusted workloads may be isolated further with next-generation sandboxing.
00:19:52 [W] So what is wrong with containers?
00:19:52 [W] Well, let's define firstly what we mean by containers.
00:19:53 [W] We starts with the high-level end faces the contain a manager's kubenetes Docker and Padma an which interact with their respective libraries to perform useful container management features, including pushing and pulling container images,
00:19:55 [W] Storage and network interfaces and finally calling the low-level containerd runtime.
00:19:59 [W] That's low level containerd runtime is directly responsible for starting and managing containers interfacing with the colonel and creating that isolated containerized process a major difference between a
00:20:11 [W] And the VM is the containers do not use hardware-assisted virtualization and so more widely compatible.
00:20:20 [W] But of course containers do not really exist.
00:20:24 [W] They are merely a user space fiction.
00:20:28 [W] However, they are kind of a form of virtualization.
00:20:31 [W] So a common perception is that containers are optimized for Speed and portability and virtual machines sacrifice these features for more robust isolation from malicious behavior.
00:20:41 [W] The higher fault tolerance, but this perception is not entirely true.
00:20:51 [W] Both Technologies share a lot of common code Pathways in the kernel itself containers and virtual machines have evolved like co-orbiting stars never fully able to escape each other's gravity and spewing matter into
00:21:01 [W] The container runtimes are form of Colonel virtualization and the oci container image specifications have become the standardized Atomic unit of deployment.
00:21:13 [W] Next Generation sandboxes combined container and virtualization techniques to reduce workloads access to the kernel by emulating Colonel functionality in user space or the isolated guest environment.
00:21:29 [W] So do you think the host attack surface to the process inside the sandbox?
00:21:34 [W] Well defined interfaces can help to reduce complexity minimizing the opportunity for untested code paths and by integrating the sand boxes with containerd e they are also able to interact with oci images and with shims
00:21:49 [W] Cuban s's, so what is wrong with containers containers are not inherently insecure, but for some workloads, they leaked the hosts abstractions too much underlying concerns about a root own
00:22:05 [W] too much underlying concerns about a root own demon can be assuaged by running rootless containers in unprivileged user name space mode, but this introduces another risk user name spaces have historically been a rich
00:22:16 [W] Alice's
00:22:19 [W] the next was originally written with a lot of code, assuming that it was being run by root in the host name space and some of these assumptions have changed with the introduction of user name spaces the last kernel namespace to be completed.
00:22:35 [W] to whether it is riskier to run a route own demon or user name spaces isn't entirely clear there have been more high-profile breakouts from Route owned Docker, but this may be down to adoption and widespread
00:22:52 [W] and Docker but this may be down to adoption and widespread use ruthless containers without a route own demon provider security boundary when compared to root own demons when code
00:23:02 [W] And by the hosts root user is compromised.
00:23:08 [W] It can potentially read and write other users files ARP spoof and sniff track of traffic or install malware to the firmware or Colonel to avoid this potential exploit chain.
00:23:20 [W] see for example, the proc self XE breakout replacing the Run see binary from inside the container and attacking the host from inside the container by responding to dock a copy.
00:23:30 [W] Rootless containers can be created from within a user name space and this is supported in the latest versions of Docker and Padma an user name spaces allowed.
00:23:45 [W] Non-root users to pretend to be the hosts root user. The route in user name space user can have a fake uid 0 and permission to create new namespaces Mount Nats cetera
00:23:57 [W] Root user the route in user name space user can have a fake uid 0 and permission to create new namespaces Mount Nats cetera change the containers hostname and
00:24:00 [W] hostname and mount find mounts and temper Fest file systems
00:24:03 [W] This allows the roots in user name space, which is unprivileged in the host name space to create new containers to achieve this additional work must be done network connections into the host
00:24:20 [W] The work must be done network connections into the host network name space can only be created by the hosts Roots.
00:24:27 [W] So for ruthless containers and unprivileged slurp for Nats Ms.
00:24:33 [W] Guarded by seccomp in the kernel is used to create a virtual Network device.
00:24:34 [W] Unfortunately Mountain remote file systems also becomes difficult when the remote filesystem. For example, NFS home directories does not understand the hosts username space.
00:24:46 [W] Well rootless pop man has selinux support and dynamic profile support run see a doctor does not yet support apple armor, and for both runtimes cryo is disabled.
00:25:01 [W] Both rootless runtimes require configuration for some networking features cap net by and service is required by the kernel to bind the port's below 1024 which is historically considered a privilege boundary and ping is not supported for users with high,
00:25:17 [W] X cryo is disabled.
00:25:17 [W] both rootless runtimes require configuration for some networking features cap net by and service is required by the kernel to bind the port's below 1024 which is historically considered a privilege boundary and ping is not supported for users with high
00:25:19 [W] If the ID is not in proxies net ipv4 pin group range, but this can be changed by the host root user host networking is not permitted as it breaks Network isolation and
00:25:33 [W] V2 function but only when running and the systemd cgroup V1 is not supported by either rootless implementation.
00:25:44 [W] So Docker and Mobi and Padma and share much of the same code for ruthlessness and have been developed in parallel.
00:25:55 [W] They share similar performance and features.
00:25:58 [W] Although Docker has an established networking model.
00:26:00 [W] That doesn't support host networking whereas Padma an reuses kubernative cni interface.
00:26:05 [W] Well, ruthless containers protect the host from the container. So abstractions may still leak from the host or though they become much less dangerous.
00:26:18 [W] For example, proc host devices the kernel interface.
00:26:19 [W] So is this exposure to see based system calls in the Linux kernel from a rootless containerd runtime bad. Well the colonel of Linux Powers the internet and the world and has done so
00:26:34 [W] But its lack of memory management leads to the same critical bugs over and over again and when the colonel openssl and other critical software are written in C.
00:26:50 [W] We just want to move everything as far away from trusted kernel space as possible.
00:26:54 [W] Virtual machine research began in the 1960s to facilitate sharing large expensive virtual machines between multiple users and processes using Hardware software or a combination of the two to achieve isolation processes memory.
00:27:13 [W] To facilitate sharing large expensive virtual machines between multiple users and processes using Hardware software or a combination of the two to achieve isolation processes memory and the resources. They require from the
00:27:15 [W] Sources they require from the physical host machine.
00:27:17 [W] The host machine is split into smaller isolated compute units traditionally referred to as guests.
00:27:30 [W] These guests interact with a virtualized interface to the host CPU and devices and the interface intercepts system calls to handle them Itself by proxying to the hosts kernel or using its own code to
00:27:39 [W] Full virtualization for example VMware emulates hardware and Boots full Colonel inside the guest operating system-level virtualization. For example, a container emulates the hosts kernel using
00:27:56 [W] cgroup capabilities and seccomp and so can start a containerized process directly on the host kernel processes in containers, share many of the colonel Pathways and security mechanisms that processes
00:28:12 [W] Executes the boot a colonel a guest operating system will require access to a subset of the host machines functionality including bios routines devices and peripherals.
00:28:27 [W] For example, keyboard graphical console access storage and networking as well as an interrupt controller and Interval Timer some source of entropy for randomization and the memory address space that it will be
00:28:41 [W] It should be noted that despite many decades of effort in practice.
00:28:49 [W] No virtual machine is completely equivalent to its real machine counterparts.
00:28:52 [W] Inside each guest virtual machine is an environment in which processes or workloads can run. The virtual machine itself is owned by privileged parent process that manages its setup and interaction with the host.
00:29:07 [W] As a virtual machine monitor or vmm.
00:29:15 [W] This is also previously known as a hypervisor, but the distinction is blurred with more recent approaches.
00:29:21 [W] So the original term vmm is preferred Linux has a built-in virtual machine manager called KVM that allows a host Colonel to run virtual machines along with Q mu which emulates physical devices and
00:29:33 [W] Management to the guests and can also run by itself if necessary and operating system can run fully emulated by the guest OS + Q mu this emulation Narrows the interface between the VM
00:29:49 [W] And reduces the amount of Kernel code the process inside the VM can reach directly this provides a greater level of isolation from unknown kernel vulnerabilities.
00:30:02 [W] Different Technologies take different approaches to moving away from Linux system call interaction for the guests Linux containers are the most lightweight form of isolation as they allow workloads to use Colonel apis directly minimizing the
00:30:17 [W] Gee visor starts a KVM virtual machine or operates in Petrus mode and inside starts a user space kernel which proxies system called down to the hosts using a Centre process this trusted
00:30:34 [W] Proxies system called down to the hosts using a Centre process. This trusted process re implements 237 Linux system calls and only needs 53 system calls to operate itself.
00:30:43 [W] Straight into that list of system called by seccomp and it starts a companion file system interaction process called gopher, which prevents a compromised Century process interacting with the hosts file system.
00:30:59 [W] Finally. It also implements its own user space networking stack conversely firecracker while also using KVM instead of implementing the heavyweight qmu process to emulate devices as a
00:31:12 [W] Conversely firecracker while also using KVM instead of implementing the heavyweight qmu process to emulate devices as a traditional Linux virtual machine might do it starts a stripped-down device emulator instead.
00:31:18 [W] might do it starts a stripped-down device emulator Instead This reduces the host attack surface and removes unnecessary code requiring only 36 system calls for itself
00:31:28 [W] 86 system calls for itself the function G visor and firecracker both operate on the premise that there statically typed I go along or rust system called proxying in between the
00:31:39 [W] Lang rust system called proxying in between the process and the host kernel is more secure against untrusted workloads than the limit kernel itself and that their performance is not significantly impacted.
00:31:50 [W] And finally at the other end of the spectrum K VM + qm uvm's emulate hardware and so provide a guest Colonel and full device emulation which increases startup times and therefore memory footprint
00:32:06 [W] Virtualization is expensive due to many layers of abstraction.
00:32:15 [W] It permits running unmodified guest operating systems and does not require Hardware that is CPU virtualization support device a firecracker and cattle containers all take different
00:32:25 [W] Operating systems and does not require Hardware that is CPU virtualization support.
00:32:27 [W] Device a firecracker and cattle containers all take different approaches to Virtual Machine isolation and them to challenge the perception of slow start of time and high memory overhead.
00:32:36 [W] Each system is a combination of virtual machine and container Technologies some vmm processes a Linux kernel within the virtual machine.
00:32:44 [W] And what's the colonel has booted a Linux user space in which to run the process?
00:32:49 [W] And some combination of kernel-based isolation that is container style namespaces cgroup set comp either within the VM around the vmm or some combination thereof.
00:33:01 [W] Gee visor protects app engines standard environment Google Cloud functions Cloud. Ml engine and Google Cloud run and as it has modified for Google gke currently has the best integration with Docker and kubernative
00:33:17 [W] Out mln engine and Google Cloud run and as it has modified for Google gke currently has the best integration with Docker and kubernative from the sandboxing Technologies.
00:33:22 [W] It can be considered as either a merged gasps Colonel and virtual machine monitor or as seccomp on steroids as well as using its Centre process. It also provides a masked view of the
00:33:34 [W] Virtual file systems these file systems have historically leaked the container abstraction by sharing information from the host such as memory devices processes cetera Centre prevents the application interacting
00:33:50 [W] Information from the host such as memory devices processes cetera Centre prevents the application interacting directly with the host Colonel and seccomp is used around the G visor Colonel to limit system calls.
00:33:56 [W] most Colonel and seccomp is used around the G visor Colonel to limit system calls and prevent escalation in case of tenants breaking into Centre and attacking the host Colonel implementing a user space
00:34:06 [W] I think I use a space kernel is a Herculean undertaking but the lack of full system called support means some applications are not able to run Angie visor in practice.
00:34:18 [W] Most of these system calls were excluded for a reason and compatibility is generally very good.
00:34:22 [W] aside process to the Centre called gopher handles iot
00:34:45 [W] However, this comes at the cost of some reduced application compatibility and the hyper system call Overhead, of course not all applications make a lot of system calls.
00:35:01 [W] So this is dependent upon usage.
00:35:02 [W] Sentries userspace OS kernel implements all the colonel functionality needed by the untrusted application.
00:35:12 [W] Although it does make some host system calls to support its operation. It will not allow the application to directly control the system calls that it makes to the underlying Linux kernel
00:35:21 [W] Application system calls are redirected to Century by a platform Cisco called switcher that intercepts the application when it tries to make system calls.
00:35:31 [W] Cool to be generated by the application it then captures the system call handles it and returns a response to the process a simple concept that abstracts the complexity from a user
00:35:51 [W] this called switcher jiva's system called Interceptor has two modes Pete rice and KVM the Petri a system called provides a mechanism for a parent process to observe and modify another processes Behavior Petrus
00:36:07 [W] Horses the traced process to stop an entry to this next Siskel & G visor is able to respond to it or proxy the request to the host Colonel going via gopher if I owe his required
00:36:23 [W] is a vmm that boots a dedicated virtual machine for its guests using KVM, but instead of using kvms traditional device simulation pairing with qmu firecracker implements, its own memory management and device simulation
00:36:39 [W] And with Q mu V crack, it implements its own memory management and device simulation.
00:36:41 [W] It has no bias instead implementing Linux boot Protocol.
00:36:47 [W] No PCI support and stripped down simple virtualized devices with a single network interface a block iot Vice a timer clock serial console and the keyboard device that only simulates control-alt-delete in
00:36:59 [W] the late in order to reset the VM firecracker itself will be compatible with kubenetes and oci when the firecracker containerd e project is complete the firecracker vmm process that
00:37:13 [W] Machine is in turn started by a Jailer process.
00:37:21 [W] The Jailer configures the security configuration of the vmm.
00:37:21 [W] Sandbox the GID and uid assignment network name spaces creating a true root and cgroup s' then terminates and pass this controlled firecracker.
00:37:33 [W] We're set competent forced around the KVM guest Colonel and the users based that it boots.
00:37:36 [W] We're g visor proxies iot through a secondary process firecracker uses the KVM virt iot drivers proxying from the guests firecracker process to the host kernel using the vmm
00:37:52 [W] Mmh starts it boots into protected mode and guest Colonel never running in its real mode.
00:38:00 [W] Firecracker in folks far less host kernel code than traditional Lexi orgy visor once it has started. Although they all touch similar amounts of Kernel code at runtime performance
00:38:16 [W] from an isolated memory stackrox
00:38:46 [W] and is compatible with Kata containers ignite via Cube by crack containerd e it provides soft allocation for more aggressive bin packing and a greater resource utilization.
00:38:58 [W] Cats containers are lightweight VMS containing a container engine highly optimized for running containers.
00:39:15 [W] They are the oldest and most mature of the recent sandboxes and were originally called clear containers compatibility as wide would support for most containerd orchestrators.
00:39:20 [W] grown from a combination of Intel clear containers and - sh run V Kata containers wraps containers with a dedicated KVM virtual machine and device simulation from a pluggable back-end
00:39:36 [W] Name you a custom stripped down Q mu or firecracker.
00:39:44 [W] It is an OCR runtime and supports Cuban Etta's which does not require modification of container images the cat containerd runtime launches each container on a guest colonel in its own Hardware isolated VM
00:39:56 [W] Container on a guest colonel in its own Hardware isolated VM. The cattle runtime is the vmm and in face to the oci cat a proxy handles io4 the cat agent and therefore the application using
00:40:06 [W] Cereal and Multiplex is a command Channel over the same connection cat a shim is the interface to The Container engine handling containerd lifecycles signals and logs.
00:40:22 [W] The guest is started using K VM + qm. Your firecracker.
00:40:33 [W] The project is forked qmu twice to experiment with lightweight start times and has re-implemented a number of features back into Q mu which is now preferred to nebu the most recent Fork inside the VM qmu boots.
00:40:37 [W] And optimized Colonel and systemd starts the cat agent process cat agent manages the containers running inside the VM which uses lidded container. And so she has a lot of code with run see networking is provided by
00:40:51 [W] And has re-implemented a number of features back into Q mu which is now preferred to Nemo the most recent Fork inside the VM qmu boots and optimized Colonel and systemd starts. The cat agent process
00:40:53 [W] Nine or Toc is CNM and the network name space is created for each VM because of its networking model the host Network cannot be joined selinux and a polymer are not currently implemented and some oci
00:41:07 [W] It's the docket integration.
00:41:10 [W] And honorable mention many new VM M technologies have some rustling components. So is rust any good.
00:41:23 [W] It's similar to golang in that it is memory safe.
00:41:28 [W] Its memory model virt iot cetera, but it is built Atop A Memory ownership model which avoids whole classes of bugs including use after free double free and dangling pointer issues.
00:41:37 [W] It has safe and simple concurrency and no garbage collector, which may incur some
00:41:41 [W] Actualized overhead and latency instead using build time analysis to find segmentation faults and memory issues based upon the safeness of the language rust vmm is a fast development toolkit for new VMS.
00:41:55 [W] It is a collection of building blocks that is rust crates comprise the virtualization components.
00:42:07 [W] These are well tested and therefore better secured and provide a simple clean interface. For example, the VM memory create is a guest
00:42:10 [W] memory abstraction providing a guest address memory regions and when used in firecracker guest shared memory.
00:42:19 [W] The project was burst birthed from Chrome OS has crossed VM which was forked by firecracker and subsequently abstracted into the hypervisor from scratch crates.
00:42:34 [W] So, how do we run these things?
00:42:48 [W] Well Cuban accent Docker support different container runtimes at the same time a node can run pods with different cri-o s this means that we can separate out workloads Docker is able
00:42:58 [W] It's this means that we can separate out workloads.
00:42:59 [W] Docker is able to run any oci compliance runtime kubenetes requires a runtime to also be cri-o.
00:43:19 [W] What are the risks well the degree of access and privilege that our guest process has to host features or virtualized versions of them impacts the attack surface available to an attacker in control of the guests process.
00:43:36 [W] So this Neutron should sandbox Technologies is under active development its code and like all new code.
00:43:44 [W] is that risk of exploitable books? This is a fact of software and this infinitely better than no new software. So
00:43:51 [W] Potentially these sandboxes are not yet a target for attackers the level of innovation and Baseline knowledge to contribute means the barrier to entry is set high from and administrators perspective modifying with the bugging
00:44:05 [W] And box becomes slightly more difficult similar to the difference between Bare Metal and containerized processes.
00:44:15 [W] These difficulties are not insurmountable and require administrator familiarization with the underlying runtime.
00:44:27 [W] It is still possible to run privileged sandboxes. And although the risks are fewer than privilege containers users should be aware that reducing isolation increases the risk of running the process inside the sandbox
00:44:35 [W] Notice uses should be aware that reducing isolation increases the risk of running the process inside the sandbox.
00:44:36 [W] Narrowing the interface between a Sandbox process and the host is a risk based decision.
00:44:42 [W] There are some trade-offs debugging becomes much harder and traditional tracing tools may not have great compatibility.
00:44:49 [W] There is a performance impact, but this may be negligible for some workloads and benchmarking is strongly encouraged as Next Generation runtimes have focused on stripping down Legacy compatibility.
00:45:02 [W] They are very small and very fast to start up compared to traditional VMS.
00:45:04 [W] Not as fast as Lexie or Mobi but fast enough for function as a service providers to offer that regressive scale up rates.
00:45:17 [W] So application workloads should be categorized by risk.
00:45:19 [W] There's this application access a high-value asset is this application able to receive untrusted traffic or have there been vulnerabilities or boat in this application before if the answer to any of those is yes, you may want to consider a next-generation
00:45:32 [W] Uprights. So application workloads should be categorized by risk.
00:45:33 [W] There's this application access a high-value asset is this application able to receive untrusted traffic will have there been vulnerabilities or boat in this application before if the answer to any of those is yes, you may want to consider a next-generation
00:45:34 [W] money
00:45:34 [W] So unless you have specific problems containers of probably just fine for higher sensitivity workloads and data.
00:45:45 [W] perhaps we want more isolation.
00:45:48 [W] There is an affinity between some of these Technologies and their supporting cloud provider which makes it a lot easier to use a supported runtime and rust vmm means that there will be many more hypervisor base containerd run times in the future.
00:46:00 [W] Thanks for listening. Have a wonderful day.
00:46:03 [W] Hello.
00:46:08 [W] thank you for your attention.
00:46:11 [W] I will go through a few of the questions.
00:46:12 [W] So which of the different sandboxing options have you seen being used in the field which you seeing as the most popular or successful?
00:46:21 [W] G5 is a generally because of its tight integration with gke is the most common thing that we see by extension. I guess everybody running in Lambda is using firecracker, but from an OC, I / spective the compatibility
00:46:36 [W] D that shim is still not quite rubber-stamped for production and that is ultimately impacting its adoption cast containers.
00:46:51 [W] I haven't come across anything personally, but I do know if some instances where it has been run for numerous I guess years at this point for at least a couple of years and especially popular one is video transcoding and it's
00:47:02 [W] I have to defend against bugs in deserialization and the bugs in the implementation of the video specifications
00:47:18 [W] And the bugs in the implementation of the of video specifications and that in terms of in terms of success, I would say again that the the ease of just using run SC.
00:47:25 [W] Terms of in terms of success.
00:47:26 [W] I would say again that the ease of just using run SC with the Run C or with mobe Docker client.
00:47:29 [W] It's huge.
00:47:32 [W] You can just bootstrap the thing and it just works the same broadly in Cuban that is maybe a little bit of extra config. Of course if you're not on GK, but that means that run see run SC orgy visor rather
00:47:44 [W] In detail and with that level of sort of ease of use then yeah, it becomes very simple to use the other confounding feature of course is is nested virtualization impacts.
00:48:00 [W] And of course that makes true or actual virtualization a little bit harder to achieve.
00:48:10 [W] What else do we have wouldn't a microkernel based OS like Minix not be a solution for container based.
00:48:15 [W] Heroes I don't have enough experience.
00:48:24 [W] I think that the general rule of thumb is a nabla was here in spirit on this talk.
00:48:25 [W] Did the difficulty with reduced system call surfaces. So, you know kernels rump kernels. I believe minutes has a slightly reduced waste microkernel is that the workload needs adapting to run in that environment and that really
00:48:40 [W] Most users from easily adopting or even testing these things if there is a compilation or a translation sort of step to go onto to change that workloads.
00:48:56 [W] What else do we have how much for security impact is it to enforce?
00:49:04 [W] Non-root users in containers?
00:49:06 [W] This is a bit of a kind of hack needed refrained from me at this stage.
00:49:12 [W] But running is the root user inside a container that is running with a root owned.
00:49:16 [W] containerd Eamon means that that user inside the container that runs the process is quite close to the host root user. The security abstractions in the middle are
00:49:26 [W] Seccomp a polymer nasty Linux the namespaces that we have excluding user name spaces and the capabilities that we've attached so actually if we just drop all capabilities through user inside the container running that process even if
00:49:42 [W] Of hackneyed refrained from me at this stage but running is the root user inside a container that is running with a root owned.
00:49:43 [W] containerd Eamon means that that user inside the container that runs the process is quite close to the host root user.
00:49:44 [W] The security abstractions in the middle are seccomp a polymer and selinux the namespaces that we have excluding user name spaces and the capabilities that we've attached so actually
00:49:45 [W] If we just drop all capabilities through user inside the container running that process even if they break out via that chain doesn't really have well, they need to privilege to break out and the actual
00:49:50 [W] It's via that chain doesn't really have well, they need the privilege to break out and the actual breakout process normally involves exploiting group privilege in some way and that really is the core point if you're ruthless like the container
00:49:58 [W] The core point if you're rooting side the container you have access to do things like remount devices.
00:50:02 [W] Remounting a device with a storage file system behind it might mean that you can then write to something that's owned by roots on the host pick.
00:50:12 [W] Have we seen a types of workloads that are not well suited or don't work. So well with hardened runtimes there's a minor performance penalty.
00:50:49 [W] So workloads that churn very rapidly would pay a slight penalty. But it in the order of sort of 250 milliseconds.
00:50:56 [W] It's not really that dramatic. There is also of course a right through penalties.
00:51:02 [W] Sorry, essentially because everything is sandboxed.
00:51:05 [W] Then anything that interacts with devices external and think they interact with devices pays a performance penalty. So networking becomes more expensive slurp for are net and S is actually really fast compared to the other options and
00:51:21 [W] minor penalty, but of course if you're in G visor, there is a dedicated TCP IP stack which is the slowest of the bunch if you're in firecracker on the other hand your disk right is Superfast because everything is just flushed
00:51:36 [W] To well nothing is actually flushed at this gets all just written to memory and then lazily persisters. So
00:51:43 [W] It is still almost horses for courses from that regard.
00:51:52 [W] Everything can probably read in something. I would say.
00:51:55 [W] what else do we have?
00:51:56 [W] My audio is gone. That is unhelpful.
00:52:02 [W] I don't know what I can do.
00:52:03 [W] Thanks to pulseaudio perhaps
00:52:06 [W] okay, allegedly its back. So hopefully you've enjoyed my wild gesticulations in order to kill the dead are what is a good way to test these runtimes yourself.
00:52:24 [W] Can I run them easily on my local machine or do recommend testing in the cloud directly?
00:52:28 [W] everything so the easiest thing is probably just to run a local Linux box with VTX virtualization extensions enabled and
00:52:46 [W] What that means is that there is no nested virtualization penalty.
00:52:55 [W] Otherwise, if you want to try some of this stuff you have to be on a bare metal box, which is quite expensive to rent.
00:53:05 [W] I mean, maybe if you're perhaps pack it is a lot cheaper than AWS from that regard or maybe one of those like hurts in the style.
00:53:11 [W] But if you just use your own machine, you can install of the all of these things really quite easily. Probably the most difficult is
00:53:15 [W] still firecracker because it is just a native implementation. You start a demon and then you've got HTTP API access to it.
00:53:27 [W] So trying that is actually a little bit easier.
00:53:29 [W] you can use the containerd ischium. And we've also have a really useful product called ignite which basically brings oci compatibility to firecracker and then on top of that you have something called fire Cube, which can obviously brings kubenetes in
00:53:42 [W] The yeah, so that's the easiest way to kind of get to grips with it at a low level.
00:53:51 [W] But then the the cloud integration for I know it's an official at this point, but you can just click a button in gke to try G visor and or add it to your workloads and get UPS deploy it of course with a good system acceptance sweet.
00:54:06 [W] Easy way to see if the whole if you have any fundamental linkerd path buses, but at this point G visor is very broadly compatible.
00:54:18 [W] And yeah, what else do we have?
00:54:26 [W] Do I have any opinions on encrypted memory is necessary in the container use case or will debug an admin overhead be too high for agility.
00:54:34 [W] That's really interesting one. Actually encrypted memory is something that I'm
00:54:38 [W] obviously because I'm interested in forensics a lot of tools do not bother encrypting memory so volt for example, if you route a volt box, you can dump the memory and run strings across it and see what was in there so
00:54:53 [W] that specific case where the
00:54:57 [W] application has not deemed itself to have a requirement for encrypted memory.
00:55:08 [W] I would love to just drop that in because that seems like fundamental sort of difference for an HSM, which volt is often used close to or in replacement for so yeah broadly. I suppose if something that resides in memories critical enough
00:55:19 [W] Hoping it would result in a bad day then.
00:55:25 [W] Yeah, it makes perfect sense how it would be implemented.
00:55:28 [W] I don't know I guess.
00:55:29 [W] It's almost as if it would be a security context extension because you wouldn't necessarily want it uniformly across everything.
00:55:39 [W] The runtime is running.
00:55:41 [W] Yes interested to talk with.
00:55:45 [W] Mr. Brandon Lum more on that one.
00:55:48 [W] Very interesting question.
00:55:49 [W] And let's see what else we've got here.
00:55:56 [W] The slides are up on sheds recommend cri-o for kubenetes.
00:56:05 [W] I mean, it depends what your workloads and risk profile our default answer from a security engineer.
00:56:09 [W] Why isn't there a Next Generation sandbox written in Python? I mean Docker began its journey in Python. So potentially there could be but of course when it comes to the virtualization extensions, it looks like everything.
00:56:19 [W] It's not jeeva, sir is going to standardize on these Rusty.
00:56:24 [W] Mmm crates.
00:56:30 [W] There's even a cloud hypervisor from from Intel using rust VM M. And this idea of plug ability in terms of the hypervisor is just is just crazy if everything becomes, yeah
00:56:39 [W] Books written in Python. I mean Docker began its journey in Python. So potentially there could be but of course when it comes to the virtualization extensions, it looks like everything that's not jevo iser is going to
00:56:41 [W] Under an interesting extension to that is only only Google of chosen golang to write one of these things.
00:56:51 [W] Everyone else is moving more towards rust and there's I guess some questions memory management. I think specifically what 4G visors the said descendancy I
00:57:03 [W] Coming from that internal Google tooling. It makes perfect sense because of course firecracker Etc comes from the Chrome OS VM and all that was written in Rust I could just make sense to continue that lineage.
00:57:19 [W] I guess they could be a python shim.
00:57:25 [W] What else you doing here?
00:57:26 [W] Sorry, just ordering these questions.
00:57:38 [W] What are the top three low-hanging fruits for securing containers in Cuban asses?
00:57:49 [W] I mean, it depends if you're talking generally.
00:57:56 [W] I've just got a cluster and I want to secure my workloads.
00:57:58 [W] policy by which I mean everything from Network policy through I mean Network policy probably just stands by itself Network policy static analysis of deployed workloads and by which I mean positivity policy and opa
00:58:15 [W] Preventing. I mean just running the standards.
00:58:19 [W] Particular T like Opa set so we don't hand out excess privilege and I mean depending on your risk profile.
00:58:35 [W] I really like supply chain security is one of my focus points and identifying that the provenance of artifacts that you deploy into your cluster.
00:58:43 [W] So did this really come from someone that I trust?
00:58:47 [W] Who do I trust in the first place? And how do I sign an artifact so that I know?
00:58:51 [W] It is what I think it is.
00:58:54 [W] That's a combination of right now probably three Technologies one is notary for freshness and replay prevention. So sopping essentially just signing images with some trust another is in-toto from
00:59:06 [W] making sure that each build a stage has passed and then re verifying that so that malicious internal actors cannot just slide stuff straight into production by editing Padma manifests at your gitops
