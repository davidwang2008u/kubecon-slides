Next Generation of CI/CD: Analytics-driven Traffic Management on Kubernetes: STYZ-5609 - events@cncf.io - Thursday, August 20, 2020 8:27 AM - 367 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:36 [W] Hello.
00:02:41 [W] Thanks for watching this presentation.
00:02:45 [W] I hope you enjoy watching it as much as I enjoy talking about this topic before I start.
00:02:52 [W] Let me tell you that what you are about to see has been done as part of a project that we call iterate.
00:02:59 [W] This is an amazing group of people who have been hard work to make this happen.
00:03:04 [W] I'm willing to Guess that a few people while looking at cubic owns program encounter the title of this of this presentation in this ci/cd track and
00:03:21 [W] It started rolling their eyes.
00:03:24 [W] Thinking to themselves.
00:03:27 [W] Oh, this is yet.
00:03:27 [W] Another demo on how to do cannibalism kubernative.
00:03:37 [W] This is not that I will not talk about how to integrate Tools in your ci/cd pipeline to perform Canary releases on kubenetes.
00:03:47 [W] What what I want to do is to raise awareness of a fundamental problem that is at the core of techniques such as Canary releases a be testing in ABN testing.
00:03:57 [W] Problem that has been largely ignored in the community.
00:04:03 [W] So I want to offer a solution to the problem and open solution to the problem and engage the community in that discussion any ass in this process. I'm going to show you two demos.
00:04:13 [W] When we talk about techniques such as Canary Lisa's a be testing an ABN testing, we know it's all about agility.
00:04:25 [W] We want to find a way is to deliver code to production as quickly as we possibly can in order to survive in a software driven Market.
00:04:32 [W] We want to stay relevant.
00:04:34 [W] We want to retain the collect the customers that we have. We want to attract new customers.
00:04:38 [W] But they're opposing forces that make us think twice before releasing new code into production.
00:04:50 [W] We are always worried about the fact that bugs might make their way find their way into production.
00:04:54 [W] We might be worried that our called do not be performant enough and perhaps violate.
00:04:56 [W] I know performance guarantees that we're promising to the same clients that we want to retain in the worst case. We might worry that our code changes will inadvertently caused the revenue for our company to decline.
00:05:13 [W] You know, I'd really gives us the opportunity to learn right the more we the play according to production the more we learn about how the car behaves in production or in test environment more importantly
00:05:30 [W] important to learn about our users
00:05:34 [W] What users want we have the political learn how to find new ways to increase our company's revenue and how about that as we learn all those things.
00:05:50 [W] How about maximizing revenue for our company?
00:05:54 [W] What if I told you that these things can be possible in you you're probably mapping to your mind the concepts that I'm talking about here to the same techniques that we mention a few moments earlier. No,
00:06:06 [W] Good behavior behavior in production. You can think about Canary lenses for that performance test.
00:06:11 [W] You know tell us what users want can be mapped to a/b testing or Indian testing.
00:06:20 [W] I would like to collectively call these techniques continues experimentation and the work continues here is important because it has the connotation that is things are done continuously very very frequent frequently and in order
00:06:32 [W] It allows us to actually embrace that mindset of always learn about our code about our users about new ways to thrive in in the market.
00:06:48 [W] However, in order to embrace that mindset with peace of mind, this is an important piece that's important and that's analytics now make no mistake continues experimentation is at its core
00:07:00 [W] And a comparative analytics problem. It's all about it's about comparing competing versions of your code.
00:07:10 [W] It's about comparing your Canary with your current version in the community observe that most of the discussion around these topics tend to revolve around the mechanisms the basic low
00:07:23 [W] Isn't that enable these techniques people say, oh, here's the solution for you to split traffic across different versions, hence. You can do a canary release or it is a solution for you to limit the users that we
00:07:38 [W] An experimental version here so you can do A/B Testing, but that's not enough.
00:07:43 [W] In order for you to be able to trust the assessments decisions that can be made with these experimentation.
00:07:57 [W] You need strong Analytics.
00:07:58 [W] In iterate, our goal is to provide a solution that not only gives this the Nick uses. The mechanism is the role of mechanisms that enable these techniques but also on
00:08:13 [W] Of them provide robust statistical inference machine learning based techniques to perform version assessment and traffic control version assessment to at every step of the way identify.
00:08:29 [W] version
00:08:31 [W] way that is as safe as possible.
00:08:47 [W] It is an overview of how iterate works.
00:08:52 [W] Now make no mistake continues experimentation is an analytics problem.
00:10:23 [W] And a comparative analysis problem.
00:10:28 [W] It's about comparing competing versions of the code.
00:10:32 [W] It's about identifying which of the version is the most promising and how which version compared with each other?
00:10:38 [W] Let's go.
00:10:41 [W] Make no mistake continues experimentation is an analytics problem at its core and a comparative analytics problem.
00:11:22 [W] It's about comparing competing versions of the code that are running simultaneously.
00:11:25 [W] I don't deserve that most of the discussion in the community around this topic tends to revolve around the low level mechanisms that enable can their releases an ABN testing. People say,
00:11:41 [W] Here's a way for us to split the traffic, hence. We can do we can support Canary releases more heat is a way for me to limit the users that will be able to see an experimental version of my
00:11:56 [W] Hence, I can do A/B Testing, but that's not enough a comprehensive solution needs to be able to support these low-level mechanisms. And at the same time have the ability to assess the versions
00:12:11 [W] In perform tasks Traffic Control in a way that it's intelligent.
00:12:17 [W] So with iterate we provide a statistically robust statistical inference techniques to perform version assessment and traffic control version assessment by revision
00:12:29 [W] some that comes with confidence in the probability that each version will be able to become the winner and traffic control that will allow iterates to continue the assess the competing versions
00:12:44 [W] How this verse will behave in a way that is as safe as possible.
00:12:49 [W] Here's an overview of how it works.
00:12:55 [W] Imagine that you have a number of versions deployed in your in your kubernative cluster. And each of those versions have some metrics associated with associated with them performance metrics correctness
00:13:08 [W] tricks perhaps even business metrics
00:13:11 [W] It's written. The lyrics is able to extract information about these methods from this versions and analyze how these versions are behaving and identify it in each point in time, which is the
00:13:26 [W] As promising version along with this version assassinated analytics also provide.
00:13:32 [W] Verse can be continuously analyzed in a way that is safe in a way that allows you to rate to continue making assessments going forward.
00:13:44 [W] It reads controller is a kubenetes controller that periodically get assessments from a threat analytics with the frequency that is specified by the user and applies that traffic control
00:14:00 [W] Nation, and we continue in this feedback loop. Whereas where Matrix are flowing from the versions into the Prometheus a Time series data base and it'll control gets assassins from analytics and songs of war and this proceeds until
00:14:16 [W] Virgil can declare a winner welcome to the experiment to duration finishes with each version assessment.
00:14:22 [W] There is a confidence associated in that decision.
00:14:24 [W] So we treat makes a decision makes recommendation and gives you how confident it is in that decision.
00:14:38 [W] It supports different traffic controllers challenges Progressive is probably what users will want for pretty much all the use cases.
00:14:43 [W] on top of the is smart traffic control users can specify some saved filters to for instance cut off traffic.
00:14:49 [W] And failure to cap the maximum amount of increment from one iteration of the experiment to the next provide a ways to limit the users that can see the key did the experiment traffic Etc another
00:15:01 [W] component of iterates worth mentioning is what we call iterate Trend and the idea of this component is that it analyzes how a particular service is evolving over time as different versions are rolled forward and it tries to uncover trends that
00:15:16 [W] Two developers might not be aware of.
00:15:19 [W] Now, let's quickly go over the types of experiments or continues experimentation that iterate enables.
00:15:31 [W] The simplest simplest case is performance tasks. In this case. There is only one version involved the idea is to assess the behavior of that version against success criteria that the user decides to use in this particular example, for instance.
00:15:43 [W] I'm saying that this service in order to be to pass the performance test. It needs to have a militancy below a hundred twenty milliseconds and an error rate of at most 0.0001
00:15:58 [W] Which means like four nines of availability. Typically this is done in a test or that environment, but it can also be done in production team.
00:16:05 [W] Canary leaves of course is another type of experiment in this case.
00:16:11 [W] There are two versions involved the baseline or the current version and a candidate. The vertex is being assessed. The idea here is to check if the canary is meeting performance criteria or ear.
00:16:26 [W] Progeria in this particular case
00:16:31 [W] relative criteria might be important for instance.
00:16:39 [W] I might want to make a determination of the be acceptable behavior of a canary with respect to the Baseline.
00:16:50 [W] So for instance, I'm showing you here an example that I would tolerate for the canary and iterate that is one point zero five dead of the Baseline meaning. I am tolerate a 5% degradation in the error rate if you will.
00:16:58 [W] and of course you can releases there is a traffic control strategy that gets applied so that the traffic hopefully will gradually shift to the canary if things are going well and if the canary fails of course a rollback decision is made
00:17:14 [W] Yet another this experiment type is a b or ABN ABN testing in this case.
00:17:21 [W] There are two or more versions being compared.
00:17:29 [W] There's a baseline version in one or more candidates and the idea here typically is to maximize some type of reward metric that is usually associated with the business.
00:17:38 [W] At the same time do so in a way that service level objectives that you want to promise to your users are still being satisfied.
00:17:50 [W] So and I reward metric is declared as such in the experiment I specification and what it really does is will it will optimize for that metric in this particular case?
00:18:06 [W] I'm highlighting a metric that is named named books purchased and imagine if you are bookstore and you want to maximize each version that you're comparing you expect that the winning one will maximize number of books that gets purchased during
00:18:19 [W] And again, a traffic control strategy is applied in the idea is that it will gradually shift the traffic towards the winner version.
00:18:28 [W] Now let's I'm going to show you now a demo of an automated ABN testing using iterate in this demo.
00:18:46 [W] We are going to see three competing versions.
00:18:48 [W] The application is a simple application that is modeled after bookstore and we are going to maximize a business-oriented method similar to the one that I just showed us showed you in the previous slide that have mean number of books purchase while satisfying
00:19:02 [W] Performance criteria in error create criteria. So let's take a look at this demo now.
00:19:09 [W] No, let's do another demo.
00:25:07 [W] So this time we're going to show what we refer to as human in the loop experimentation in particular in this demo human-in-the-loop ABN testing the scenario in this demo is the same as before.
00:25:20 [W] Here's the difference in human in the loop.
00:25:25 [W] user is in control of the experiment instead of the trade controller automatically applying the recommended recommended traffic's planetscale.
00:25:31 [W] It by using the assessments coming from iterate analytics in this type of experimentation that decision is entirely up to the user the user seizing sites since the assessment and decide what to do with them.
00:25:46 [W] Idea in order to support human in the loop experimentation.
00:25:54 [W] We have integrated iterate with kui kui is a visual terminal that essentially is a hybrid between a command line interface and webinar face as you're going to see. So again same scenarios before three competing
00:26:08 [W] ABN test this time around as you will see the user is in control. Let's take a look at this demo now.
00:26:16 [W] All right, let's summarize what we have discussed.
00:31:55 [W] All right, let's summarize what we have discussed.
00:32:09 [W] So we showed you iterate.
00:32:09 [W] So we're is awareness for key problem in continues experimentation.
00:32:12 [W] And the goal of iterate is ready to unleash the power of continue acquaintance experimentation in kubenetes. And it does that through statistically robust techniques
00:32:16 [W] Even by Machine learning for version assessment in traffic control.
00:32:20 [W] This is a state-of-the-art techniques and we happen to have some scientific work that we've done to substantiate that it's rate currently supports, you know kubernative is to for control and and uses
00:32:34 [W] For metrics in addition to the integration with Hui that you show that I showed you in the previous demo it reaches also integrated with key Ally Kia is The Defector open source, ey for the Easter servicemeshcon have
00:32:49 [W] Need to get extension for Kelly that can be used to create experiments right there and observe the experiments as they run.
00:32:56 [W] Here's where to find us.
00:33:03 [W] You have a link to our iterates landing page and our GitHub organization.
00:33:06 [W] Thank you so much for your time.
00:33:06 [W] Hello everybody.
00:33:23 [W] I think you can see me here me alive now, so there's a little bit of a glitch in the pre-recorded video.
00:33:28 [W] So I hope you were able to follow everything.
00:33:31 [W] I'm happy to entertain any further questions that you might have and we can also continue the The Conversation Over swag in the hip bone to keep corn to - keep going - ci/cd sectional
00:33:43 [W] So we'll wait to see if there are more questions.
00:33:46 [W] Let's see.
00:34:08 [W] There's one question here that I I see so let's hit second.
00:34:08 [W] Okay.
00:34:09 [W] Okay. I can see some questions.
00:34:10 [W] You don't have an answer yet.
00:34:11 [W] Let me let me try to answer that.
00:34:17 [W] Next question I have anniversary at so let me try to address them now live.
00:34:21 [W] Does this follow SMI Strife explicit definitions of canary meet cetera.
00:34:32 [W] So we use so we have our own crd that we Define that because of the kind of experiment and we use issues control plane to control the traffic across multiple versions.
00:34:39 [W] So, so again, we defined our CRT then we rely on issue for control.
00:34:43 [W] So let's see. The other question can iterate deal with removing the unsuccessful candidates to ployment for kubernative.
00:34:58 [W] So yes, there is one option in our CRT that allow you to basically clean up after the experiment is done in which case it would remove the versions to which traffic is no longer sent after the experiment.
00:35:06 [W] Another question is it's possible to have a session user is thickness in the experiment.
00:35:21 [W] So this is considered for tamino to to how it performs an experiment.
00:35:29 [W] Although for for can be a be a be type of experiment CBN type of experiments you can kind of limit the universe of users to which the traffic
00:35:36 [W] it is going to be kind of getting the will participate in the expense but sessions thickness is kind of orthogonal to to the operations of iterate.
00:35:48 [W] Let's see. There are more questions are the statistics also available using could control and similar tools.
00:36:00 [W] So there are some statistics that are associated with the custom resource that user creates for the experiment.
00:36:13 [W] So if you kind of look at all the details of the of the cri-o some some information some information there and do the experiment of course there Richard
00:36:20 [W] Information there and we kind of omit every time there's a decision made there's a new version assessment with the new confidence.
00:36:31 [W] We omit the kubernative nth doing that so we can also see through that anyone is using if a production did we have some proof of concept engagements with some some users and we're
00:36:44 [W] What kinds of Trends can fins module detect? So right now we can basically we rely on the metrics that it comes out of the box with iterate procedure plus realization Matrix.
00:37:01 [W] You can see if there are spikes anomalies in those in those metrics right now.
00:37:12 [W] So if I see something is deviating from the normal and we did we did some some as part of the iterative.
00:37:14 [W] Ella project umbrella. We did some work on kind of Time series analysis.
00:37:27 [W] We didn't bring the but we didn't bring those into the trends module yet.
00:37:30 [W] So the trends module is is pretty new so actively developing it.
00:37:32 [W] How many experiments is possible to run the same time?
00:37:38 [W] The limit is it depends on it?
00:37:45 [W] There's no limitation need to rate per se limiting them of experiments that can happen at the same time.
00:37:48 [W] It's it boils down to your kubernative cluster in terms of resources Etc.
00:37:56 [W] So I think we are running out of time.
00:37:59 [W] I will be engaging with you folks in our in the to - keep going - ci/cd.
00:38:03 [W] These like channel to answer further questions and engage with you further.
00:38:10 [W] Thanks for watching.
00:38:11 [W] Thanks for engaging.
