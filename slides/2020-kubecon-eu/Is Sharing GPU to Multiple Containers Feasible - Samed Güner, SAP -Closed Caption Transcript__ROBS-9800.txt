Is Sharing GPU to Multiple Containers Feasible?: ROBS-9800 - events@cncf.io - Tuesday, August 18, 2020 7:42 AM - 82 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:14 [W] Hi all thanks for joining this session in this session. I will try to give you an answer on a very interesting question for machine learning workloads running on top of kubernative Sands consuming gpus this question also gained huge traction and attention by with kubernative Community
00:02:34 [W] So we at a saved artificial intelligence.
00:02:40 [W] We're also very interested in finding an answer in this session.
00:02:43 [W] I will not only give you an introduction of a provisioning of gpus verb, but also share our findings implementation of look how you could get started already today in answering question whether sharing GPU to multiple containers is visible.
00:02:57 [W] So but first a little bit about myself, I am currently working as developer at sap artificial intelligence mainly on infrastructure related topics for machine learning and continuous delivery.
00:03:11 [W] I have been actively contributing to community projects of terraform and Cloud Foundry and the most important I combine the Two Worlds of Germany and turkey with the German be Beyond Church kept up by living in Germany and
00:03:22 [W] Part of the safety artificial intelligence mainly on infrastructure related topics for machine rolling and continuous delivery.
00:03:24 [W] I have been actively contributing to community projects of terraform and Cloud Foundry and the most important. I can combine the Two Worlds of Germany and turkey with the German be beer and church kept up by living in Germany and
00:03:25 [W] If you want to have a filter coffee with me after the session, feel free to VM me on Twitter or over the conference platform.
00:03:32 [W] I'm really looking forward to chat with you.
00:03:33 [W] So regarding open source sap is a platinum sponsor of a club knative Computing Foundation via sap are committed to contribute to open source.
00:03:44 [W] So we do this with projects such as garma Kuma or for kubernative world, but also for gevalt of the Java 7 machine or Luigi and feel free to check out this projects.
00:03:53 [W] And about artificial intelligence We Care a Lot at sap about artificial intelligent and work on different Business Services, which are later directed consumed on the AI platform or embedded into a large product portfolio or vision is
00:04:09 [W] Enterprise by embedding step-by-step easy to consume machine learning Services running on top of kubernative.
00:04:21 [W] One of the most consumed services or document information extraction can see tool right here. We can extract information from invoices this dead and many other services. We run a very serious number of models and production coordinators, which
00:04:31 [W] Production 22 which not only consume CPUs, but also gpus foreign Machinery inference.
00:04:37 [W] So for machine and workloads, its most of the time we are using gpus and keep use are very expensive workloads.
00:04:48 [W] This is also what we have seen and the production environment of ours.
00:04:53 [W] So the GPU is also a challenge meaning if I look into our production environments and seeing gpus only averaged utilization of 70 persons makes me really feel sad about it
00:05:04 [W] Meaning if I look into our production environments and seeing gpus only averaged utilization of 70% makes me really feel sad about it because why we are not able to fully utilize the GPU so I'm right side.
00:05:09 [W] I did some fictional calculation how much one can lose actually if we run about 500 a GPU models for an average price of three dollars and you can see that we are that one can actually use about 300,000.
00:05:23 [W] one can use actually if we run about 500 a GPU models for an average price of three dollars and you can see that we are that one can actually use about three hundred thousand dollars per month
00:05:26 [W] Dollars per month when running on gpus 500 models on an average CPU utilization of 70 person and this is a challenge. So at sap we asked ourselves how we can improve the situation can be
00:05:40 [W] Huge improve the situation.
00:05:43 [W] So we ask ourselves whether we sharing of GPU is in containers is feasible.
00:05:57 [W] So in general at sap, we are using gpus from Nvidia and I think in machine learning area this also white adopted talking about gpus I Ask of myself back then how the magic Works behind the see. How does
00:06:02 [W] You can still kubernative can consume it and in the end how its consumed five apart.
00:06:17 [W] There has been a great work done back in kubernative 1.10 with so-called device flattened where everyone and every window I can provide its own custom device.
00:06:23 [W] This could be a GPU and fpga or your own custom device such as Bitcoin miner could be consumed by containers the contributors put a great work here to love our boundaries of accessing custom devices.
00:06:30 [W] So let's check it out. Let's keep it General and imagine if you
00:06:33 [W] our on a worker note as you all know, each of our node is running kubelet or note agent, which is responsible for a variety of things such as spawning or containers and keeping them running and talking to work when it's Martha if we not attached or own custom
00:06:48 [W] And we want to make it not only consumable but scheduled through kubernative self.
00:06:56 [W] We need to install Windows device for them, which is actually a simple grpc server running on every note where we have and want to all of our custom device.
00:07:07 [W] Once the device clacking is supplied through usual deployment through kubernative.
00:07:12 [W] The defender device crackling does some initial Hardware initialization, for example finding the hardware loading material Etc once done. It reduces Itself by
00:07:19 [W] Bye it reduces Itself by calling rajasa and passes through resource identifier. We already have seen on our part specification consisting of a vendor name and the device name once done kubelet will register the given device plug in.
00:07:34 [W] It's which is subset by calling richest and passes through resource.
00:07:35 [W] identifier.
00:07:35 [W] We already have seen on our part specification consisting of a vendor name and the device name once done kubelet will register the given device plug in and offered resource and continue on requested more details on the number of available resources
00:07:40 [W] It's but offered resource and continue on requested more details on the number of available resources and also identifies by kubelet regularly checks the have setups of the device IDs. The given devices with Device IDs are used before
00:07:49 [W] and passed by kubelet kubeedge locate function to the device tekton during the allocate function the device tekton prepares for device marks it internally as use and returns back data, which is crucial for
00:08:05 [W] To yeah to Consumer device correctly.
00:08:14 [W] I'm Sookie function to be implemented in a device plugin are the functions allocate register and listen watch.
00:08:21 [W] So let's have a look on what happens behind the scenes once I deploy a machine learning model to kubernative switch request an Nvidia GPU in the same line diagram. You can see to the most right or GPU to the left
00:08:32 [W] Plenty of machine learning model to kubernative switch request an Nvidia GPU in the same line diagram.
00:08:33 [W] You can see to the most right or GPU to the left most right Bible device pregnant, then our workers and our Master nodes in the first stage as discussed before or device tekton does some - its relation by
00:08:43 [W] Second then over close and or Master node in the first stage as discussed before or device tekton does some - its relation by Will scouring the gpus which are attached to a note once done. It reaches itself in the second stage at
00:08:49 [W] Chart touch to a note once done it reaches itself in the second state at kubelet and interestingly.
00:08:53 [W] is that that kubelet reports back to the master note the availability of the new device as just advertised by with Device plugging. If not already done by other device platforms, which are running on other notes upon that kubelet gets data about where available
00:09:05 [W] Also available devices on the note from a device tagging the third stage the device plug and makes all gpus unique and returns back their IDs as kubernative snow aware of the number of available gpus
00:09:19 [W] It reports to count back to the master because of Master needs to know what is available to schedule the tons of from mother a which are just submitting as the user once kubelet receives a request to schedule the model. It allocates
00:09:35 [W] Beep you given that we use are requested GPU and then the Nvidia device plucking returns environment variable and media visible device Jeep you before GQ ID, which is later then passed to The Container which can directly access the
00:09:51 [W] The Thin Blue container and at this point where it gets very interesting for implementation and actually this was the whole Magic provisioning a GPU back then we actually shared
00:10:07 [W] It's findings and implementation approaches with the community in many comments.
00:10:16 [W] Thanks again here for Thomas Young brute working with me on this topic back. Then the identified free possible variants on sharing a GPU the first which was already in place.
00:10:25 [W] It's the model stuffing on application logic where we bake multiple models into one Docker image and use tensorflow serving to switch between these this very static approach does not offer any cgroup for each model, but the
00:10:37 [W] role model and in addition to that extended features such as rate-limiting per model is not possible.
00:10:47 [W] The third variant has been the node selected hacking and you hear it. It is hacking. We do not use with the West f****** at all.
00:10:55 [W] We give the container but privileged mode which you should never ever do and let the containers use the GPS on their own the drawback of this approach is that kubernative does not really know that there are gpus and cannot respect them during the scheduling and a very large
00:11:06 [W] Combine its might schedule pots on your note, even though you do not have any GPS left in the end.
00:11:16 [W] We have decided for variant to well.
00:11:18 [W] that's not really a decision but more exploration and the the help initial help of some developers from the Nvidia device clicking the variant to tries to solve this issue in a more knative Way by extending with Device plugged in by so-called vgp use to
00:11:31 [W] You can sound very terrific but we are not so let's check out how we extended the device back into search share some gpus.
00:11:44 [W] So the problem in general was that wants to GPU IDs allocated.
00:11:48 [W] It is marked as used by with Device talking and so no more device art with highs from a kubelet to the kubernative Massa. In other words. Any deployments requesting GPU will be pushed back as R across requirements cannot be met in addition to that
00:12:00 [W] Select the hacking and model stuffing that the cheap you can actually handle multiple processes accessing directly with to GPU. So we started thinking about how we can do that why we shouldn't advertise more gpus if there is even only one
00:12:16 [W] It was that simple we generate using the GPU ID of a physical GPU and number of literals Accused by simply adding a suffix afterwards.
00:12:29 [W] We assign those VG poussin the device package to the physical one and calculate that we all know gpus of the GPU ID. So be like once we get calls for allocate function. We simply remap which OG Q ID to physical one
00:12:41 [W] Five a simple songs. We have quite a few trade-offs of this approach it already starts with a problem how many we gpus can be or surely provision per GPU.
00:12:57 [W] How do we set the limit and are there any boundaries with regards to recall and GPU memory?
00:13:01 [W] So are we ask ourselves following questions?
00:13:04 [W] How many models can be fit on the Nvidia?
00:13:04 [W] k8s. We are running on a machine an inference models on the Kata.
00:13:10 [W] does the whole system behave and what arbitrators in doing? So
00:13:12 [W] Imitations of course, so we did our experiments and collect the data talking about collecting data.
00:13:27 [W] The device clucking is nothing more than a part except during the local devices to advertise resources to grow beneath has meaning that the very same strategy of parts for monitoring and collecting data can be used here too.
00:13:32 [W] extended with Nvidia device plug in with internal users for NVIDIA management library to collect additional data from the devices and the
00:13:44 [W] Envy mlibrary actually offers low level access ebsco see binding. So in the end we created a few graph and a dashboards with the data we got from problem toys to track different values such as some of which will diffuse we consume cheap you per ampere
00:13:57 [W] And we GIF utilization.
00:14:01 [W] So we have collected data.
00:14:02 [W] We have a proper question.
00:14:05 [W] And so we did some experiments to answer those.
00:14:11 [W] So let's check out our experiments and the first experiment we do Bevelle noun and body simulation why let us not really comparable to machine learning inference still gives us some insight on how the work might be distributed internally and amazingly to left we can see
00:14:21 [W] Flops per second are distributed evenly very pot. We just added this task scheduling is also confirmed by our data to the right where the amount of time for an experiment to finish grows linearly with the number of Parts accessing the
00:14:36 [W] Keep in mind but we did not limit with gpgpu nóva GP utilization.
00:14:49 [W] I hadn't have given every part of fair share of the CPU from a note Pete Wicks large at AWS.
00:14:53 [W] We were running on why the experiment is kind of comparable to machine learning training.
00:14:58 [W] We were more interested in Sharing deep use in the case of machine on inference.
00:15:03 [W] Actually. It includes more factors such as Network throughput and latency. So let's check out those into experiments we have done.
00:15:07 [W] In general forget to say this.
00:15:17 [W] We are running all of our experiments on a P2 x large instance which has 4 cores and 1nv zettaset k8e attached with tough. Go by tram as Ben said to me free model was used back then for a large amount of oil production workload.
00:15:27 [W] We use it to thrive or experiments in the first very influence experience. We spawn and total up to 12 V gpus and sent in total of 10,000 requests per pot and limited each pot with 350 amps if you
00:15:37 [W] He will see later.
00:15:40 [W] how important in our implementation of the limiting of CPU actually is a disclaimer at this point. This experiment is done using a sequential request pattern meaning each model handles at most one request at the time given the data to
00:15:54 [W] This running on 12 we GPS. We have a P99 in response time below 500 milliseconds.
00:16:06 [W] You might be asking how we manage to fit 12 Inception models onto one k8s in yourself there my drum While most of the time a model one silicate with full view for that matter.
00:16:15 [W] We use a very nice functionality of tons of rock serving and in terms of who's serving one can specify using TF dot tipu options for fraction of the pyramid model is limited to
00:16:25 [W] given our data to the right you can see that the stuff models running on one k8s assign each model only 5% of available VM which corresponds a proxy 600 megabytes for each model another approach was to offer
00:16:40 [W] He's full to get the most of the GPU. But actually we were very keen about finding the upper limit. How many models can be run before crashing on a 1K it so we ask ourselves. Can we go actually deeper and hell? Yeah,
00:16:55 [W] All three free mother requested most 228 megabytes per model.
00:17:07 [W] So it's fewer typically possible to stack up to 50 models on one GPU due to the CPU limitation on ocp to instance.
00:17:14 [W] We could run this experiment only for 30 models with a CPU limit of 100 m and assigning the three persons of the virtual realm. We are having on a GPU keep in mind that this is the sequential request pattern nevertheless furtive models or
00:17:24 [W] Response time version only by facto 10 compared to a single deployment in all opinion. This is an amazing way to keep models running which do not have large spikes in the amount of incoming requests
00:17:40 [W] Interesting results, but we also have related. Of course the parallel requests pattern and keep in mind you are still on our petabyte-scale Inception we free but this time we let the model process 10 requests at the time.
00:17:56 [W] We did not enable matching here to establish a baseline where we could later on comparable fetching.
00:18:05 [W] Let me start for experiment for one deployment to establish a baseline. We were able to observe that increasing or decreasing the CPU limit with also
00:18:11 [W] Limit or GPU to ization. So simply stated we introduce an artificial bottleneck here for the GPU to ization.
00:18:20 [W] This has a huge impact as we can avoid the CPU limit and over commitment of the GPU in general.
00:18:30 [W] We have observed that an over commitment will lead to large increase in latency.
00:18:36 [W] So we equation is really simple in that regard. If you increase your throughput with GPU has more work to do so given the limit of 350 amp of a CPU and
00:18:44 [W] Person vram we were able to have 10.7 queries per second at the GPU to ization of 50% for one deployment for two deployments.
00:18:53 [W] We were successfully able to fully utilize a GPU at about 98% and achieved more than double of a coup PS4 assimilating enabling batching be could even decrease the latency and increase
00:19:06 [W] Two times in exchange for six times more vram given all these data we ask ourselves again.
00:19:18 [W] It's sharing two GPU to multiple containers really feasible.
00:19:25 [W] Well in Germany Germans after seeing all of these results would say Yang, which is a slang for.
00:19:27 [W] Yes, and no or in other words in our world.
00:19:32 [W] Yes, you can share with you a few but we have trade-offs and accept at least for this internal implementation some limitations, so
00:19:38 [W] So after I having so many figures and numbers you are asking yourself. Okay, what happened now here? So what does it all mean for implementation typically in harness here all solutions far away from product in it - but we were still
00:19:51 [W] GPU the very promising results, but for that result, we really tried hard we had to do a lot of runs find out how much we're MB actually need and what are the minimum limits are and what is the relationship between Jeep utilization
00:20:07 [W] Actually need and what are the minimum limits are and what is the relationship between GP utilization CPU limitation and throughput and latency nevertheless. We are able to save up to 30 times of our cast for Inception briefie
00:20:15 [W] And throughput and latency nevertheless. We are able to save up to 30 times of our cast for Inception reefy models running on top of k8s for machine learning inference beside the lock into tens of serving for inference do
00:20:23 [W] Tootie to limit which will Ram from a device button we had to misuse kubernative CPU limits to artificially limit order fuel station while this is very happy and not really kubernative native.
00:20:38 [W] We were still able to deploy multiple models with very similar Layton sees by fully utilizing or GPU and to remember the calculation Jeep utilization was a problem.
00:20:51 [W] with that we actually can fully utilize so GPU so we have saved some money, too.
00:20:53 [W] Buy us some things with our 300,000 K.
00:21:03 [W] So well, there are all first other limitations with side the control future RAM and CPU utilization.
00:21:12 [W] The first and foremost is that we do not really have a clue about what happens on the GPU level. Then we run multiple processes.
00:21:16 [W] We assume that there is some sort of fair scheduling mechanism, which we could find out by our experiments, but still given the fact Nvidia cannot guarantee isolation on Hardware level.
00:21:23 [W] Does not make sense for us to run deep you sharing in all multi-tenancy set up moreover.
00:21:37 [W] We are simply not able to specify the vram and GPU costs which makes it very hard to use knative kubernative scheduling to schedule pots speaking about scheduling. We do our scheduling during bible number of which will gpus it is not kubernative
00:21:44 [W] This details of resources such as cars and we're am it's only using we gpus where are we are responsible on the siding.
00:22:00 [W] Yeah how we map or GPU to which are approvals and another very simple problem is the resource fragmentation. Even if we have deep you sharing where we have isolation and also may be possibility to specify limits on vram GPU
00:22:10 [W] We are still unable to buildpacks or models correctly because kubelet returns to a pi server aggregated information and it does not see the GPU as a first class citizen so it can happen that during the scheduling of a
00:22:25 [W] So it can happen that during the scheduling of a workload is if you might get overcommitted if there are multiple gpus running on the same note for this problem.
00:22:35 [W] We would have needed a local de Tierra scheduling mechanism implemented with Nvidia device Dragon which avoids for my commitment of gpus.
00:22:46 [W] So now what do we really need to run GPU sharing in production? We need isolation on GPU level with GPS device driver should offer an API tufin.
00:22:51 [W] To specify we're am and GPU cost constraints per process like like we had in cgroup. We see we already seen CPU and memory at the current point of time and media doesn't spot any solution with regard to that but we
00:23:06 [W] Epi to specify we're am and GPU cost constraints per process like like we had in cgroup and we see we already seen CPU and memory at the current point of time and media doesn't spot any solution with
00:23:08 [W] We Source the fragmentation of equipment we need to devise second which is able to manage sharing of multiple gpus on one note by being local tier where and a widened over commitment of gpus and another chapter.
00:23:22 [W] We did not really discuss at all is the initialization overhead when we spawn a vgpu, but also how fast can device packing switch processes which are using the GPU and of course last but not least we need
00:23:34 [W] please we need those limits to actually use knative kubernative scheduling which where people can just specify in their pot and yeah, which limits we want to have on the machine learning model and I have to tell you that
00:23:48 [W] Amazing since our initial proposal on the issue. There have been four projects trying to solve this issue where even one very promising was released after the original date of the session.
00:24:01 [W] So let's check them out.
00:24:06 [W] The first project is diplomatic. It uses the same approach like we do and let's see the user specify the number of bgp use to propose to a user using tangible serving and secure limits one could realize the very same set up like we did
00:24:16 [W] The second project is Fontaine content and called GPU manager. It internally uses another project developed by Tank and called the Kudo controller which is a wraparound and media device Library who can up on Kudo Accords and for syseleven to right you can see that you are
00:24:32 [W] Able to specify limits according to the GPU moabites archives at the custom scheduler extended to extend with default kubernative scheduler for GQ admission at Lea elasticity of a newly released work capture, which also enforces
00:24:47 [W] yeah, which also enforces hard to isolation by intercepting who decodes it GPU and implementing and own scheduling mechanism by GQ manager requests the extent of community scheduler we seed from stankin to share
00:25:00 [W] Tula and solves a lot of aforementioned problems with custom resources.
00:25:13 [W] We have also written a paper about the approach shoving their approach and their results. I would recommend you to check it out.
00:25:14 [W] It gives a very comprehensive insight into the world of containers taking scheduling gpus and GPU sharing if you are planning to contribute or Implement at your company GPU sharing should definitely check out those visits projects and contribute to them at this point.
00:25:28 [W] want to give a big thanks to all of these people.
00:25:31 [W] Abusing to wear ecosystem.
00:25:35 [W] We already have and the said we are actually coming to the end of the session.
00:25:39 [W] Thanks for having me here.
00:25:44 [W] Feel free to contact me on guitar LinkedIn or Twitter and the slice some iguana. I'm happy to answer your question.
00:25:47 [W] Call Em. So thanks for joining the session.
00:25:57 [W] I'm not going to answer to question.
00:25:59 [W] I got here. So the first question is from Josh.
00:26:06 [W] He's asking based on the fact that Nvidia can't see a way to get cgroup equivalents GPS into white glue this remaining unsolved.
00:26:12 [W] Keep this very Niche and outside of Upstream.
00:26:13 [W] So this has something to do with the wise or with the end. We did the vice. We actually use so.
00:26:20 [W] All devices which are allowing up to seven gpus or seven workloads on one one GPU. So we have in our experiments we have used k8s to do this and the k8s at the current state does not support
00:26:38 [W] I'm not believing that k8s and also be hundreds are going to support GPU soon or at any time.
00:26:48 [W] There are four we have also these great works.
00:26:50 [W] I've just shown you for example from Cube share which are trying to solve this by implementing and own.
00:26:58 [W] Yeah scheduling mechanism on the lower level.
00:27:01 [W] Let's go.
00:27:03 [W] I hope this answers would question.
00:27:05 [W] go on on the next question from claim or is it usable on gke?
00:27:08 [W] There is already kubelet device tekton them as if I pass it. So the device talking is just a demon sack you install on the notes you which are actually having Nvidia gpus and as far as I know you
00:27:23 [W] Which are actually having Nvidia gpus and as far as I know, you can also uninstall existing device platform. This shouldn't be a problem.
00:27:33 [W] So, yes, you could use any of Open Source projects to install magic a cluster with GPU sharing. Another question is by job.
00:27:42 [W] I always thought that context switching between processes slow down GPU processes from your experiment. It seems like you've just proven this does have
00:27:48 [W] have anything to do with the software you are using for serving or this context switching not not big of a problem anymore.
00:27:57 [W] I asked to be honest here.
00:27:58 [W] So because we are tried to solve the problem on a very high level on the device Platinum level.
00:28:07 [W] We do not really know what happens in on the GPU level.
00:28:12 [W] I strongly assume that there's some kind of thrashing behavior that process are stopped and then new processes takeover offer.
00:28:20 [W] Available capacity of the GPU but to be honest.
00:28:25 [W] Yeah, I do not really know about it's really amazing to see that context switching is actually not a big deal.
00:28:30 [W] Yes.
00:28:32 [W] So I hope that answers your question.
00:28:35 [W] There's another question.
00:28:44 [W] What are your thoughts about on GPU sharing for short-lived MLG jobs, but you think it's better to get it done faster, or have a higher bandwidth.
00:28:49 [W] We do not really know what happens in on the GPU level.
00:28:54 [W] I strongly assume that there are some kind of thrashing behavior that process are stopped and then new processes takeover offer available capacity of the GPU but to be
00:28:55 [W] machine learning experiments for example or for training I would suggest that get it done faster would be a better thing than deploy multiple machine learning training jobs on to the same GPU
00:29:09 [W] Of course this in changes from use case to use case but in general my advice would be to get it done fast.
00:29:18 [W] You can also if you see that model doesn't converge in a given time period you can also cast and deploy the next job and I cannot see any other questions here all other question. I have put an answer
00:29:32 [W] So I think I can end this session you event. Thanks very much for joining again the session it was great to be here in the spiritual event, and have a nice virtual event and upcoming days.
00:29:47 [W] See you.
00:29:47 [W] Bye.
00:29:47 [W] Bye.
00:29:48 [W] Days, see you bye.
00:31:03 [W] Bye.
