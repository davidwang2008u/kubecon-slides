Multicloud Vitess over Network Service Mesh: HXVB-4918 - events@cncf.io - Wednesday, August 19, 2020 10:47 AM - 1190 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:07:32 [W] Hi.
00:07:35 [W] Hi, welcome to Kupa Kai on this is the multi-cloud Vitesse over Network servicemeshcon won't talk between Network servicemeshcon the test.
00:08:51 [W] And I'm John Watson slight reliability engineer for the planetscale DB with planetscale.
00:09:09 [W] So today we're going to go through the integration of the test with network servicemeshcon virtual layer 3 deployment, so we're going to start off by highlighting the pertinent use cases for why you would
00:09:26 [W] Integrate with these to complete these two projects and then we're going to just describe the Tessa's architecture so we can understand what components are involved when we do this deployment and then we're going
00:09:42 [W] how some of the details for Network servicemeshcon layer 3 inter-domain and how it works and some of the benefits and then we'll go into the integration between the test deployment on a
00:09:57 [W] Bold system.
00:09:59 [W] or multi-cloud system with with nsmcon domain
00:10:05 [W] and will do a demonstration.
00:10:07 [W] The test is core philosophy is to facilitate the scaling of applications by managing the complexity of scaling databases with little to no interference with application development.
00:10:26 [W] For example, the test multi-cloud can be used to Ephesus effortlessly provide a modernist interface to your database in any cloud provider. Thus preventing vendor lock-in while increasing the availability and durability of the database to
00:10:38 [W] Protest provides durability by replicating shards across cells and for replication to work for test needs to be able to securely communicate across availability zones regions and or cloudbees vide errs.
00:10:53 [W] Another vitess multi-cloud use case is able to comply with data location regulations without react architecting your application.
00:11:11 [W] The test sharding will handle routing data and traffic to the appropriate jurisdiction with little to no changes to your application.
00:11:15 [W] Mitosis is already built with availability in mind.
00:11:24 [W] Can while increasing the availability and durability of the database to?
00:11:28 [W] Protest provides durability by replicating shards across cells and for replication to work for test needs to be able to securely communicate across availability zones regions and or Cloud providers.
00:11:30 [W] Another vitess multi-cloud use case is able to comply with data location regulations without react architecting your application.
00:11:33 [W] The test sharding will handle routing data and traffic to the appropriate jurisdiction with little to no changes to your application.
00:11:34 [W] Mitosis is already built with availability in mind.
00:11:37 [W] By replicating charge across failure to Mains the test multi-cloud takes this concept and applies it to whole Cloud providers.
00:11:38 [W] In fattest a failed main is mapped to a cell each cell usually contains a set of tablets BT Gates and application servers.
00:11:44 [W] With multiple cells the local VT gate will handle routing traffic to the designating charge and the appropriate cell and they tablets will handle replicating The Shard data across cells.
00:12:02 [W] The test multi-cloud is cells across hard providers instead of just regions or availability zones.
00:12:15 [W] The test relies on the networking provided by the infrastructure.
00:12:26 [W] It's deployed on in the case of communities.
00:12:28 [W] That's the cni-genie cloud provider and it's poddisruptionbudgets working.
00:12:34 [W] We need to securely connect multiple connectivity bands together.
00:12:38 [W] Unfortunately implying an organization's networking and security requirements usually entails toil since it requires using a cloud specific apis and / kublr 9 is cluster Network policies to allow access to only specific workloads.
00:12:48 [W] Multi-cloud takes this concept and applies it to whole Cloud providers.
00:12:58 [W] In fattest it fails. Our main is mapped to a cell each cell usually contains a set of tablets BT Gates and application servers.
00:12:59 [W] With multiple cells the local VT gate will handle routing traffic to the designating charge and the appropriate cell and they tablets will handle replicating The Shard data across cells.
00:13:00 [W] The test multi-cloud is selves across the harbor by ders instead of just regions or availability zones.
00:13:02 [W] The test relies on the networking provided by the infrastructure.
00:13:03 [W] It's deployed on in the case of crew and he's that's the cni-genie cloud provider and it's poddisruptionbudgets working.
00:13:04 [W] We need to securely connect multiple connectivity bands together.
00:13:05 [W] Unfortunately implying organizations networking and security requirements usually entails toil since it requires using a cloud specific apis and / kubernative cluster Network policies to allow access to only specific workloads.
00:13:08 [W] Okay, as John was saying in the last section of about the takeaways from implications of multi-cloud networking the toil that's involved in clouds provider specific
00:13:13 [W] working the toil that's involved in clouds provider specific network connectivity between other clouds and being managed in bespoke ways
00:13:15 [W] Something that we're trying to get away from and we're trying to create the that's a kind of a goal of the network servicemeshcon.
00:13:44 [W] So this brings up what is nsmcon the so nsmcon?
00:14:15 [W] functions so as a network function provider, you can perform you can you can Implement a network service endpoint and have a administrator
00:14:31 [W] So this brings up, what is nsmcon Ali so nsmcon?
00:14:34 [W] As a network function provider you can perform you can you can Implement a network service endpoint and have a administrator
00:14:35 [W] Network service to any of the client workloads in their domain there.
00:14:39 [W] nsmcon loin.
00:15:01 [W] to handle that it's uses it in it consults a network service registry to just to find any endpoints that are registered for the that type of network service and
00:15:17 [W] each network service in point that it got back from the registry to to connect the client to and then it it's forwards the request to the network service endpoint that it shows and
00:15:32 [W] It's forwards the request to the network service endpoint that it shows and if every minute that negotiates the connection parameters and does the actual connection setup to the client.
00:15:42 [W] So that's the the gist of The nsmcon L4 connection managing connections.
00:15:47 [W] So you applied to a kubernative environment, which we refer to as a single kubernative environment is a what we refer to as a domain in nsmcon.
00:16:17 [W] Then endpoints on other nodes.
00:16:21 [W] So the same work same basic flow as the previous slide, but it's triggered by nsmcon admission controller that pays attention to annotations on pods or deployments and
00:16:35 [W] And points on other nodes.
00:16:36 [W] So the same work same basic flow as the previous slide, but it's triggered by nsmcon admission controller that pays attention to annotations on pods or deployments and
00:16:37 [W] That causes a container to be added to that clients deployment and that is what is doing the actual request initiation to the network service meshmark.
00:17:05 [W] Is local to that NSC and that nsmcon injure will then forward the request to the actual NSC.
00:17:17 [W] So there's there's kind of layers of connection parameters being also exchanged here.
00:17:24 [W] The clients are saying what type of connection that they would be able to handle and the NSA. Anna jurors are saying are indicating what kind of connections that can go between the nodes and like a vehicle and tunnel.
00:17:33 [W] To the nsmcon adjourn that's local to that NSC and that nsmcon injure will then forward the request to the actual NSC.
00:17:35 [W] So there's there's kind of layers of connection parameters being also exchanged here.
00:17:36 [W] The clients are saying what type of connection that they would be able to handle and the MS managers are saying are indicating what kind of connections that can go between the nodes and like a vehicle and
00:17:37 [W] And tunnel or in the actual nsmcon injure on the Node to the MSC is also negotiating their side of the connection.
00:17:47 [W] So there there's a lot of connection parameter Exchange in this flow and that that's a lot of that provides a lot of power.
00:17:59 [W] can see there's no centralization here. It's just the central essential component is the network service registry and that is really just pointing at
00:18:03 [W] available network service endpoints
00:18:06 [W] So when we go inter-domain, you can see how this model extends because the same idea can be used to carry out the connection across domain boundaries or cluster boundaries.
00:18:24 [W] There's a lot of connection parameter Exchange in this flow. And that that's a lot of that provides a lot of power.
00:18:24 [W] You can see there's no centralization here. It's just the central essential component is the network service registry and that is really just pointing at available network service endpoints.
00:18:26 [W] So when we go inter-domain, you can see how this model extends because the same idea can be used to carry out the connection across domain boundaries or cluster boundaries.
00:18:28 [W] Between the nodes in different clusters is the is that is the difference of this environment versus the single cluster, but on a node level the
00:18:44 [W] In flow is the same and then between clusters the control plane flow is basically the same but the parameters in that connection message are and their mate and there are possibly are intermediaries
00:19:00 [W] get the control plane message to the actual other network service manager and the other cluster, but but all that logically is is just essentially like proxying
00:19:16 [W] Message to A to the to the right destination and then the the exchange of the actual parameters required for the layers of the connections to enable the connection between the actual.
00:19:32 [W] Client in one cluster to the the network service endpoint in another cluster.
00:19:38 [W] that's all figured out in the context of that control claim message exchange.
00:19:45 [W] it looks exactly the same from a from a flow pattern point of view and this in the details of that connection pipe are going to be different but the at the setup logical view, it's the same.
00:20:00 [W] and the other thing to take away here is the network service registry and inter-domain has to be able to know which end points are a part of the inter domain environment and
00:20:15 [W] Block as the network service registry, but that that also could be a distributed kind of registry as well.
00:20:24 [W] So in that in the virtual are three, what we're doing is we're implementing a different specific type of NSC and that NSE is functioning as a layer 3 Gateway router.
00:20:40 [W] So it's from the point of view of the client workloads pods, and it's connecting to the the pier its peer NSE s so the other types of the other
00:20:52 [W] Main environment it's connecting to those via nsmcon S using nsmcon and the other ones other peers and it's using so using nsmcon service registry for that purposes and then using nsmcon.
00:21:18 [W] So the model is is pretty in this pitch in this picture.
00:21:31 [W] We're trying to show the the same flow but with L3 and I see as the specific how it specifically works.
00:21:34 [W] So in the in the case of the VL three and a see we have a new notation the same model where we apply an annotation for a network service that a
00:21:49 [W] Corresponds to the actual connectivity domain network service of the network service.
00:21:59 [W] So in this case, it's the connectivity million called green and that that NSE that it's connecting to has a mapping of of that
00:22:08 [W] Connect to the network service registry for that connectivity domain. So that network service registry knows about the other NSE s in the connectivity domain and when that happens
00:22:24 [W] happens from the client the NSC will find the other NSE s that are in that connectivity domain and Trigger connection requests to form the actual peering between the other nscs so it
00:22:40 [W] Was the nsmcon fermentation to create the data playing connectivity between those NSE s across the cloud boundaries or cluster boundaries?
00:22:51 [W] So when they're another client comes up in the cluster to this in this case in the flow that we just described that actual connectivity domain is already up and all that is happening is that client is
00:23:07 [W] Getting connected to the NSC and then it is now a part of the L3 Network that makes up that connectivity domain so the client and cluster one can reach the client and cluster to and vice versa as if they were
00:23:23 [W] So the client in cluster one can reach the client and cluster to and vice versa as if they were, you know in the same cluster or you know, just via the same layer just a single layer 3 Network they things out
00:23:32 [W] Layer 3 Network. They things out pods outside of this that that are in those clusters that are not part of this connectivity domain cannot reach things across the cluster boundary because they are not enabled
00:23:44 [W] acted to that and I see
00:23:47 [W] so connectivity domains are a totally an opt-in approach to to connect to joining to multi-cloud connectivity.
00:24:02 [W] Only the workloads that are a part of the connectivity to a domain are a part of that.
00:24:04 [W] So there's no need for extra multi inter-cluster Network policy.
00:24:16 [W] So this is now we're going to transition into the how Vitesse and nsmcon integrated with
00:24:18 [W] On the virtual layer 3 and ICS. So like in the previous slide with about the the example for the nscs we have now of the test deployment that's that's in the connectivity domain that is between
00:24:33 [W] Stirs and the vitess deployment has multiple pods and mold.
00:24:44 [W] There's multiple reasons for those pods to be able to communicate with the other clusters instances of different pods.
00:24:56 [W] So all of those in our in the deployment are all the content in these the tests deployments are a part of the connectivity domain.
00:24:59 [W] And then the connectivity domain also has some additional services like DNS that is DNS only for the workloads that are connected in that connectivity domain. So there's a DNS.
00:25:15 [W] For services for those service workloads for the workloads that are in that connectivity to me.
00:25:24 [W] So this is a kind of a breakout of which the pods are deployed where in this example the planetscale operator deploys VT G AV 2 tablet in both
00:25:40 [W] But it at CD in a single cluster and the VT Gates can reach either in EVT tablet. So that that kind of fulfills your data location use case since you can
00:25:57 [W] Shards or tables according to your regulations location regulations.
00:26:11 [W] And then the VT tablets can reach each other across the connectivity domain and that would solve your data replication multi-site data replication use cases.
00:26:20 [W] So both of these use cases are solved just by the single type of deployment.
00:26:21 [W] Okay, so we're going to do the an actual demo where we show the pre what deployment my quote the previous slide show.
00:26:32 [W] So the details of this demo are a to kubernative Cluster set up with nsmcon the NSC control services that I showed in the previous slide where with for DNS and I Pam.
00:26:47 [W] In the registry are in that third cluster, so they're not going to be shown but there there there and then the actual demo flow we are going to set up the virtual layer 3
00:27:04 [W] each cluster and that's going to be a script that does that so it's going to be hard to follow but the gist is that it it installs the nscs and it has them registered as network service named vitess
00:27:20 [W] Registered as network service named vitess and and they're configured to be a part of the connectivity domain with the same name.
00:27:28 [W] So they're going to be configured to point back at those and I see services that are in the control of the other third cluster.
00:27:35 [W] Then we're going to Ploy the test positive both clusters as virtual L 3n s clients.
00:27:56 [W] We're going to be using the the test operator for kubernative to do the handle the to put to the test deployment because it not only automates the deployment of a test. It will automate the management of a test on communities as well.
00:28:04 [W] then we will use for tests to create a database table and Shard config that will illustrate the
00:28:13 [W] connectivity domains and then we'll show the shards are split between the clusters by use separate vitess cells and then we will show replication across this clusters by using
00:28:30 [W] Failover has a way of illustrating that there's inter-cluster Network provided through the virtual L3 connect a domain.
00:28:40 [W] So now we here we are with the demo and we are going to have an installer cluster or in the end Europe cluster in the Americas in each of those are going to be a not only of a test cell but a cluster and nsmcon.
00:29:08 [W] This will take a few seconds and then wall of text, but no it comes through.
00:29:13 [W] And there we go.
00:29:18 [W] We now have nsmcon Ting in both clusters.
00:29:19 [W] And next up we are going to stall the test in the Americas region.
00:29:24 [W] This will be our primary region.
00:29:32 [W] So we are going to put our topology servers Running on Empty D. And in the Americas regions, we are using the vitess kubernative operator to handle all of this.
00:29:38 [W] We just feed it a cluster configuration and vitess and the operator then handles bringing up each of the STDs the tablets and the control plane as well and then follow that up with the BT Gates which provides
00:29:50 [W] Activity to not only this local cluster this local cell. It will also provide connectivity to the other cell.
00:29:59 [W] Seconds and then wall of text, but no it comes through.
00:30:01 [W] And there we go.
00:30:01 [W] We now have nsmcon Ting in both clusters.
00:30:01 [W] And next up we are going to stall the test in the Americas region.
00:30:02 [W] This will be our primary region.
00:30:03 [W] So we are going to put our topology servers Running on Empty D. And in the Americas regions, we are using the vitess kubernative operator to handle all of this.
00:30:04 [W] We just feed it a cluster configuration and vitess and the operator then handles bringing up each of the STDs the tablets and the control plane as well and then follow that up with the BT Gates which provides
00:30:07 [W] Activity to not only this local cluster this local cell. It will also provide connectivity to the other cell.
00:30:12 [W] Here we go with Europe.
00:30:13 [W] We told the operator to apply the nsmcon activator be an annotation to all pods in the vitess cluster. And since the operator is also managing the tablets with just a couple lines of code.
00:30:14 [W] It extends The annotation so nsmcon Chester's a service for each tablet so they can communicate across the clouds using the nsmcon.
00:30:20 [W] You too, man, and then now that all tablets are ready.
00:30:27 [W] That means they've are connected and there it has now chosen a master and the replicas are connected.
00:30:34 [W] in each region has both
00:30:37 [W] both things
00:30:38 [W] and
00:30:41 [W] we can see the nsmcon work services are not configured with a virtual L 3s.
00:30:47 [W] And here's our nsmcon points.
00:30:58 [W] As well as The annotation that shows us attaching the deployment to the connectivity domain.
00:31:07 [W] And same thing in the Europe cluster.
00:31:16 [W] with the same number of surface endpoints as well as the annotations that tells the
00:31:27 [W] And then now that all tablets are ready.
00:31:31 [W] That means they've are connected and there it has now chosen a master and the replicas are connected in each region has both
00:31:31 [W] both things
00:31:32 [W] and
00:31:32 [W] we can see the an assignment work services are not configured with a virtual L 3s.
00:31:32 [W] And here is our nsmcon points.
00:31:32 [W] As well as The annotation that shows us attaching the deployment to the connectivity domain.
00:31:34 [W] And same thing in the Europe cluster.
00:31:35 [W] with the same number of surface in points as well as the annotations that tells the
00:31:35 [W] injects the the the sidecar into our pods
00:31:36 [W] Here's our protest control panel. We have two shards in the user database - 40 where we have the Americas has the master and it represent Europe.
00:31:48 [W] As well as a second chart of everything greater than 40 is only in Europe because we will map All European all non America's users into the European cluster
00:32:05 [W] B-side car into our pods
00:32:06 [W] Here's our protest control panel. We have to shards in the user database - 40 where we have the Americas has the master and it represent Europe.
00:32:07 [W] As well as a second chart of everything greater than 40 is only in Europe because we will map All European all non America's users into the European cluster
00:32:08 [W] And we'll have a full name national Indian Country will use country as our way of routing our users. We tell the test to use a a the index based on the
00:32:27 [W] Country that is provided by the user.
00:32:30 [W] We have a static mapping in a Json file that is just says country name of a country to a sharding key and that charting key then is applies to the key space of being less than 40 or greater than 40.
00:32:48 [W] Here you can see this the same to shards in the - 40 key space and here's our the European equivalent showing that they're only in Europe.
00:33:03 [W] And the control panel also gives us the same detail and shows us that each tablet reachable it is and which one is the current master?
00:33:16 [W] We are now going to watch the career logs of each of the tablets so that we can see how the test is routing the queries locally and across the connectivity to Maine.
00:33:32 [W] So now we're going to start a MySQL client that is not too dense attached to the connectivity domain and it will not allow us to use these special NSE that we created for the VTA.
00:33:51 [W] Not disable the local community service definition. There is still a the kubernetes service cluster service that is is created for the VT gates for local services to connect to.
00:34:12 [W] And that is still available to use.
00:34:22 [W] however, if we do start in my SQL client within the connectivity domain
00:34:34 [W] Then it will be able to use the connectivity and servicemeshcon act and we were able to disable the VT gate service and kubernative service entirely.
00:34:46 [W] So that only connectivity to main pod to be there.
00:34:51 [W] So now we're inserting some data making one into the us and one with a French French national and you'll see that it only went the US citizen went to the Americas master.
00:35:02 [W] the French citizen went to the European master.
00:35:04 [W] You can see we were able to get the data from both died from both Masters.
00:35:13 [W] and both different regions
00:35:17 [W] Then it will be able to use the connectivity and servicemeshcon act and we were able to disable the VT gate service kubernative service entirely.
00:35:28 [W] so that only connectivity to main pod to be there.
00:35:29 [W] So now we're inserting some data making one into the us and one with a French French national and you'll see that it only went the US citizen went to the Americas Master the French citizen
00:35:31 [W] In this case, we're saying we're going to clearly only the America Shard. So now we only see the US citizen be inserted.
00:35:34 [W] And then we can do the same thing with the European chard.
00:35:40 [W] And now we can only see the French citizen.
00:35:48 [W] So now we are going to do a failover of the Americas Master into the European region safety wanted to do some maintenance and with your and the Americas region we can now do a plan
00:36:22 [W] Plan failover that your application would not notice at all because the VT gate well pause the query while this repairing occurs.
00:36:32 [W] So now we can see that the master is now in the European cell and the Americas is now a replica.
00:36:49 [W] And while still connected to the Americas VT gate we can see that it rerouted the query to the European replica through the connectivity to Maine.
00:37:01 [W] And then we can still perform normal scatter gather queries. And in this case now in both replica shards are entirely in the European cell.
00:37:16 [W] are now fail back to the to the Americas sell for the American chard.
00:37:23 [W] And we can show that is we are now failed back over.
00:37:29 [W] We can insert more data to show that it's the answer now goes to the America shard.
00:37:37 [W] So that was that's our Toc for the references. The nsmcon links here for the nsmcon tanned the virtual layer 3 and I see that we've worked on at Cisco and the
00:38:16 [W] The nsmcon jecht and the virtual layer 3 and I see that we've worked on at Cisco and the vitess project and their planetscale operator that you saw live in the demo.
00:38:23 [W] That was what was orchestrating the whole vitess installation.
00:38:29 [W] This is the remaining the the test schedule some of these have happened but they're interesting talks that you may want to check out recordings of
00:38:37 [W] and similarly the nsmcon was a day Zero event that had a couple of talks that would be directly useful if you're interested in nsmcon
00:38:53 [W] Virtual layer 3 and the keynote was on the previous slide.
00:39:05 [W] We have discussed it a little bit and then there's a talk coming up on Thursday that's going to have a lot more of the noodle latest and greatest details for the network servicemeshcon jecht.
00:39:14 [W] Thank you, and this is our contact information and have a great conference.
00:39:25 [W] Thanks. Okay, everybody.
00:39:27 [W] Okay.
00:39:33 [W] Thanks for listening to the talk.
00:39:39 [W] We do have a couple of questions in our QA.
00:39:44 [W] So the the first one was so I entered it online but I'll read it again is the network service registry and nsmcon Trudeau main scenarios run outside of the kubernative Clusters
00:39:55 [W] Answer to that is that it can be run anywhere really?
00:40:03 [W] We ran it in our example and a third cluster.
00:40:05 [W] It just had we and we made we made it reachable via a DNS from the to member clusters of the Clusters with the actual workload the test workloads in
00:40:18 [W] Both reaching so that the nsa's could register.
00:40:22 [W] The next question is whether we're using the latest version of nsmcon.
00:40:54 [W] Fixes that we've been working on pushing up stream. We are pretty close to master the
00:41:01 [W] the the next question
00:41:04 [W] is 10 the Vitesse components be part of a servicemeshcon the same time using nsmcon.
00:41:34 [W] Yeah, we can so I'm trying to figure out how to show this on the screen.
00:41:49 [W] But yes, we can use we can put a servicemeshcon like sto on top of this work there.
00:41:52 [W] You would have to have some additional service entry configurations in the at the sto layer to essentially treat the components as external services.
00:41:53 [W] From the sto the point of view for for that cross the domain boundary.
00:41:55 [W] There is a possibility that we could have the actual service representations from the nsmcon connections be represented in in the sto and natively, but that would require
00:42:05 [W] Even with how service Discovery Works in Korea CEO pilot, so that meat would lead like an extension.
00:42:16 [W] Ken molds that following question 10 multiple NSE sbus together like a service chain.
00:42:31 [W] Yes. That is a powerful feature of nsmcon.
00:42:36 [W] We in our example, we have a single NSA providing the network service for vol 3, but potentially you could have other nscs that do something like intrusion prevention or detection
00:42:47 [W] Firewalling something that has some other logical Network function.
00:42:53 [W] That's what kind of a bump in the wire would be certainly possible and nsmcon should change would handle the chaining part of it.
00:43:01 [W] You would just have to have an administrator that is setting up the nsmcon stew Define the chain so to find the sequence of Pops.
00:43:10 [W] the following question would traffic go out the normal the nsmcon our face or would have follow the path of the sto linkerd e product that provides
00:43:28 [W] So this is a follow-up, I guess for the servicemeshcon watchin.
00:43:43 [W] ya. So with how we like I was mentioning by creating service entries in the servicemeshcon.
00:43:58 [W] Destination in the other cluster then the the actual L3 routing would would cause the client traffic to go out this nsmcon interface
00:44:13 [W] To the other cluster so it will be fully integrated at the routing layer and at the service application servicemeshcon layer, there would be some configuration.
00:44:29 [W] that would be required to handle the L7 traffic management part to direct the destination. Basically how the service looks to that servicemeshcon.
00:44:41 [W] Out to that to the destination IP or entity nsmcon.
00:44:48 [W] And there's a following question.
00:44:54 [W] Can we share the slides, please?
00:44:57 [W] Yes, we will upload the slides in PDF form to the schedule after this talk.
00:45:08 [W] There's a there's also a slack channel for the hallway track for the networking track in Coop con that will be on and John.
00:45:15 [W] Do you want to mention the test hallway track details?
00:45:19 [W] Yeah, we're in the cube comes storage all holy trick Channel.
00:45:26 [W] Cool.
00:45:32 [W] So thank you guys there.
00:45:33 [W] I don't think we have any more questions live right now.
00:45:41 [W] But any other questions either the networking hallway track The Cisco Booth as well. We have a virtual Booth with a slack Channel and then the protest hallway track John will be lurking
00:45:50 [W] They're John Planet. This planetscale have a another and then planetscale also has a bit within its startup hallway be
00:45:59 [W] nice. I need to visit that.
00:46:09 [W] Okay, that's probably it.
