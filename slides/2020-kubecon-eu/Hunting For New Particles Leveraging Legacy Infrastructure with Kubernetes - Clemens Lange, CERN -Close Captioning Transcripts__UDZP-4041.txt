Hunting For New Particles Leveraging Legacy Infrastructure with Kubernetes: UDZP-4041 - events@cncf.io - Thursday, August 20, 2020 6:54 AM - 58 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:05:51 [W] Hi in this presentation.
00:05:56 [W] I'm going to talk to you about how one can leverage Legacy infrastructure that's agnostic of kubernative from within communities to potentially discover a new physics.
00:06:08 [W] My name is Clemens and I'm a particle physicists at CERN.
00:06:14 [W] I'm working on the CMS experiment which is one of the experiments at the Large Hadron Collider in the Franco Swiss area.
00:06:17 [W] Very close to Geneva in Switzerland in this photo on the right hand side.
00:06:24 [W] You can see me standing right next to the accelerator so you can see in blue the dipole magnets.
00:06:29 [W] These are there to actually bend the particle beams. So they go around in circle. So and the circle actually has a circumference of 27 kilometers and it's a kilometer underground and you can see here a Lego model of the LHC
00:06:42 [W] Comment on this photo on the right hand side.
00:06:43 [W] You can see me standing right next to the accelerator. So you can see in blue the dipole magnets. These are there to actually bend the particle beams.
00:06:44 [W] So they go around in circle. So and the circle actually has a circumference of 27 kilometers and it's a kilometer underground and you can see here a Lego model of the LHC obviously much smaller, but here's one of the
00:06:45 [W] But here's one of the experiments that's particularly interesting of dear to me, which is here in red. That's the CMS detector, which is recording the collisions by the LHC and I'm analyzing these data.
00:06:55 [W] So the field that I'm working is called a high-energy physics and short tap and is all about trying to understand the smallest building blocks of matter.
00:07:09 [W] So as I said the particle detectors such as the CMS detector, they record these collisions and the LHC actually provides up to 40 million collisions per second and the detector you can imagine it as some kind of digital camera.
00:07:21 [W] Inside it's actually taking photos of the collisions and that's happening 24/7.
00:07:29 [W] almost all year long and you can see one of those photos that is taken that has been taken on the right hand side.
00:07:38 [W] So the particle beams come in from both sides.
00:07:46 [W] And then in the very center of the detector of the particle beams are brought into collision and then these sprays of particle develop here which are you know, lots of particles that are very close together and these particles that we are investigating.
00:07:54 [W] Getting the actually thinner than a hair.
00:07:55 [W] And the camera is actually a bit bigger than a standard digital camera.
00:08:03 [W] It's so the CMS detector it weighs 14,000 tonnes as a height of 15 meters and the length of 21 meters and has around 100 million channels.
00:08:14 [W] So that's effectively around 100 mega pixels, and these are particle detectors. I mean, they are better than your digital camera, which might also have a hundred Mega pick us because they can take 14 billion seconds.
00:08:27 [W] 40 million photos per second.
00:08:30 [W] So because there's one Collision every 25 nanoseconds and this is so much data that we can actually not store this immediately. We have to do something special. We have to filter things as quickly as possible largely based on hardware and then later on also based on software.
00:08:42 [W] Up to thousand of such photos per second for later analysis and this analysis since we're running all year long is then a big data analysis. So we are actually analyzing Terror to petabyte-scale for PETA bytes of data using C++ Python
00:08:58 [W] It's for the plumbing.
00:09:00 [W] For this kind of analysis.
00:09:06 [W] We really need to have a big infrastructure.
00:09:13 [W] And this is provided by the worldwide LHC Computing grid which consists of around a hundred seventy computer centers all around the world.
00:09:15 [W] You can see that on the right hand side here.
00:09:18 [W] So the data there are accumulated at zone.
00:09:20 [W] So that's the tr0.
00:09:22 [W] and then added their distributed the reconstructed and then distributed to the different sites all around the world. So you can see the Tier 1 sides that have particularly high bandwidth links to the t 0
00:09:35 [W] Though, you know for instance in Russia in Italy and Germany and the UK and the US and all kinds of places and then there are around 160 further cites. The so-called tier two sides that also have parts of the data that's
00:09:47 [W] So that we can send all analysis drops to the grid and analyze the the data where they're actually located and there are even more smaller batch Farms so called tier threes there then local and limited to those working at these
00:10:02 [W] So called tier threes then local and limited to those working at these for instance research institutions and that and all these sites.
00:10:09 [W] There are often already managed using communities and that sun. We have a slightly bigger local cluster.
00:10:18 [W] So we have a batch from that's running on HT Condor a high throughput Computing software for batch Computing and then we have 230 thousand calls and that allows up to a hundred fifty thousand jobs to run simultaneously and that
00:10:30 [W] results in about a maximum of 1.4 million jobs that are completed per day.
00:10:33 [W] now this grid size there are large and therefore the you know large scale is processing and most of these steps already automated so, you know from actually reconstructing the data that we've taken and also
00:10:50 [W] Associated simulation that we compare the data to process.
00:11:01 [W] This is all automated. But the part that is still pretty manual is actually the so-called high level physics analysis.
00:11:05 [W] So this is some analysis that tries to address a specific physics question.
00:11:08 [W] So for instance when we were searching for the Higgs boson, there were several analyses trying to find the Higgs boson eventually worked out but this is something that is a somewhat manual process at this point because it needs a lot of very detailed inputs that come at that are provided from
00:11:24 [W] Several groups and individuals from within the collaboration so so for instance calibration constants and factors and Corrections are certain uncertainties Etc.
00:11:36 [W] They are all provided by the group and one has to collect them in order than to provide perform the final analysis.
00:11:46 [W] So this is pretty complicated and therefore challenging task to achieve.
00:11:48 [W] And we can actually keep track of all this thanks to Version Control.
00:11:53 [W] So we're using get and so we can capture the implementation and to execute these jobs. We can also make use of software images so effectively Docker containers in this case.
00:12:04 [W] The challenging part is now once you've captured the software.
00:12:10 [W] So you have the software under Version Control and you you then build a Container for instance using continuous integration.
00:12:17 [W] We're using gitlab mostly for that.
00:12:19 [W] To know how you actually execute this container of this image.
00:12:27 [W] So we also need to capture the commands that's also doable, but the big challenge lies and actually capturing the full workflow.
00:12:35 [W] So how do we connect the individual analysis steps? And for those there are several tools under investigation and some of them already used by smaller groups are just want to find out a few here.
00:12:44 [W] So that's for instance a project at CERN.
00:12:45 [W] That's called Rihanna.
00:12:46 [W] That's a common workflow language implementation. It's has a help Focus, but it's also possible to use it.
00:12:54 [W] For the things then there's the adage which really focused on hap only and then but there are also tools that are very general purpose for instance Luigi that has originally been developed by Spotify.
00:13:06 [W] And that's also being used by a few groups and I asked myself the question.
00:13:12 [W] Can we actually do this kind of financial physics so work for processing in a cloud knative way and for that for workflows in communities. I found Argo workloads and this seems to be a really nice tool to use.
00:13:25 [W] So now before I actually go into implementing a workflow from my hand of high energy physics in Argo I first wanted to talk to you about my cluster.
00:13:37 [W] So at Sun we can actually provision clusters on-prem with openstack and then we have some additional plugins that are somewhat certain specific so or high energy physics specific, so there's a certain VM file
00:13:51 [W] We can efficiently cash the software which is quite a lot.
00:13:59 [W] So it's actually gigabytes of software that we need to use for and for the block storage we have file system. So that's that's a very particular for the big data sets.
00:14:07 [W] They were for which we use EOS. My cluster itself here is actually pretty small and has four nodes with 4 cores each and the server 8 gigabytes of RAM.
00:14:20 [W] I have some object storage and also some block storage so it's 3 & 7
00:14:21 [W] S and 300 gigabytes are sufficient to store for instance the output.
00:14:27 [W] So that's the final product from the big data sets on disk for later and some analysis. This cluster runs on kubernative version one 18.2 and this
00:14:41 [W] Master I actually managed by a gitops that's you end and here I'm using Argo City.
00:14:51 [W] I can also put my secrets on the Version Control by using stops.
00:14:56 [W] There's a bar become Barbican modification. So that's the secret manager for openstack and I can deploy this using customized stops.
00:15:01 [W] There's a plug-in for that with Argo City.
00:15:04 [W] Now let's actually see how you know typical workflow would look like when when using Argo and and here this particular example is about searching for a new signal in the data.
00:15:20 [W] All right, that's the demo.
00:17:34 [W] All right, that's the demo.
00:17:46 [W] I cut it down from four minutes to 2 minutes only and I told you at the very end that there's a plot that is produced by a fit in actually in the end and we're searching for new physics
00:17:47 [W] Actually in the end and we're searching for new physics here and what you can see on the left hand side and in this plot in the different colors the different processes that actually no would end up in this particular
00:17:57 [W] Searching for the signal and in the very center and dark green. You see something that's labeled as a signal and this is what a potential new physics signal would look like and the black markers actually the data so you can see already pre fit so before
00:18:13 [W] Adjustment the simulation that we produced matches nicely with the data.
00:18:24 [W] And now the question that we are asking us in a typical physics analysis.
00:18:26 [W] Is there any new physics signal right, so we optimized so that we can in simulation actually see the signal our have to tell use the data to test if there is a new physics signal so we running this fit that allows the individual components to adjust to the data.
00:18:39 [W] So the black markers won't work, but everything else Allah is allowed to move on the right hand side you actually
00:18:46 [W] Really see that post fit. So after the fit the data do not support the presence of the signal.
00:18:50 [W] So in this case, we have not to discovered physics and that's actually the case most of the time but I mean, that's no reason to give up and it's the confident that we'll find new physics in the near future.
00:19:02 [W] Now, let me take a step back and look again at the example workflow here.
00:19:11 [W] So that's the screenshot from the demo that we just saw and I should point out that a you know, this is really just a demo a realistic physics analysis much more complex.
00:19:18 [W] Usually when I run, you know, the first step that is here delightfully parallel as you can see so that has the large amounts of parallel jobs.
00:19:32 [W] I would usually run several hundreds of them and they would each run.
00:19:34 [W] in the near future
00:19:43 [W] now, let me take a step back and look again at the example workflow here.
00:19:44 [W] So that's the screenshot from the demo that we just saw and I should point out that a you know, this is really just a demo a realistic physics analysis much more complex.
00:19:45 [W] Usually when I run, you know, the first step that is here delightfully parallel as you can see so that has the large amounts of parallel drops.
00:19:48 [W] would usually run several hundreds of them and they would each run for zettabytes.
00:19:48 [W] Hours, if not even days depending on the complexity and the size of the data set then I'm analyzing what's was really nice hear about our goals that I actually could scatter out dynamically so I could create jobs.
00:19:55 [W] I could Define at the very beginning. I want, you know, five jobs for this particular data says I want to certain other number four another step and I'll go Justice and then at the end I couldn't collect the information again, and as I mentioned in the video demo that can be
00:20:02 [W] Next I would dynamically so I could create jobs.
00:20:03 [W] I could Define at the very beginning. I want, you know, five jobs for this particular data set and I want to certain other number four another step and I'll go Justice and then in the end I couldn't collect the information again, and as I mentioned in the video demo that can be
00:20:04 [W] That I can pick up again.
00:20:07 [W] So this works really nicely.
00:20:11 [W] Now what isn't so nice is that life's a bit unfair and realistic physics analysis workflow can actually not be run on my cluster.
00:20:22 [W] All right. So I mean my cluster has 16 cores and the HD condo batch Farm. It has 230,000 cores and 970 terabytes of memory.
00:20:31 [W] So the question that I ask myself if I want to be able to run realistic Cloud native workloads.
00:20:32 [W] With Argo can I actually somehow make use of these cars and HD Connor and I you know, because with the 16 causes that II won't get very far.
00:20:44 [W] And I found the solution to that and that is what I call here.
00:20:50 [W] The HT condo operator.
00:20:53 [W] So the idea is to introduce an HTC job custom resource definition.
00:20:59 [W] You can see the definition on the left-hand side here.
00:21:03 [W] So there is an API version the kind is the custom resource definition and then you know, the name is HTC job. And the idea of this job is that it should mimic classical communities job so needs to
00:21:15 [W] somehow reflect running failed and succeeded status and I can do that via status properties of this resource.
00:21:28 [W] How make use of these cars and HD condo and I you know, because with the 16 causes that II won't get very far.
00:21:35 [W] And I found the solution to that and that is what I call here.
00:21:35 [W] The HT condo operator.
00:21:36 [W] So the idea is to introduce an HTC job custom resource definition.
00:21:36 [W] You can see the definition on the left-hand side here.
00:21:37 [W] So there is an API version the kind is the custom resource definition and then you know, the name is HTC job. And the idea of this job is that it should mimic classical communities job so needs to somehow reflect.
00:21:42 [W] I mean failed and succeeded status and I can do that via status properties of this resource.
00:21:53 [W] And in addition, I want to be able to understand what's happening inside the HD Condor badge cluster so I can add additional Fields such as the job ideas here that reflect the actual drop ideas within the HD conduct cluster so that I can
00:21:53 [W] So that I can know Monitor and interact by using these job Heidi's and this is actually not work only by me. But this has largely been done by Titus break is a bioinformatics student at Vilnius University who
00:21:55 [W] Get back into let's get down into the implementation.
00:22:03 [W] So when you have a custom resource definition you need to do something that acts on it and the operator so and communities that's done by an operator.
00:22:11 [W] So the and we looked around a bit with what kind of operators we could use.
00:22:15 [W] We looked into matter controller and then we found the operator SDK and that makes it really easy to get a communities operator implemented in running in the first place. So we went with that.
00:22:23 [W] Now this operator needs to know about HD condo in one way or the other. So I built a Docker container that contains the HD conduct client and in addition.
00:22:31 [W] We need to authenticate to this cluster. So 2ht condo and that's done by a Kerberos and I can create a Kerberos token via the secrets that I can store in my cluster.
00:22:45 [W] The HD canonical later itself is not actually installed into this container. And in addition what this kind of operator needs to know. It needs to translate the information from the drops back into something that can be executed on HD Condor.
00:23:01 [W] So here in particular the drops are not executed using Docker when running on HD condo, but the executed using Singularity so there's a translation step that can use it translates the image.
00:23:14 [W] Can store in my cluster?
00:23:25 [W] The HD canonical later itself is not actually installed into this container. And in addition what this kind of operator needs to know. It needs to translate the information from the drops back into something that can be executed on HD Condor.
00:23:27 [W] So here in particular the drops are not executed using Docker when running on HD condo, but the executed using Singularity so there's a translation step that use it translates the image spec and the componentconfig.
00:23:30 [W] The the script into something that is then effectively a singularity command with a script attached to it.
00:23:33 [W] If we now look at the implementation in the boxes below you can see that in my cluster. I install the HTC operator that knows about the about HD corner and has my credentials when I cried in SGC job,
00:23:37 [W] You can see that in my cluster.
00:23:38 [W] I installed the HTC operator that knows about the about HD corner and has my credentials when I create an HTC job.
00:23:43 [W] So it washes the operator Watchers for them to be created. And as soon as there's one it will use the API of HT Condor to to submit jobs to the batch cluster.
00:23:51 [W] Then these batch jobs are non HD.
00:23:52 [W] Condor side will be executed and in addition to having the API implemented.
00:23:57 [W] Here I also have at the very end of my job with book via cloudevents that actually tell my operator that a job is done and which one is the good job that is done in particular as well as exit code so that I can reduce
00:24:10 [W] Far is from the HTC operator through the batch cluster.
00:24:14 [W] Okay. So this is the HTC job definition. I created a job and then it's um, it's something now I have to make this work with our go in. The nice thing is that Argo can actually manage any kind of community's resources.
00:24:31 [W] So here's a step from from Argo.
00:24:37 [W] It's called generate batch has a couple of parameters among them for instance the number of jobs and then I can say I want to have a resource and the action for this request and resource should be to create it.
00:24:46 [W] And then I at the very bottom you see the beginning of the Manifest and you can see that the kind of resource that I want to be created is a nice HTC job.
00:24:54 [W] So now what I need to do in addition is I need to Define some success condition that determines whether my jobs are all done or if any of them failed.
00:25:10 [W] So I Define a success condition that the number of so the status field in my custom resource information needs to be equal to the number of drops that I started with and if any of the jobs failed then also this job is supposed to fail so that
00:25:21 [W] Actually move the long-running steps into HD condom directly from my Argo workflow.
00:25:27 [W] So what I'm going to do now is I'm going to run the same workflow as before but to moving this first step to HD Condor and I'm going to speed this up a bit, but let's see how this goes.
00:25:42 [W] All right. So this worked really well as I said I sped up things a bit.
00:30:33 [W] So this in-toto would have taken something like 30 minutes just because I'm not the only one running jobs inside the cluster. So an HD camera so such a cluster is not really made for short running jobs, but the short Q is actually something like 15
00:30:41 [W] All right. So this worked really well as I said I sped up things a bit.
00:30:43 [W] So this in-toto would have taking something like 30 minutes just because I'm not the only one running jobs inside the cluster. So an HD camera so such a cluster is not really made for short running jobs, but the short queues actually something like
00:30:43 [W] It's okay that things take a some time to be scared to actually run.
00:30:54 [W] But if I run a real physics analysis, you know, that would be perfectly fine with me. But just to close out I think you know what I managed to show you is here that I leveraged Legacy
00:31:04 [W] No, kubernative agnostic Computing infrastructure by means of a cabinet has custom resource definition that is combined with an operator and the operator concept is extremely powerful for this purpose. And also I found that
00:31:20 [W] Energy physics workloads are possible using Argo. So that's really nice.
00:31:27 [W] And now there are a couple of steps still to be done.
00:31:31 [W] So the HTC operator actually needs to be made a bit more flexible because when you choose Qs + HT canonical you can also directly translate for instance computer requirements.
00:31:40 [W] So how long should your job run etcetera in so you're going to need to translate? What is there in kubernative to something that is understandable or understood by HT coredns?
00:31:51 [W] On door and of course, it would be nice sooner or later to also make use of these hundred seventy grid sites that are available for me to use.
00:32:04 [W] So the W LC G.
00:32:07 [W] So that's on the road map. But actually if you want to see something more on this topic, there are there is a presentation by Alessandra 40 and Lucas Henry also that coupon here who actually talked about how they have
00:32:19 [W] On the on the laptop so I couldn't principal run my workflow in kubernative then send it to the grid where the grid would be.
00:32:27 [W] There are local laptop.
00:32:29 [W] So that's fun, isn't it?
00:32:32 [W] All right. So thanks to tell us again for his great work.
00:32:38 [W] I would also like to thank the cloud containers team at CERN. So that means in particular Thomas Ricardo and Spiros and also Lucas and now I'm very happy to take your questions.
00:32:46 [W] So thank you for your attention.
00:32:47 [W] Hello.
00:32:54 [W] So thanks again for joining.
00:33:01 [W] I got a lot of questions in the chat.
00:33:03 [W] I already tried to answer a couple of them.
00:33:08 [W] I'll just go through them in the following probably the those that are no generally most interesting for other things. You can chat with me in this cncf slack afterwards
00:33:20 [W] I'll be monitoring the the machine Learning Channel. So in that there was a very general question on general information on son. We actually have a our own top-level domain so you can just
00:33:36 [W] Look information on son.
00:33:37 [W] We actually have a our own top-level domain so you can just go to home dot CERN CER in and from there you find information in particular on the the CERN experiment.
00:33:47 [W] So the Large Hadron Collider and also the experiments at the Large Hadron Collider, but also smaller experiments that are taking place at son.
00:33:57 [W] Then there was another question on how many communities class.
00:34:01 [W] Esters are being used at CERN and how many nodes on the biggest cluster actually don't have a concrete answer to that.
00:34:12 [W] I mean II know that there are several hundred clusters, but I think that most of them actually rather small and I know that this one of the Clusters
00:34:23 [W] And cause so usually have opted depends on all these are set up some some, you know, we have different flavors of the VM style that we use of the nodes.
00:34:38 [W] So some of them have eight cores others have four. So the standard would be for with 8 gigabytes of RAM.
00:34:45 [W] Then there was another question on the storage technology that were using to store this data and we actually a larger using EOS and I put the link to EOS into the into the Q&A window.
00:35:02 [W] Can also find it by just searching for z/os and son. So that's a technology that's been developed at CERN.
00:35:13 [W] I think it's generally available and for long-term preservation. We rely on tape slots for data that aren't actively use the actually copied over to tape and then, you know
00:35:26 [W] around on them the seventh time to me and the other day than it can easily take a few days until they actually staged out to disk and this archiving does not only happen at some but it also happens on the different grid sizes so you can be a
00:35:42 [W] Or unlucky. Mmm, then there was another question on having a link to the presentation.
00:35:56 [W] So I uploaded the presentation as a PDF of that to the scat.com entry you so you should be able to find it there and also added a PDF with the two links to the demos.
00:36:05 [W] I just uploaded them to YouTube because I wasn't sure if the quality will be good enough, but I think they showed up nicely. So yeah, but still if you want to
00:36:14 [W] To re-watch them specifically feel free to do though. You can find them there with the links.
00:36:22 [W] Okay, and then are a couple of questions on the workflows.
00:36:30 [W] So the question was, you know, what is so specific about the high energy physics that that we might need separate tools.
00:36:42 [W] So in general I would say that we
00:36:43 [W] Do not really need to dedicate it tools so we could actually work with the existing tools. But yeah, there are a lot of there's a lot of custom tooling we're using also rather old systems,
00:36:58 [W] Always a couple of years behind with the latest greatest software Etc.
00:37:08 [W] So, you know, we can work around that with containers, but still, you know, that's the the custom tooling then, you know, you have to do authentication.
00:37:20 [W] So we have certificates that we use Kerberos and other places and all that all that and so say one of the few people
00:37:28 [W] people actually trying to make these designer GIF is except work flows accessible for people within the community and for that I think it's really important to give them a really good experience.
00:37:43 [W] So to make it as easy as possible for them to get started with Sai cloud knative or automated workflows and also actually stay as close as possible.
00:37:58 [W] Flo's accessible for people within the community and for that I think it's really important to give them a really good experience.
00:37:59 [W] So to make it as easy as possible for them to get started with Sai Cloud native or automated workflows and also actually stay as close as possible
00:38:00 [W] It's a some some industry or open source solution that can be used because you know, otherwise I would have to do all the maintenance and that's probably not what I want to do. But
00:38:13 [W] Yeah, so the custom tooling is mostly just around for instance this scatter gather Paradigm.
00:38:24 [W] So we you know, we have a couple of data sets but a data set actually contains a couple of thousand files and then you want to paralyze this in a dynamic way.
00:38:36 [W] So and then afterwards you want to collect this again, but you don't want to collect all the files in one step but you want to do this in some kind of cascade and that's something that's
00:38:43 [W] Maybe a bit more have specifics of this could be abstracted away from the user so that the physics analyst that's something useful.
00:38:56 [W] Okay, I mean about the choice of the workflow tool.
00:39:04 [W] I just went with Argo because I had you know, I found out a few years back and and I was actually curious like could we use it as I mentioned the presentation there are other tools for instance.
00:39:14 [W] G and probably a flow is an equally good solution.
00:39:23 [W] It just needs someone to actually give this a try and see if that works for them.
00:39:27 [W] Okay, so
00:39:31 [W] Now completely switch to kubernative and I'm actually actually have to say we were only really getting started with kubernative. I mean, the the Our IT department is no heavily invested in kubernative, but say all the jobs that we running
00:39:59 [W] Still running on HD condo or slum or other platform so we can execute the containers, but we cannot really we we are not using kubernative
00:40:14 [W] Actually fought for the workforce as I'm doing it now.
00:40:17 [W] Okay, there are a couple of other questions on for instance this case sobs plugin for Argo CD and and other links or hit me up in the chat. I can post them there.
00:40:33 [W] I don't think I will have time for that.
00:40:36 [W] Maybe I take like one last question.
00:40:44 [W] So so I know there was a question like you know, what is the what needs to be improved?
00:40:48 [W] for kubenetes to make it easier for sir nor for high energy physics in general and I said, yeah, I'm probably it's a multi Tennessee and I would say,
00:41:03 [W] You know the way that we do authentication and authorization is also currently changing. So so in principle, of course each user should be able to create jobs in a cluster but for instance not delete some other users.
00:41:18 [W] um job and so principle each user needs to have their own dedicated namespace that need to be authenticated for instance via oauth and one way or the other but it's a most of the works actually happening in the terminal so this
00:41:33 [W] We linked and that's currently giving me some kind of had like even though I mean there are solutions but this needs to be figured out.
00:41:41 [W] All right, so I see we're out of time. So I thank you again very much for joining this presentation.
00:41:51 [W] Hope you found it useful and I'll be monitoring the slack check for further questions. So have a great day. Thank you.
